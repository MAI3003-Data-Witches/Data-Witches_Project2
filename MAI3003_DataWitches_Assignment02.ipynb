{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/MAI3003-Data-Witches/Data-Witches_Project2/blob/main/MAI3003_DataWitches_Assignment02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data Witches**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| **Name**         | **Student ID** |\n",
    "|------------------|----------------|\n",
    "| Claessen, VVHJAE | i6339543       |\n",
    "| Ovsiannikova, AM | i6365923       |\n",
    "| Pubben, J        | i6276134       |\n",
    "| Roca Cugat, M    | i6351071       |\n",
    "| Záboj, J         | i6337952       |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Logbook**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Changes**\n",
    "Also see Git Commit History.\n",
    "\n",
    "| **Version** | **Changes**      | **Date** |\n",
    "|-------------|------------------|----------|\n",
    "| v0.0        | Dataset loaded, EDA, cleaning   | 18-11-25 |\n",
    "| v1.1        | *** | XX-11-25 |\n",
    "| v0.2        | ***  | XX-11-25 |\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Methods\n",
    "\n",
    "Let's ensure we all use the same names for all components.  \n",
    "\n",
    "| **Variable**                   | **Name**                   |\n",
    "|-------------------------------|----------------------------|\n",
    "| Raw ECG dataframe             | df                         |\n",
    "| Label dataframe               | df_labels                  |\n",
    "| HRV features (train)          | hrv_train                  |\n",
    "| HRV features (test)           | hrv_test                   |\n",
    "|HRV extraction type           | FULL (nk.hrv — time + freq + nonlinear + RSA)|\n",
    "| Clean HRV dataframe (train)   | hrv_train_clean            |\n",
    "| Clean HRV dataframe (test)    | hrv_test_clean             |\n",
    "| HRV + labels (train)          | hrv_train_with_labels      |\n",
    "| Winsorized HRV column         | HRV_MedianNN_winsor       |\n",
    "| Model feature matrix (train)  | X_train                    |\n",
    "| Model feature matrix (test)   | X_test                     |\n",
    "| Model target vector (train)   | y_train                    |\n",
    "| Model target vector (test)    | y_test                     |\n",
    "\n",
    "\n",
    "| **Function**              | **Description**                                | **Arguments**                                |\n",
    "|---------------------------|------------------------------------------------|----------------------------------------------|\n",
    "| corr_plot_hrv()           | Correlation plot for HRV features              | df, cols=None                                |\n",
    "| distplots_hrv()           | Distribution plots (hist + KDE)                | df, cols=None                                |\n",
    "| boxplots_hrv()            | Boxplots for selected HRV variables            | df, cols                                     |\n",
    "| check_missing_hrv()       | Missingness summary                            | df                                           |\n",
    "| identify_outliers()       | IQR-based outlier detection                    | df, column_name, threshold=1.5               |\n",
    "| model_evaluation()        | Confusion matrix + classification report       | model                                        |\n",
    "| model_desc()              | Accuracy, CV, ROC-AUC, model performance       | model                                        |\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preface"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Packages imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#!pip install -r requirements.txt",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import random\n",
    "import os.path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib.testing import xfail\n",
    "import plotly.graph_objects as go\n",
    "from colorama import Fore, Back, Style\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Options settings"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(3003)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DATA_PRESENT = os.path.isfile(\"data/Physionet2017Training.tar.xz\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset download"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not DATA_PRESENT:\n",
    "    !mkdir data\n",
    "    !wget https://github.com/MAI3003-Data-Witches/Data-Witches_Project2/raw/refs/heads/main/data/Physionet2017Training.tar.xz -O data/Physionet2017Training.tar.xz\n",
    "    !tar -xf data/Physionet2017Training.tar.xz -C data\n",
    "else: print(\"You already have the dataset downloaded, skipping\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/Physionet2017TrainingData.csv', header=None, index_col=False) * 1000 # Load the dataset already in mV\n",
    "#TODO: is it actually needed to convert to mV?\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing\n",
    "## Extract ECG signals and class labels"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_labels = pd.read_csv('data/Physionet2017TrainingLabels.csv', header=None, names=['label'])\n",
    "df_labels['classification'] = df_labels['label'].replace({\"N\": 0, \"A\": 1})\n",
    "df_labels['label'] = df_labels['label'].replace({\"N\": 'Normal Sinus Rhythm', \"A\": 'Atrial Fibrillation'})\n",
    "# 0: Normal Sinus Rhythm\n",
    "# 1: Atrial Fibrillation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_labels",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset splitting"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_labeled = pd.merge(df_labels.drop(columns='label'), df, left_on='classification', right_index=True)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df.index,\n",
    "    test_size=0.2,\n",
    "    stratify=df_labels[\"label\"],\n",
    "    random_state=3003\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Functions Definitions"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correlation plot"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correlation plot\n",
    "def corr_plot_hrv(df, cols=None):\n",
    "    \"\"\"\n",
    "    Correlation heatmap for HRV features.\n",
    "    \"\"\"\n",
    "    data = df[cols] if cols else df.select_dtypes(\"number\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(data.corr(), cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"Correlation Map (HRV Features)\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution plots"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution plots\n",
    "def distplots_hrv(df, cols=None):\n",
    "    \"\"\"\n",
    "    Distribution plots (hist + KDE) for HRV features.\n",
    "    \"\"\"\n",
    "    data = df[cols] if cols else df.select_dtypes(\"number\")\n",
    "    for col in data.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.histplot(data[col], kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.show()\n",
    "\n",
    "def distplots(df):\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    num_features = len(numeric_df.columns)\n",
    "    cols = int(np.ceil(np.sqrt(num_features)))\n",
    "    rows = int(np.ceil(num_features / cols))\n",
    "\n",
    "    # A figure with subplots looks much nicer\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(numeric_df.columns):\n",
    "        # Replace inf with NaN and then drop NaNs\n",
    "        numeric_df_nona = numeric_df[column].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        axes[i].hist(numeric_df_nona, bins=30, alpha=0.7, edgecolor='black')\n",
    "\n",
    "        if len(numeric_df_nona) > 1:\n",
    "            density = stats.gaussian_kde(numeric_df_nona)\n",
    "            xs = np.linspace(numeric_df_nona.min(), numeric_df_nona.max(), 200)\n",
    "            axes[i].plot(xs, density(xs) * len(numeric_df_nona) * (numeric_df_nona.max() - numeric_df_nona.min()) / 30,\n",
    "                         'r-', linewidth=2)\n",
    "\n",
    "        axes[i].set_xlabel(column)\n",
    "        axes[i].set_ylabel('Number of Patients')\n",
    "        axes[i].set_title(f'Distribution of {column}')\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Remove any empty subplots if they exist\n",
    "    for j in range(num_features, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Boxplots"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Boxplots\n",
    "def boxplots_hrv(df, cols):\n",
    "    \"\"\"\n",
    "    Boxplots for detecting unusual HRV values.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.boxplot(y=df[col])\n",
    "        plt.title(f\"Boxplot of {col}\")\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missingness overview"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Missingness\n",
    "def check_missing_hrv(df):\n",
    "    \"\"\"\n",
    "    Summarize missingness across HRV features.\n",
    "    \"\"\"\n",
    "    missing = df.isna().sum()\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": df.columns,\n",
    "        \"missing_n\": missing,\n",
    "        \"missing_%\": (missing / len(df))*100\n",
    "    })\n",
    "    display(out.sort_values(\"missing_%\", ascending=False))\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "## Dataset characteristics"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_ecgs = len(df) # Number of ECGs\n",
    "\n",
    "num_samples = df.shape[1] # Number of samples per ECG\n",
    "\n",
    "sampling_frequency = 300#Hz\n",
    "duration = num_samples / sampling_frequency # Duration of each ECG\n",
    "\n",
    "class_distribution = df_labels['label'].value_counts() # Distribution over classes #TODO: quick pie chart?\n",
    "\n",
    "print(f\"Number of ECGs: {num_ecgs}\")\n",
    "print(f\"Number of samples per ECG: {num_samples}\")\n",
    "print(f\"Duration of each ECG: {duration} seconds\")\n",
    "print(f\"\\nClass Distribution:\\n{class_distribution}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Indices per class (based on df_labels)\n",
    "sinus_indices = df_labels[df_labels[\"label\"] == \"Normal Sinus Rhythm\"].index.tolist()\n",
    "af_indices    = df_labels[df_labels[\"label\"] == \"Atrial Fibrillation\"].index.tolist()\n",
    "\n",
    "example_sinus_idx = random.choice(sinus_indices)\n",
    "example_af_idx    = random.choice(af_indices)\n",
    "\n",
    "ecg_sinus_raw = df.iloc[example_sinus_idx].astype(float).values\n",
    "ecg_af_raw    = df.iloc[example_af_idx].astype(float).values\n",
    "\n",
    "time = np.arange(0, len(ecg_sinus_raw)) / sampling_frequency\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(time, ecg_sinus_raw, label=f\"NSR (index {example_sinus_idx})\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Voltage (mV)\")\n",
    "plt.title(\"Example raw ECG – Normal Sinus Rhythm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(time, ecg_af_raw, label=f\"AF (index {example_af_idx})\", color=\"orange\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Voltage (mV)\")\n",
    "plt.title(\"Example raw ECG – Atrial Fibrillation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Summary statistics for each ECG\n",
    "summary_stats = df.describe().T\n",
    "summary_stats = pd.concat([summary_stats, df_labels], axis=1)\n",
    "\n",
    "# Plotting the distributions of summary statistics\n",
    "stats_to_plot = ['mean', 'std', 'min', 'max', '25%', '75%']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, stat in enumerate(stats_to_plot):\n",
    "  sns.kdeplot(ax=axes[i], data=summary_stats, x=stat, hue='label', fill=True, common_norm=False)\n",
    "  axes[i].set_title(f'Distribution of voltage {stat} by rhythm category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "counts = df_labels.label.value_counts().tolist()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(counts, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Rhythm Distribution')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ECG exploration\n",
    "Let's have a look at the ECG signals. The AliveCor ECGs are single lead ECGs, similar to **Lead I** in a standard 12-lead ECG. First, we determine which ECGs are in Normal Sinus Rhythm, and which ECGs are in Atrial Fibrillation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Randomly select ECGs from each class\n",
    "number_of_selectedECGs = 1  # change to plot more ECGs for each rhythm category\n",
    "selected_sinus_indices = random.sample(sinus_indices, number_of_selectedECGs)\n",
    "selected_af_indices = random.sample(af_indices, number_of_selectedECGs)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=number_of_selectedECGs, cols=2, subplot_titles=(\"Normal Sinus Rhythm\", \"Atrial Fibrillation\"))\n",
    "\n",
    "# Plot NSR ECGs on the left subplot\n",
    "for i, index in enumerate(selected_sinus_indices):\n",
    "    time = np.arange(0, len(df.iloc[index])) / sampling_frequency  # Assuming sampling_frequency is defined\n",
    "    fig.add_trace(go.Scatter(x=time, y=df.iloc[index], mode='lines', name=f'ECG {index + 1}'), row=i+1, col=1)\n",
    "\n",
    "# Plot AF ECGs on the right subplot\n",
    "for i, index in enumerate(selected_af_indices):\n",
    "    time = np.arange(0, len(df.iloc[index])) / sampling_frequency  # Assuming sampling_frequency is defined\n",
    "    fig.add_trace(go.Scatter(x=time, y=df.iloc[index], mode='lines', name=f'ECG {index + 1}'), row=i+1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title_text=\"Selected ECGs: NSR vs. AF\",\n",
    "                  xaxis_title='Time (s)',\n",
    "                  yaxis_title='Voltage (mV)',\n",
    "                  showlegend=False,)\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ECG frequency analysis\n",
    "An important feature of any signal is the frequency content: it tells us which frequency a predominantly present in the signal. In this case, differences in cardiac rhythm may be associated with differences in frequency content. The following code computes the frequency content of an ECG signal using the *Fourier transform* (`fft` in Python).\n",
    "\n",
    "**Question:** how could we differentiate between NSR and AF based on the frequency content?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show the frequency content of each of the selected ECGs\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# Create subplots\n",
    "num_ecgs = len(selected_sinus_indices) + len(selected_af_indices)\n",
    "\n",
    "subplot_titles = []\n",
    "for index in selected_sinus_indices + selected_af_indices:\n",
    "    subplot_titles.extend([f\"ECG {index+1}\", f\"FFT {index+1}\"])\n",
    "\n",
    "fig = make_subplots(rows=num_ecgs, cols=2, subplot_titles=subplot_titles)\n",
    "\n",
    "# Plot selected ECGs and their FFTs\n",
    "row_num = 1\n",
    "for index in selected_sinus_indices + selected_af_indices:\n",
    "    ecg_signal = df.iloc[index].astype(float)\n",
    "    time = np.arange(0, len(ecg_signal)) / sampling_frequency\n",
    "\n",
    "    # Calculate FFT\n",
    "    ecg_data = np.asarray(ecg_signal)\n",
    "    N = len(ecg_data)\n",
    "    yf = fft(ecg_data)\n",
    "    xf = fftfreq(N, 1 / sampling_frequency)\n",
    "    xf = xf[:N // 2]  # Only consider positive frequencies\n",
    "    yf = 2.0 / N * np.abs(yf[:N // 2])\n",
    "\n",
    "    # Plot ECG on the left subplot\n",
    "    fig.add_trace(go.Scatter(x=time, y=ecg_signal, mode='lines', name=f'ECG {index + 1}'), row=row_num, col=1)\n",
    "\n",
    "    # Plot FFT on the right subplot\n",
    "    fig.add_trace(go.Scatter(x=xf, y=yf, mode='lines', name=f'FFT {index + 1}'), row=row_num, col=2)\n",
    "\n",
    "    row_num += 1\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title_text=\"Selected ECGs and FFTs\",\n",
    "                  xaxis_title='Time (s)',\n",
    "                  xaxis2_title='Frequency (Hz)',\n",
    "                  yaxis_title='Amplitude',\n",
    "                  yaxis2_title='Magnitude',\n",
    "                  showlegend=False,\n",
    "                  height=num_ecgs * 300)  # Adjust height based on the number of ECGs\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The *Welch periodogram* is an approach to estimate the frequency content of a signal that computes the *average* spectrogram of overlapping signal segments."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the Welch periodogram instead of the fft\n",
    "# Show the frequency content of each of the selected ECGs using Welch's method\n",
    "\n",
    "# Create subplots\n",
    "num_ecgs = len(selected_sinus_indices) + len(selected_af_indices)\n",
    "\n",
    "subplot_titles = []\n",
    "for index in selected_sinus_indices + selected_af_indices:\n",
    "    subplot_titles.extend([f\"ECG {index+1}\", f\"Welch Periodogram {index+1}\"])\n",
    "\n",
    "fig = make_subplots(rows=num_ecgs, cols=2, subplot_titles=subplot_titles)\n",
    "\n",
    "# Plot selected ECGs and their Welch periodograms\n",
    "numberOfSamplesPerFFT=1024\n",
    "row_num = 1\n",
    "for index in selected_sinus_indices + selected_af_indices:\n",
    "    ecg_signal = df.iloc[index].astype(float)\n",
    "    time = np.arange(0, len(ecg_signal)) / sampling_frequency\n",
    "\n",
    "    # Calculate Welch periodogram\n",
    "    ecg_data = np.asarray(ecg_signal) # Convert pandas Series to NumPy array\n",
    "    frequencies, power_spectrum = welch(ecg_data, fs=sampling_frequency, nperseg=numberOfSamplesPerFFT)\n",
    "\n",
    "    # Plot ECG on the left subplot\n",
    "    fig.add_trace(go.Scatter(x=time, y=ecg_signal, mode='lines', name=f'ECG {index + 1}'), row=row_num, col=1)\n",
    "\n",
    "    # Plot Welch periodogram on the right subplot\n",
    "    fig.add_trace(go.Scatter(x=frequencies, y=power_spectrum, mode='lines', name=f'Welch {index + 1}'), row=row_num, col=2)\n",
    "\n",
    "    row_num += 1\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title_text=\"Selected ECGs and Welch Periodograms\",\n",
    "                  xaxis_title='Time (s)',\n",
    "                  xaxis2_title='Frequency (Hz)',\n",
    "                  yaxis_title='Amplitude',\n",
    "                  yaxis2_title='Power Spectral Density',\n",
    "                  showlegend=False,\n",
    "                  height=num_ecgs * 300)  # Adjust height based on the number of ECGs\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# We do not need synthetic data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Powerline interference"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate an ECG\n",
    "ecg_sim = nk.ecg_simulate(duration=5, sampling_rate = sampling_frequency, method=\"ecgsyn\")\n",
    "time = np.arange(0, len(ecg_sim)) / sampling_frequency\n",
    "\n",
    "# Add a 50Hz signal\n",
    "frequency = 50  # Hz\n",
    "amplitude = 0.1  # Adjust the amplitude as needed\n",
    "signal_50hz = amplitude * np.sin(2 * np.pi * frequency * time)\n",
    "\n",
    "# Add the 50Hz signal to ecg_sim\n",
    "ecg_sim_with_50hz = ecg_sim + signal_50hz\n",
    "\n",
    "# Plot both signals\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim_with_50hz, mode='lines', name='ECG with 50Hz Noise'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim, mode='lines', name='Original ECG'))\n",
    "fig.update_layout(title_text=\"ECG Signals: Original vs. with 50Hz Noise\",\n",
    "                  xaxis_title=\"Time (s)\",\n",
    "                  yaxis_title=\"Amplitude\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute and show the Welch periodogram of the 50Hz ecg signal\n",
    "frequencies, power_spectrum = welch(ecg_sim_with_50hz, fs=sampling_frequency, nperseg=1024)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=frequencies, y=power_spectrum, mode='lines', name='Welch Periodogram'))\n",
    "fig.update_layout(title_text=\"Welch Periodogram of 50Hz ECG Signal\",\n",
    "                  xaxis_title=\"Frequency (Hz)\",\n",
    "                  yaxis_title=\"Power Spectral Density\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we create a so-called *notch filter*: this is a filter that supresses a certain frequency in a signal, in this case 50Hz"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standard signal processing algorithm can be found in the package scipy.signal\n",
    "import scipy.signal as signal\n",
    "\n",
    "# filter design\n",
    "notch_freq = 50 # frequency that we want to filter out\n",
    "quality_factor = 30\n",
    "nyquist_freq = 0.5 * sampling_frequency\n",
    "normalized_notch_freq = notch_freq / nyquist_freq\n",
    "b, a = signal.iirnotch(normalized_notch_freq, quality_factor)\n",
    "\n",
    "# apply the filter to the signal with noise\n",
    "filtered_ecg = signal.filtfilt(b, a, ecg_sim_with_50hz)\n",
    "\n",
    "# Plot the original signal with noise and filtered signal\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim_with_50hz, mode='lines', name='ECG with 50Hz Noise'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim, mode='lines', name='Original ECG'))\n",
    "fig.add_trace(go.Scatter(x=time, y=filtered_ecg, mode='lines', name='Filtered ECG'))\n",
    "\n",
    "fig.update_layout(title_text=\"ECG Signals: with 50Hz Noise vs. Filtered\",\n",
    "                  xaxis_title=\"Time (s)\",\n",
    "                  yaxis_title=\"Amplitude\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###Baseline wander"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Simulate an ECG\n",
    "sampling_frequency = 300  # Hz\n",
    "ecg_sim = nk.ecg_simulate(duration=10, sampling_rate=sampling_frequency, method=\"ecgsyn\")\n",
    "time = np.arange(0, len(ecg_sim)) / sampling_frequency\n",
    "\n",
    "# Add baseline wander\n",
    "wander = 0.5 * np.sin(2 * np.pi * 0.3 * time)  # 0.3 Hz baseline wander\n",
    "ecg_wander = ecg_sim + wander\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_wander, mode='lines', name='ECG with Wander'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim, mode='lines', name='Original ECG'))\n",
    "fig.update_layout(title_text=\"ECG Signals: Baseline Wandering\",\n",
    "                  xaxis_title=\"Time (s)\",\n",
    "                  yaxis_title=\"Amplitude\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute and show the Welch periodogram of the baseline wander ecg signal\n",
    "frequencies, power_spectrum = welch(ecg_wander, fs=sampling_frequency, nperseg=1024)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=frequencies, y=power_spectrum, mode='lines', name='Welch Periodogram'))\n",
    "fig.update_layout(title_text=\"Welch Periodogram of baseline wander ECG Signal\",\n",
    "                  xaxis_title=\"Frequency (Hz)\",\n",
    "                  yaxis_title=\"Power Spectral Density\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply a median filter\n",
    "window_size = 101  # Adjust window size as needed\n",
    "ecg_median_filtered = ecg_wander - signal.medfilt(ecg_wander, kernel_size=window_size)\n",
    "\n",
    "# Apply a 0.5Hz high-pass filter\n",
    "cutoff_freq = 0.5  # Hz\n",
    "nyquist_freq = 0.5 * sampling_frequency\n",
    "normalized_cutoff = cutoff_freq / nyquist_freq\n",
    "b, a = signal.butter(4, normalized_cutoff, btype='high', analog=False)\n",
    "ecg_highpass_filtered = signal.filtfilt(b, a, ecg_wander)\n",
    "\n",
    "# Plot the results\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_wander, mode='lines', name='ECG with Wander'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim, mode='lines', name='Original ECG'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_median_filtered, mode='lines', name='ECG Median Filtered'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_highpass_filtered, mode='lines', name='ECG High-pass Filtered'))\n",
    "\n",
    "fig.update_layout(title_text=\"ECG Signals: Baseline Wander Removal Comparison\",\n",
    "                  xaxis_title=\"Time (s)\",\n",
    "                  yaxis_title=\"Amplitude\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###Muscle noise"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate EMG noise at random intervals\n",
    "emg_noise = np.zeros_like(ecg_sim)\n",
    "\n",
    "std_dev_emg_noise = 0.05\n",
    "duration_emg_noise = int(0.05 * sampling_frequency)\n",
    "\n",
    "for _ in range(10 * 20):  # Assuming an average of 20 contractions per second\n",
    "    start = np.random.randint(0, len(ecg_sim) - duration_emg_noise)\n",
    "    emg_noise[start:start + duration_emg_noise] += np.random.normal(0, std_dev_emg_noise, duration_emg_noise)\n",
    "\n",
    "# Combine ECG signal with EMG noise\n",
    "ecg_muscle = ecg_sim + emg_noise\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim, mode='lines', name='Original ECG'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_muscle, mode='lines', name='ECG with Muscle Noise'))\n",
    "fig.update_layout(title_text=\"ECG Signals: Muscle Noise\",\n",
    "                  xaxis_title=\"Time (s)\",\n",
    "                  yaxis_title=\"Amplitude\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply a 100Hz low-pass filter\n",
    "cutoff_freq = 100  # Hz\n",
    "nyquist_freq = 0.5 * sampling_frequency\n",
    "normalized_cutoff = cutoff_freq / nyquist_freq\n",
    "b, a = signal.butter(4, normalized_cutoff, btype='low', analog=False)\n",
    "ecg_lowpass_filtered = signal.filtfilt(b, a, ecg_sim)\n",
    "\n",
    "# Plot the original signal with noise and filtered signal\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_muscle, mode='lines', name='ECG with Muscle Noise'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_lowpass_filtered, mode='lines', name='ECG Low-pass Filtered'))\n",
    "fig.add_trace(go.Scatter(x=time, y=ecg_sim, mode='lines', name='Original ECG',\n",
    "                         line=dict(color='gray', width=1.5)))\n",
    "fig.update_layout(title_text=\"ECG Signals: with Muscle Noise vs. Filtered\",\n",
    "                  xaxis_title=\"Time (s)\",\n",
    "                  yaxis_title=\"Amplitude\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##ECG feature engineering"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tradionally, ECG analysis is done visually, with a cardiologist/electrophysiologist that assesses the ECG signal(s) and makes a diagnosis based on standardized criteria (for instance the [Minnesota Code](https://link.springer.com/book/10.1007/978-1-84882-778-3)). Conventional ECG analysis tool translate these criteria into a set of automated algorithms that extract features that can then be used to classify the rhythm on an ECG. Standard criteria to diagnose AF from an ECG are:\n",
    "- Required\n",
    "  - Absence of a visible P-wave\n",
    "  - Irregularly irregular heart rhythm\n",
    "- Optional\n",
    "  - Presence of f-waves / absence of isoelectric baseline\n",
    "  - Fast ventricular rate\n",
    "\n",
    "In this workshop we will focuse on crafting features related to heart rate variability, as the visibility of the P-wave is often limited in lead I, due to its low amplitude."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###Feature extraction using NeuroKit\n",
    "\n",
    "NeuroKit can be used to process ECG and compute features related to *heart rate variability* (HRV). See https://doi.org/10.3390/s21123998 for an overview of HRV features.\n",
    "\n",
    "The package computes features in 3 domains:\n",
    "- *time domain*: features based on variability in the heart rate intervals (R-R intervals)\n",
    "- *frequency domain*: frequency content of the R-R interval series, reflecting parasympathetic and sympathetic activity\n",
    "- *nonlinear dynamics*: features that try to capture the underlying dynamics of the R-R series. The Poincare plot is a well-known example of a nonlinear analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select an ECG in Normal Sinus Rhythm and one in AF and process them\n",
    "selected_sinus_indices = random.sample(sinus_indices, 1)\n",
    "selected_af_indices = random.sample(af_indices, 1)\n",
    "\n",
    "ecg_NSR = df.iloc[selected_sinus_indices[0]].astype(float)\n",
    "signals_NSR, info_NSR = nk.ecg_process(ecg_NSR, sampling_rate=sampling_frequency)\n",
    "\n",
    "# Visualise the processing\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "nk.ecg_plot(signals_NSR, info_NSR)\n",
    "\n",
    "ecg_AF = df.iloc[selected_af_indices[0]].astype(float)\n",
    "signals_AF, info_AF = nk.ecg_process(ecg_AF, sampling_rate=sampling_frequency)\n",
    "\n",
    "# Visualise the processing\n",
    "nk.ecg_plot(signals_AF, info_AF)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####**R-peaks**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find R-peaks\n",
    "peaks_NSR, info_NSR = nk.ecg_peaks(ecg_NSR, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)\n",
    "peaks_AF, info_AF = nk.ecg_peaks(ecg_AF, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Time-domain features"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Time domain features NSR\n",
    "hrv_time_NSR = nk.hrv_time(peaks_NSR, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_NSR"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Time domain features AF\n",
    "hrv_time_AF = nk.hrv_time(peaks_AF, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_AF"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TRAIN)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#FULL HRV feature extraction for all ECGs (TRAIN)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "hrv_features_train = []\n",
    "\n",
    "for i in tqdm(train_idx, desc=\"HRV (ALL FEATURES): TRAIN SET\"):\n",
    "    # Grab raw ECG\n",
    "    ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "    try:\n",
    "        # 1. Clean ECG\n",
    "        ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # 2. Detect R-peaks\n",
    "        peaks, _ = nk.ecg_peaks(\n",
    "            ecg_clean,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            correct_artifacts=True\n",
    "        )\n",
    "\n",
    "        # 3. Compute FULL HRV feature set\n",
    "        hrv_full = nk.hrv(\n",
    "            peaks,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            show=False\n",
    "        )\n",
    "\n",
    "        # Ensure row is a proper 1-row DataFrame and add ecg_index\n",
    "        hrv_full = hrv_full.copy()\n",
    "        hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "        hrv_features_train.append(hrv_full)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TRAIN ECG {i}: {e}\")\n",
    "\n",
    "        if hrv_features_train:\n",
    "            empty = pd.DataFrame(\n",
    "                [np.nan] * hrv_features_train[0].shape[1],\n",
    "                index=hrv_features_train[0].columns\n",
    "            ).T\n",
    "            empty[\"ecg_index\"] = i\n",
    "            hrv_features_train.append(empty)\n",
    "\n",
    "# Combine to single DataFrame\n",
    "hrv_train = pd.concat(hrv_features_train, ignore_index=True)\n",
    "\n",
    "print(\"hrv_train shape:\", hrv_train.shape)\n",
    "hrv_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove all columns from the dataframe that contain more than 50% NaN\n",
    "threshold = 0.5\n",
    "hrv_train_clean = hrv_train.dropna(thresh=len(hrv_train) * threshold, axis=1)\n",
    "\n",
    "# Remove all rows that are all NaN\n",
    "hrv_train_clean = hrv_train_clean.dropna(how='all')\n",
    "\n",
    "hrv_train_clean.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TEST)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hrv_features_test = []\n",
    "\n",
    "for i in tqdm(test_idx, desc=\"HRV (ALL FEATURES): TEST SET\"):\n",
    "    ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "    try:\n",
    "        # 1. Clean ECG\n",
    "        ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # 2. Detect R-peaks\n",
    "        peaks, _ = nk.ecg_peaks(\n",
    "            ecg_clean,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            correct_artifacts=True\n",
    "        )\n",
    "\n",
    "        # 3. Compute FULL HRV feature set\n",
    "        hrv_full = nk.hrv(\n",
    "            peaks,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            show=False\n",
    "        )\n",
    "\n",
    "        # Same as TRAIN: keep as 1-row DataFrame, add index\n",
    "        hrv_full = hrv_full.copy()\n",
    "        hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "        hrv_features_test.append(hrv_full)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TEST ECG {i}: {e}\")\n",
    "\n",
    "        if hrv_features_test:\n",
    "            empty = pd.DataFrame(\n",
    "                [np.nan] * hrv_features_test[0].shape[1],\n",
    "                index=hrv_features_test[0].columns\n",
    "            ).T\n",
    "            empty[\"ecg_index\"] = i\n",
    "            hrv_features_test.append(empty)\n",
    "\n",
    "hrv_test = pd.concat(hrv_features_test, ignore_index=True)\n",
    "\n",
    "print(\"hrv_test shape:\", hrv_test.shape)\n",
    "hrv_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###**Feature exploration**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge the HRV data with the rhythm labels\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on='ecg_index', right_index=True\n",
    ").reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "selectedMetric = 'HRV_MedianNN'\n",
    "rhythms = hrv_train_with_labels['label'].unique()\n",
    "for rhythm in rhythms:\n",
    "    subset = hrv_train_with_labels[hrv_train_with_labels['label'] == rhythm]\n",
    "    plt.hist(subset[selectedMetric], alpha=0.7, label=rhythm, bins='auto')\n",
    "\n",
    "plt.xlabel(selectedMetric)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution by Rhythm')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualisations"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correlation plot"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correlation heatmap\n",
    "corr_plot_hrv(hrv_train_clean)\n",
    "\n",
    "# Quick ranking: strongest HRV–AF correlations (top 10)\n",
    "hrv_only = hrv_train_with_labels[feature_cols + [\"classification\"]]\n",
    "corr_with_af = hrv_only.corr()[\"classification\"].sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 HRV features correlated with AF:\")\n",
    "print(corr_with_af.head(10))\n",
    "\n",
    "print(\"\\nBottom 10 (most negative):\")\n",
    "print(corr_with_af.tail(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution plots"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.stats as stats\n",
    "distplots(hrv_train_clean)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "distplots_hrv(hrv_train_clean)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##**Boxplots**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Boxplots\n",
    "boxplots_hrv(hrv_train_clean, [\"HRV_MedianNN\", \"HRV_SDNN\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##**Missingness**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Missingness\n",
    "check_missing_hrv(hrv_train_clean)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Outlier Detection**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to identify outliers in the data\n",
    "def identify_outliers(df, column_name, threshold=1.5):\n",
    "\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    row_indices = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)].index.tolist()\n",
    "    outlier_values = df.loc[row_indices, column_name].tolist()\n",
    "\n",
    "    return row_indices, outlier_values, lower_bound, upper_bound"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Outlier detection ONLY ON TRAIN\n",
    "\n",
    "# Merge labels with TRAIN features (cleaned hrv_train)\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on=\"ecg_index\", right_index=True\n",
    ")\n",
    "\n",
    "# Outlier detection ONLY on TRAIN\n",
    "train_outlier_idx, outlier_values, iqr_lower, iqr_upper = identify_outliers(\n",
    "    hrv_train_with_labels,\n",
    "    \"HRV_MedianNN\",\n",
    "    threshold=1.5\n",
    ")\n",
    "\n",
    "# ecg_index as (int)\n",
    "hrv_train_with_labels[\"ecg_index\"] = hrv_train_with_labels[\"ecg_index\"].astype(int)\n",
    "\n",
    "print(\"Train outliers detected:\", len(train_outlier_idx))\n",
    "print(\"Row indices (in hrv_train_with_labels) with outliers:\", train_outlier_idx)\n",
    "print(\"Outlier HRV_MedianNN values:\", outlier_values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualise one outlier ECG\n",
    "\n",
    "example_outlier_row = train_outlier_idx[0]\n",
    "\n",
    "# Single row\n",
    "row = hrv_train_with_labels.loc[example_outlier_row]\n",
    "\n",
    "# Extract ECG index value\n",
    "ecg_index_values = row.filter(like=\"ecg_index\").values\n",
    "\n",
    "# Use first value\n",
    "ecg_idx = int(ecg_index_values[0])\n",
    "\n",
    "# Extract raw ECG from df\n",
    "ecg_raw = df.iloc[ecg_idx].astype(float).values\n",
    "\n",
    "# Visualise R-Peaks\n",
    "peaks_outlier, info_outlier = nk.ecg_peaks(\n",
    "    ecg_raw,\n",
    "    sampling_rate=sampling_frequency,\n",
    "    correct_artifacts=True,\n",
    "    show=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hrv_train_with_labels.loc[example_outlier_row]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####**Outliers TEST set** done the same way as for TRAINING"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Align TEST columns to TRAIN columns\n",
    "\n",
    "# Align TEST columns to TRAIN columns (no leakage, same feature space)\n",
    "train_cols = hrv_train_clean.columns  # already cleaned on TRAIN\n",
    "shared_cols = [c for c in train_cols if c in hrv_test.columns]\n",
    "\n",
    "hrv_test_aligned = hrv_test[shared_cols].copy()\n",
    "\n",
    "# Merge TEST HRV with labels\n",
    "hrv_test_with_labels = pd.merge(\n",
    "    hrv_test_aligned,\n",
    "    df_labels[[\"label\", \"classification\"]],\n",
    "    left_on=\"ecg_index\",\n",
    "    right_index=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Same IQR bounds as on hrv_train\n",
    "\n",
    "Q1  = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3  = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_test_clean = hrv_test_with_labels[\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] >= lower_bound) &\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] <= upper_bound)\n",
    "].copy()\n",
    "\n",
    "print(\"hrv_test shape:\", hrv_test.shape)\n",
    "print(\"hrv_test_with_labels shape:\", hrv_test_with_labels.shape)\n",
    "print(\"hrv_test_clean shape:\", hrv_test_clean.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution TRAIN + TEST | Sanity check"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(hrv_train_clean.columns[:5])\n",
    "print(hrv_test_clean.columns[:5])\n",
    "print(hrv_test_clean[[\"HRV_MedianNN\", \"classification\"]].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for feat in [\"HRV_MedianNN\", \"HRV_SDNN\"]:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.kdeplot(\n",
    "        data=hrv_train_clean, x=feat, label=\"Train\", fill=True, common_norm=False\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=hrv_test_clean, x=feat, label=\"Test\", fill=True, common_norm=False, color=\"orange\"\n",
    "    )\n",
    "    plt.title(f\"{feat}: Train vs Test\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling TRAIN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Winsorising outliers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Q1 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_clip = Q1 - 1.5 * IQR\n",
    "upper_clip = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_train_winsor = hrv_train_with_labels.copy()\n",
    "hrv_train_winsor[\"HRV_MedianNN_winsor\"] = hrv_train_with_labels[\"HRV_MedianNN\"].clip(\n",
    "    lower=lower_clip, upper=upper_clip\n",
    ")\n",
    "\n",
    "print(\"Shape after winsorizing (same as original):\", hrv_train_winsor.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling Comparison"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(hrv_train_with_labels[\"HRV_MedianNN\"], kde=True, color=\"red\", label=\"Original\")\n",
    "sns.histplot(hrv_train_winsor[\"HRV_MedianNN_winsor\"], kde=True, color=\"green\", label=\"Winsorized\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Outlier Handling Comparison\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final Preprocessing: Building ML Matrices (X_train, X_test)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select HRV feature columns only\n",
    "feature_cols = [col for col in hrv_train_with_labels.columns\n",
    "                if col.startswith(\"HRV_\")]\n",
    "\n",
    "# TRAIN data\n",
    "x_train = hrv_train_with_labels[feature_cols].copy()\n",
    "y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "\n",
    "# TEST data\n",
    "x_test  = hrv_test_clean[feature_cols].copy()\n",
    "y_test  = hrv_test_clean[\"classification\"].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace +/- inf with NaN in both TRAIN and TEST\n",
    "for df_ in (x_train, x_test):\n",
    "    df_.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop columns that are all-NaN (if any)\n",
    "all_nan_cols = x_train.columns[x_train.isna().all()]\n",
    "if len(all_nan_cols) > 0:\n",
    "    print(\"Dropping all-NaN columns before imputation:\", list(all_nan_cols))\n",
    "    x_train.drop(columns=all_nan_cols, inplace=True)\n",
    "    x_test.drop(columns=all_nan_cols, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Median imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_imputed = imputer.fit_transform(x_train)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_imputed  = imputer.transform(x_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaling"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_scaled  = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert back to df with column names\n",
    "x_train = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "x_test  = pd.DataFrame(X_test_scaled, columns=feature_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Sanity checks**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Median X_train\n",
    "\n",
    "print(\"Median of scaled features (should be ~0):\")\n",
    "print(x_train.median().round(3))\n",
    "\n",
    "# IQR X_train\n",
    "print(\"\\nIQR of scaled features (should be ~1):\")\n",
    "print((x_train.quantile(0.75) - x_train.quantile(0.25)).round(3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Boxplot scaled feature distributions\n",
    "\n",
    "X_train_imputed_df = pd.DataFrame(X_train_imputed, columns=feature_cols)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(data=X_train_imputed_df, ax=axes[0])\n",
    "axes[0].set_title(\"Before Scaling (Imputed HRV Features)\")\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.boxplot(data=x_train, ax=axes[1])\n",
    "axes[1].set_title(\"After RobustScaler (X_train)\")\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final ML datasets (X_train, X_test, y_train, y_test"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    # Feature matrices (winsorised > imputation > scaling)\n",
    "    x_train = X_train_scaled\n",
    "    x_test  = X_test_scaled\n",
    "\n",
    "    # Target vectors (created earlier from HRV + labels AF(0/1))\n",
    "    y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "    y_test  = hrv_test_clean[\"classification\"].copy()\n",
    "\n",
    "    print(\"Final X_train shape:\", x_train.shape)\n",
    "    print(\"Final X_test shape:\", x_test.shape)\n",
    "    print(\"Final y_train shape:\", y_train.shape)\n",
    "    print(\"Final y_test shape:\", y_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Safety check"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert len(x_train) == len(y_train), \"Misaligned TRAIN matrix and labels!\"\n",
    "assert len(x_test) == len(y_test), \"Misaligned TEST matrix and labels!\"\n",
    "\n",
    "assert not np.isnan(x_train).any(), \"NaNs detected in X_train!\"\n",
    "assert not np.isnan(x_test).any(), \"NaNs detected in X_test!\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison framework"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resultsTable = pd.DataFrame(columns = ['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC', 'ROC_AUC', 'cm'])\n",
    "\n",
    "def modelResults(model, accuracy, f1, precision, recall, roc_auc, roc_cur, cm):\n",
    "    print(f\"Model {model} evaluated. \\nAccuracy: {accuracy} \\nF1 Score: {f1} \\nPrecision: {precision} \\nRecall: {recall} \\nROC AUC: {roc_auc}\")\n",
    "    resultsTable.loc[len(resultsTable)] = [model, accuracy, f1, precision, recall, roc_cur, roc_auc, cm]\n",
    "    resultsTable.to_csv(\"trainingResults.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"X train length: {len(x_train)}\\n X test length: {len(x_test)} \\n Y train length: {len(y_train)}\\n Y test length: {len(y_test)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Logistic Regression"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Whole"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_LR = LogisticRegression(multi_class='auto', max_iter=1000, class_weight='balanced')\n",
    "model_LR.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_LR.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_LR.classes_)\n",
    "disp.plot();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "modelResults(model_LR, accuracy, f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), roc_auc_score(y_test, y_pred), roc_curve(y_test, y_pred) , cm)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Select features"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selected_features = \"Insert features here\"\n",
    "model_LRL = str(model_LR) + f\" Selected features{selected_features}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Selected features logistic regression",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_names = hrv_train_with_labels.columns",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_RF = RandomForestClassifier(max_depth = 5, random_state = 3003)\n",
    "model_RF.fit(x_train, y_train)\n",
    "y_pred = model_RF.predict(x_test)\n",
    "\n",
    "_ = tree.plot_tree(model_RF.estimators_[0],\n",
    "                   feature_names=feature_names,\n",
    "                   class_names=['Normal rythm', 'Atrial fibrillation'],\n",
    "                   filled=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "modelResults(model_RF, accuracy, f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), roc_auc_score(y_test, y_pred), roc_curve(y_test, y_pred) , cm)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neighbours Classifiers\n",
    "### K Neighbours"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for n in range(15):\n",
    "    n = n+1\n",
    "    model_KNN = KNeighborsClassifier(n_neighbors=n)\n",
    "    model_KNN.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model_KNN.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    roc_cur = roc_curve(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_KNN.classes_)\n",
    "    disp.plot()\n",
    "\n",
    "    model_KNN_n = str(model_KNN) + str(f\" n={n}\")\n",
    "    modelResults(model_KNN_n, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Radius Neighbours"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for radius in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 5.0, 10.0]:\n",
    "    model_RNC = RadiusNeighborsClassifier(radius=radius, outlier_label='most_frequent')\n",
    "    model_RNC.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model_RNC.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    roc_cur = roc_curve(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_RNC.classes_)\n",
    "    disp.plot()\n",
    "\n",
    "    modelResults(model_RNC, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multi-layer Perceptron Classifier"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler_mlp = StandardScaler()\n",
    "x_train_scaled = scaler_mlp.fit_transform(x_train)\n",
    "x_test_scaled = scaler_mlp.transform(x_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_MLP = MLPClassifier(solver=\"lbfgs\", hidden_layer_sizes=(100, 50), max_iter=1000, random_state=3003)\n",
    "model_MLP.fit(x_train_scaled, y_train)\n",
    "\n",
    "y_pred = model_MLP.predict(x_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "roc_cur = roc_curve(y_test, y_pred)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_MLP.classes_)\n",
    "disp.plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "modelResults(model_MLP, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gradient Boosting Classifier"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_GBC = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=3003\n",
    ")\n",
    "\n",
    "model_GBC.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model_GBC.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "roc_cur = roc_curve(y_test, y_pred)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_GBC.classes_)\n",
    "disp.plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "modelResults(model_GBC, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model evaluation\n",
    "## Quick conclusion"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "resultsTable",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model = resultsTable.loc[resultsTable['F1 Score'].idxmax()]\n",
    "\n",
    "print(f\"The model with the highest F1 score was {best_model['Model']} with an F1 score of {best_model['F1 Score']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Graphs of numerical metrics\n",
    "### Logarithmic scale"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "numeric_metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "\n",
    "model_labels = [str(model).split('(')[0] for model in resultsTable['Model']]\n",
    "\n",
    "fig, axes = plt.subplots(len(numeric_metrics), 1, figsize=(10, len(numeric_metrics) * 8))\n",
    "\n",
    "for i, col in enumerate(numeric_metrics):\n",
    "    bars = axes[i].bar(range(len(resultsTable)), resultsTable[col])\n",
    "    axes[i].set_xticks(range(len(resultsTable)))\n",
    "    axes[i].set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "    axes[i].set_ylabel(col)\n",
    "    axes[i].set_title(f'{col} by Model')\n",
    "    axes[i].set_yscale('log')\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                     f'{resultsTable[col].iloc[j]:.3f}',\n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()fa"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "\n",
    "model_labels = [str(model).split('(')[0] for model in resultsTable['Model']]\n",
    "\n",
    "fig, axes = plt.subplots(len(numeric_metrics), 1, figsize=(10, len(numeric_metrics) * 8))\n",
    "\n",
    "for i, col in enumerate(numeric_metrics):\n",
    "    bars = axes[i].bar(range(len(resultsTable)), resultsTable[col])\n",
    "    axes[i].set_xticks(range(len(resultsTable)))\n",
    "    axes[i].set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "    axes[i].set_ylabel(col)\n",
    "    axes[i].set_title(f'{col} by Model')\n",
    "    axes[i].set_ylim(top=1)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                     f'{resultsTable[col].iloc[j]:.3f}',\n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ROC Curves"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for idx, row in resultsTable.iterrows():\n",
    "    model_name = str(row['Model']).split('(')[0]\n",
    "    fpr, tpr, thresholds = row['ROC']\n",
    "    roc_auc = row['ROC_AUC']\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=8)\n",
    "plt.grid(alpha=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Data-Witches",
   "name": "data-witches",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
