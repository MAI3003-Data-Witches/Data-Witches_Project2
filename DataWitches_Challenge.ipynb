{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data Witches**",
   "id": "dc59ac1af6732554"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| **Name**         | **Student ID** |\n",
    "|------------------|----------------|\n",
    "| Claessen, VVHJAE | i6339543       |\n",
    "| Ovsiannikova, AM | i6365923       |\n",
    "| Pubben, J        | i6276134       |\n",
    "| Roca Cugat, M    | i6351071       |\n",
    "| Záboj, J         | i6337952       |"
   ],
   "id": "a7b45f2690fd28b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Logbook**",
   "id": "41aece5caa34473"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Methods\n",
    "\n",
    "Let's ensure we all use the same names for all components.\n",
    "\n",
    "| **Variable**                 | **Name**                                      |\n",
    "|------------------------------|-----------------------------------------------|\n",
    "| Raw ECG dataframe            | df                                            |\n",
    "| Label dataframe              | df_labels                                     |\n",
    "| HRV features (train)         | hrv_train                                     |\n",
    "| HRV features (test)          | hrv_test                                      |\n",
    "| HRV extraction type          | FULL (nk.hrv — time + freq + nonlinear + RSA) |\n",
    "| Clean HRV dataframe (train)  | hrv_train_clean                               |\n",
    "| Clean HRV dataframe (test)   | hrv_test_clean                                |\n",
    "| HRV + labels (train)         | hrv_train_with_labels                         |\n",
    "| Winsorized HRV column        | HRV_MedianNN_winsor                           |\n",
    "| Model feature matrix (train) | X_train                                       |\n",
    "| Model feature matrix (test)  | X_test                                        |\n",
    "| Model target vector (train)  | y_train                                       |\n",
    "| Model target vector (test)   | y_test                                        |\n",
    "\n",
    "\n",
    "| **Function**              | **Description**                                | **Arguments**                                |\n",
    "|---------------------------|------------------------------------------------|----------------------------------------------|\n",
    "| corr_plot_hrv()           | Correlation plot for HRV features              | df, cols=None                                |\n",
    "| distplots_hrv()           | Distribution plots (hist + KDE)                | df, cols=None                                |\n",
    "| boxplots_hrv()            | Boxplots for selected HRV variables            | df, cols                                     |\n",
    "| check_missing_hrv()       | Missingness summary                            | df                                           |\n",
    "| identify_outliers()       | IQR-based outlier detection                    | df, column_name, threshold=1.5               |\n",
    "| model_evaluation()        | Confusion matrix + classification report       | model                                        |\n",
    "| model_desc()              | Accuracy, CV, ROC-AUC, model performance       | model                                        |\n"
   ],
   "id": "a28fc7429c87cc80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preface",
   "id": "b71e4ef40a858913"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Packages imports",
   "id": "25be65589590fcb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    print(\"Loading required packages...\")\n",
    "    import sys\n",
    "    import random\n",
    "    import os.path\n",
    "    import warnings\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import neurokit2 as nk\n",
    "    from scipy import stats\n",
    "    import scipy.signal as signal\n",
    "    from scipy.signal import welch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from joblib.testing import xfail\n",
    "    import plotly.graph_objects as go\n",
    "    from colorama import Fore, Back, Style\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import make_column_transformer\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "    from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "        f1_score, precision_score, recall_score, roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve\n",
    "    from sympy import false\n",
    "\n",
    "    print(\"Loading successful!\")\n",
    "except Exception:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install -r requirements.txt"
   ],
   "id": "a1c035c7772fa865",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Options settings",
   "id": "59e2fb78a188e3b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(3003)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DATA_PRESENT = os.path.isfile(\"data/Physionet2017Training.tar.xz\")"
   ],
   "id": "b5fc6e0801ceba25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset download",
   "id": "bbba3f2c07dd4e52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset_location = 'data/Physionet2017TrainingData.csv'",
   "id": "64d3019d4720d71a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not DATA_PRESENT:\n",
    "    !mkdir data\n",
    "    !wget https://github.com/MAI3003-Data-Witches/Data-Witches_Project2/raw/refs/heads/main/data/Physionet2017Training.tar.xz -O data/Physionet2017Training.tar.xz\n",
    "    !tar -xf data/Physionet2017Training.tar.xz -C data\n",
    "else:\n",
    "    print(f\"You already have the dataset downloaded at {dataset_location}, skipping\")"
   ],
   "id": "322a29c57caf6a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(dataset_location, header=None, index_col=False) * 1000  # Load the dataset already in mV\n",
    "\n",
    "df.head()"
   ],
   "id": "8194ca8419d6a2c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing\n",
    "## Extract ECG signals and class labels"
   ],
   "id": "d791f19d174d1807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_labels = pd.read_csv('data/Physionet2017TrainingLabels.csv', header=None, names=['label'])\n",
    "df_labels['classification'] = df_labels['label'].replace({\"N\": 0, \"A\": 1})\n",
    "df_labels['label'] = df_labels['label'].replace({\"N\": 'Normal Sinus Rhythm', \"A\": 'Atrial Fibrillation'})"
   ],
   "id": "df6051637427b19c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_labels",
   "id": "ea89a4141e557191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset splitting",
   "id": "7675a917e497d489"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_labeled = pd.merge(df_labels.drop(columns='label'), df, left_on='classification', right_index=True)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df.index,\n",
    "    test_size=0.2,\n",
    "    stratify=df_labels[\"label\"],\n",
    "    random_state=3003\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "id": "c5fced9de142ddf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "## Dataset characteristics"
   ],
   "id": "3828b0d61f36b49e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_ecgs = len(df)  # Number of ECGs\n",
    "\n",
    "num_samples = df.shape[1]  # Number of samples per ECG\n",
    "\n",
    "sampling_frequency = 300  #Hz\n",
    "duration = num_samples / sampling_frequency  # Duration of each ECG\n",
    "\n",
    "class_distribution = df_labels['label'].value_counts()  # Distribution over classes\n",
    "\n",
    "print(f\"Number of ECGs: {num_ecgs}\")\n",
    "print(f\"Number of samples per ECG: {num_samples}\")\n",
    "print(f\"Duration of each ECG: {duration} seconds\")\n",
    "print(f\"\\nClass Distribution:\\n{class_distribution}\")"
   ],
   "id": "84bd5e353e92a4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Indices per class (based on df_labels)\n",
    "sinus_indices = df_labels[df_labels[\"label\"] == \"Normal Sinus Rhythm\"].index.tolist()\n",
    "af_indices = df_labels[df_labels[\"label\"] == \"Atrial Fibrillation\"].index.tolist()\n",
    "\n",
    "example_sinus_idx = random.choice(sinus_indices)\n",
    "example_af_idx = random.choice(af_indices)\n",
    "\n",
    "ecg_sinus_raw = df.iloc[example_sinus_idx].astype(float).values\n",
    "ecg_af_raw = df.iloc[example_af_idx].astype(float).values\n",
    "\n",
    "time = np.arange(0, len(ecg_sinus_raw)) / sampling_frequency"
   ],
   "id": "29c63dc163550429",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Summary statistics for each ECG\n",
    "summary_stats = df.describe().T\n",
    "summary_stats = pd.concat([summary_stats, df_labels], axis=1)\n",
    "\n",
    "# Plotting the distributions of summary statistics\n",
    "stats_to_plot = ['mean', 'std', 'min', 'max', '25%', '75%']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()"
   ],
   "id": "b6b534d7e483039f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature extraction",
   "id": "fbfb192ded15f5a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ECG feature engineering",
   "id": "c27b648ef4075e99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select an ECG in Normal Sinus Rhythm and one in AF and process them\n",
    "selected_sinus_indices = random.sample(sinus_indices, 1)\n",
    "selected_af_indices = random.sample(af_indices, 1)\n",
    "\n",
    "ecg_NSR = df.iloc[selected_sinus_indices[0]].astype(float)\n",
    "signals_NSR, info_NSR = nk.ecg_process(ecg_NSR, sampling_rate=sampling_frequency)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "nk.ecg_plot(signals_NSR, info_NSR)\n",
    "\n",
    "ecg_AF = df.iloc[selected_af_indices[0]].astype(float)\n",
    "signals_AF, info_AF = nk.ecg_process(ecg_AF, sampling_rate=sampling_frequency)\n",
    "\n",
    "nk.ecg_plot(signals_AF, info_AF)"
   ],
   "id": "fd97ff94c0210770",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### R-peaks**",
   "id": "8f74f5d0762fab8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find R-peaks\n",
    "peaks_NSR, info_NSR = nk.ecg_peaks(ecg_NSR, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)\n",
    "peaks_AF, info_AF = nk.ecg_peaks(ecg_AF, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)"
   ],
   "id": "b5cd75b1c1a167e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Time-domain features",
   "id": "af2f35def467d115"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Time domain features NSR\n",
    "hrv_time_NSR = nk.hrv_time(peaks_NSR, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_NSR"
   ],
   "id": "abdd28449f7935bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Time domain features AF\n",
    "hrv_time_AF = nk.hrv_time(peaks_AF, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_AF"
   ],
   "id": "9fa3719508ed858d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TRAIN)",
   "id": "58c989c0300881b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Getting all the ECG readouts so we can extract P-wave information\n",
    "\n",
    "def get_ECG_readout():\n",
    "    test_run = 0\n",
    "    ecg_full = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for i in tqdm(train_idx):\n",
    "\n",
    "        ecg = df.iloc[i].astype(float)\n",
    "        signals, info = nk.ecg_process(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # Assign the current ecg_index to the signals DataFrame before concatenation\n",
    "        signals[\"ecg_index\"] = i\n",
    "\n",
    "        ecg_full = pd.concat([ecg_full, signals], ignore_index=True)\n",
    "\n",
    "        test_run += 1\n",
    "\n",
    "        if test_run == 10:\n",
    "            break  # Stop after 10 iterations for the example\n",
    "\n",
    "    return ecg_full"
   ],
   "id": "66181ce776e08427",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ecg_full = get_ECG_readout()",
   "id": "a2fbd64ac7af7c9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_ECG_metrics(ecg_full):\n",
    "    ecg_metrics_list = []\n",
    "\n",
    "    for i in tqdm(train_idx[:10]):\n",
    "        mean_quality = ecg_full.loc[ecg_full.ecg_index == i]['ECG_Quality'].mean()\n",
    "        mean_pwave_amplitude = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)][\n",
    "            'ECG_Clean'].mean()  #You could consider taking sqrt, mean and then **2\n",
    "        #(more robust) to outliers\n",
    "        stdev_pwave = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)]['ECG_Quality'].std()\n",
    "        #Perhaps I could add something about irregularly irregular rhythm, but it's (really) difficult mathematically\n",
    "        ecg_metrics_list.append({\n",
    "            'Mean_Quality': mean_quality,\n",
    "            'Mean_PWave_Amplitude': mean_pwave_amplitude,\n",
    "            'STDEV_Pwave': stdev_pwave,\n",
    "            'ecg_index': i\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(ecg_metrics_list)"
   ],
   "id": "9c1e52120f1c8ecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ecg_metrics = get_ECG_metrics(ecg_full)",
   "id": "89c6662ea882815a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#FULL HRV feature extraction for all ECGs (TRAIN)\n",
    "\n",
    "hrv_features_train = []\n",
    "\n",
    "for i in tqdm(train_idx, desc=\"HRV (ALL FEATURES): TRAIN SET\"):\n",
    "    # Grab raw ECG\n",
    "    ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "    try:\n",
    "        # 1. Clean ECG\n",
    "        ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # 2. Detect R-peaks\n",
    "        peaks, _ = nk.ecg_peaks(\n",
    "            ecg_clean,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            correct_artifacts=True\n",
    "        )\n",
    "\n",
    "        # 3. Compute FULL HRV feature set\n",
    "        hrv_full = nk.hrv(\n",
    "            peaks,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            show=False\n",
    "        )\n",
    "\n",
    "        # Ensure row is a proper 1-row DataFrame and add ecg_index\n",
    "        hrv_full = hrv_full.copy()\n",
    "        hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "        hrv_features_train.append(hrv_full)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TRAIN ECG {i}: {e}\")\n",
    "\n",
    "        if hrv_features_train:\n",
    "            empty = pd.DataFrame(\n",
    "                [np.nan] * hrv_features_train[0].shape[1],\n",
    "                index=hrv_features_train[0].columns\n",
    "            ).T\n",
    "            empty[\"ecg_index\"] = i\n",
    "            hrv_features_train.append(empty)\n",
    "\n",
    "# Combine to single DataFrame\n",
    "hrv_train = pd.concat(hrv_features_train, ignore_index=True)\n",
    "\n",
    "print(\"hrv_train shape:\", hrv_train.shape)\n",
    "hrv_train.head()"
   ],
   "id": "296fe0bbc2b51eb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge our new dataframe with our extra variables\n",
    "hrv_train = pd.merge(hrv_train, ecg_metrics, on='ecg_index', how='left')\n",
    "hrv_train.head()"
   ],
   "id": "735ec8e7cdeee43a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove all columns from the dataframe that contain more than 50% NaN\n",
    "threshold = 0.5\n",
    "hrv_train_clean = hrv_train.dropna(thresh=len(hrv_train) * threshold, axis=1)\n",
    "\n",
    "# Remove all rows that are all NaN\n",
    "hrv_train_clean = hrv_train_clean.dropna(how='all')\n",
    "\n",
    "hrv_train_clean.to_csv(\"hrv_train.csv\", index=False)\n",
    "hrv_train_clean.head()"
   ],
   "id": "2ba89cd6d4a3db0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TEST)",
   "id": "3d45a439ee1629c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hrv_features_test = []\n",
    "\n",
    "for i in tqdm(test_idx, desc=\"HRV (ALL FEATURES): TEST SET\"):\n",
    "    ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "    try:\n",
    "        # 1. Clean ECG\n",
    "        ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # 2. Detect R-peaks\n",
    "        peaks, _ = nk.ecg_peaks(\n",
    "            ecg_clean,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            correct_artifacts=True\n",
    "        )\n",
    "\n",
    "        # 3. Compute FULL HRV feature set\n",
    "        hrv_full = nk.hrv(\n",
    "            peaks,\n",
    "            sampling_rate=sampling_frequency,\n",
    "            show=False\n",
    "        )\n",
    "\n",
    "        # Same as TRAIN: keep as 1-row DataFrame, add index\n",
    "        hrv_full = hrv_full.copy()\n",
    "        hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "        hrv_features_test.append(hrv_full)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TEST ECG {i}: {e}\")\n",
    "\n",
    "        if hrv_features_test:\n",
    "            empty = pd.DataFrame(\n",
    "                [np.nan] * hrv_features_test[0].shape[1],\n",
    "                index=hrv_features_test[0].columns\n",
    "            ).T\n",
    "            empty[\"ecg_index\"] = i\n",
    "            hrv_features_test.append(empty)\n",
    "\n",
    "hrv_test = pd.concat(hrv_features_test, ignore_index=True)\n",
    "\n",
    "print(\"hrv_test shape:\", hrv_test.shape)\n",
    "hrv_test.head()"
   ],
   "id": "8901b24c6fcaaba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge our new dataframe with our extra variables\n",
    "hrv_test = pd.merge(hrv_test, ecg_metrics, on='ecg_index', how='left')\n",
    "hrv_test.head()"
   ],
   "id": "23d1c4f5c5a6e1a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove all columns from the dataframe that contain more than 50% NaN\n",
    "threshold = 0.5\n",
    "hrv_test_clean = hrv_test.dropna(thresh=len(hrv_test) * threshold, axis=1)\n",
    "\n",
    "# Remove all rows that are all NaN\n",
    "hrv_test_clean = hrv_test_clean.dropna(how='all')\n",
    "\n",
    "hrv_test_clean.head()\n",
    "\n",
    "hrv_test.to_csv(\"hrv_test.csv\", index=False)"
   ],
   "id": "f369f6128cff75af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature exploration",
   "id": "dcd63735dd3e1a14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge the HRV data with the rhythm labels\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on='ecg_index', right_index=True\n",
    ").reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "selectedMetric = 'HRV_MedianNN'\n",
    "rhythms = hrv_train_with_labels['label'].unique()\n",
    "for rhythm in rhythms:\n",
    "    subset = hrv_train_with_labels[hrv_train_with_labels['label'] == rhythm]\n",
    "    plt.hist(subset[selectedMetric], alpha=0.7, label=rhythm, bins='auto')\n",
    "\n",
    "plt.xlabel(selectedMetric)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution by Rhythm')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "ec48f78233e03aa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "84011cdd339fb0a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missingness",
   "id": "1fd7156d74494298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_missing_hrv(df):\n",
    "    \"\"\"\n",
    "    Summarize missingness across HRV features.\n",
    "    \"\"\"\n",
    "    missing = df.isna().sum()\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": df.columns,\n",
    "        \"missing_n\": missing,\n",
    "        \"missing_%\": (missing / len(df)) * 100\n",
    "    })\n",
    "    display(out.sort_values(\"missing_%\", ascending=False))\n",
    "    return out"
   ],
   "id": "85661f71d7d18bdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "check_missing_hrv(hrv_train_clean)",
   "id": "d610d38ba4561404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Outlier Detection",
   "id": "a00714e3ee6f401e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to identify outliers in the data\n",
    "def identify_outliers(df, column_name, threshold=1.5):\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    row_indices = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)].index.tolist()\n",
    "    outlier_values = df.loc[row_indices, column_name].tolist()\n",
    "\n",
    "    return row_indices, outlier_values, lower_bound, upper_bound"
   ],
   "id": "dc5d9abcf1bb4e03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Outlier detection ONLY ON TRAIN\n",
    "\n",
    "# Merge labels with TRAIN features (cleaned hrv_train)\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on=\"ecg_index\", right_index=True\n",
    ")\n",
    "\n",
    "# Outlier detection ONLY on TRAIN\n",
    "train_outlier_idx, outlier_values, iqr_lower, iqr_upper = identify_outliers(\n",
    "    hrv_train_with_labels,\n",
    "    \"HRV_MedianNN\",\n",
    "    threshold=1.5\n",
    ")\n",
    "\n",
    "# ecg_index as (int)\n",
    "hrv_train_with_labels[\"ecg_index\"] = hrv_train_with_labels[\"ecg_index\"].astype(int)\n",
    "\n",
    "print(\"Train outliers detected:\", len(train_outlier_idx))\n",
    "print(\"Row indices (in hrv_train_with_labels) with outliers:\", train_outlier_idx)\n",
    "print(\"Outlier HRV_MedianNN values:\", outlier_values)"
   ],
   "id": "7fa9774d200f7932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualise one outlier ECG\n",
    "\n",
    "example_outlier_row = train_outlier_idx[0]\n",
    "\n",
    "# Single row\n",
    "row = hrv_train_with_labels.loc[example_outlier_row]\n",
    "\n",
    "# Extract ECG index value\n",
    "ecg_index_values = row.filter(like=\"ecg_index\").values\n",
    "\n",
    "# Use first value\n",
    "ecg_idx = int(ecg_index_values[0])\n",
    "\n",
    "# Extract raw ECG from df\n",
    "ecg_raw = df.iloc[ecg_idx].astype(float).values\n",
    "\n",
    "# Visualise R-Peaks\n",
    "peaks_outlier, info_outlier = nk.ecg_peaks(\n",
    "    ecg_raw,\n",
    "    sampling_rate=sampling_frequency,\n",
    "    correct_artifacts=True,\n",
    "    show=True\n",
    ")"
   ],
   "id": "c7026677b951d4dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hrv_train_with_labels.loc[example_outlier_row]",
   "id": "6f7a3330073a16e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Outliers TEST set** done the same way as for TRAINING",
   "id": "76b35d498eed70a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Align TEST columns to TRAIN columns\n",
    "\n",
    "# Align TEST columns to TRAIN columns (no leakage, same feature space)\n",
    "train_cols = hrv_train_clean.columns  # already cleaned on TRAIN\n",
    "shared_cols = [c for c in train_cols if c in hrv_test.columns]\n",
    "\n",
    "hrv_test_aligned = hrv_test[shared_cols].copy()\n",
    "\n",
    "# Merge TEST HRV with labels\n",
    "hrv_test_with_labels = pd.merge(\n",
    "    hrv_test_aligned,\n",
    "    df_labels[[\"label\", \"classification\"]],\n",
    "    left_on=\"ecg_index\",\n",
    "    right_index=True\n",
    ")"
   ],
   "id": "d355c44c827365f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Same IQR bounds as on hrv_train\n",
    "\n",
    "Q1 = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3 = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_test_clean = hrv_test_with_labels[\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] >= lower_bound) &\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] <= upper_bound)\n",
    "    ].copy()\n",
    "\n",
    "print(\"hrv_test shape:\", hrv_test.shape)\n",
    "print(\"hrv_test_with_labels shape:\", hrv_test_with_labels.shape)\n",
    "print(\"hrv_test_clean shape:\", hrv_test_clean.shape)"
   ],
   "id": "97a81e7e91f3cb56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution TRAIN + TEST | Sanity check",
   "id": "e8f18f565bb691b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(hrv_train_clean.columns[:5])\n",
    "print(hrv_test_clean.columns[:5])\n",
    "print(hrv_test_clean[[\"HRV_MedianNN\", \"classification\"]].head())"
   ],
   "id": "95a28d78ac47363a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for feat in [\"HRV_MedianNN\", \"HRV_SDNN\"]:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.kdeplot(\n",
    "        data=hrv_train_clean, x=feat, label=\"Train\", fill=True, common_norm=False\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=hrv_test_clean, x=feat, label=\"Test\", fill=True, common_norm=False, color=\"orange\"\n",
    "    )\n",
    "    plt.title(f\"{feat}: Train vs Test\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "3c25fde41c2e9aa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling TRAIN",
   "id": "f5ba9ff7868a4a8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Winsorising outliers",
   "id": "cd2ff30ad886cc05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Q1 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_clip = Q1 - 1.5 * IQR\n",
    "upper_clip = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_train_winsor = hrv_train_with_labels.copy()\n",
    "hrv_train_winsor[\"HRV_MedianNN_winsor\"] = hrv_train_with_labels[\"HRV_MedianNN\"].clip(\n",
    "    lower=lower_clip, upper=upper_clip\n",
    ")\n",
    "\n",
    "print(\"Shape after winsorizing (same as original):\", hrv_train_winsor.shape)"
   ],
   "id": "5e5ef13507fe5518",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling Comparison",
   "id": "5784e0c94e4a6076"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(hrv_train_with_labels[\"HRV_MedianNN\"], kde=True, color=\"red\", label=\"Original\")\n",
    "sns.histplot(hrv_train_winsor[\"HRV_MedianNN_winsor\"], kde=True, color=\"green\", label=\"Winsorized\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Outlier Handling Comparison\")\n",
    "plt.show()"
   ],
   "id": "7e270ce4d4d3105d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final Preprocessing: Building ML Matrices (X_train, X_test)",
   "id": "8811d9407209d33f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select HRV feature columns only\n",
    "feature_cols = [col for col in hrv_train_with_labels.columns if col.startswith(\"HRV_\")]\n",
    "\n",
    "# TRAIN data\n",
    "x_train = hrv_train_with_labels[feature_cols].copy()\n",
    "y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "\n",
    "# TEST data\n",
    "x_test = hrv_test_clean[feature_cols].copy()\n",
    "y_test = hrv_test_clean[\"classification\"].copy()"
   ],
   "id": "808717ddc32befd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace +/- inf with NaN in both TRAIN and TEST\n",
    "for df_ in (x_train, x_test):\n",
    "    df_.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop columns that are all-NaN (if any)\n",
    "all_nan_cols = x_train.columns[x_train.isna().all()]\n",
    "if len(all_nan_cols) > 0:\n",
    "    print(\"Dropping all-NaN columns before imputation:\", list(all_nan_cols))\n",
    "    x_train.drop(columns=all_nan_cols, inplace=True)\n",
    "    x_test.drop(columns=all_nan_cols, inplace=True)"
   ],
   "id": "4205f6e90e740053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputation",
   "id": "f416990de42cd4a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Median imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_imputed = imputer.fit_transform(x_train)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_imputed = imputer.transform(x_test)"
   ],
   "id": "331970276ce7a777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalisation",
   "id": "15abcdeb289f5573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Temporarily convert to DataFrame to calculate Skewness easily\n",
    "temp_df = pd.DataFrame(X_train_imputed, columns=feature_cols)\n",
    "skewness = temp_df.skew().sort_values(ascending=False)\n",
    "\n",
    "#Identify skewed columns (Threshold > 1.0)\n",
    "skewed_cols = skewness[abs(skewness) > 1.0].index.tolist()\n",
    "\n",
    "#Apply Log Transform directly to the NumPy arrays\n",
    "for col_name in skewed_cols:\n",
    "    # Find the column index (integer position)\n",
    "    col_idx = feature_cols.index(col_name)\n",
    "\n",
    "    # Check for negative values (Log crashes on negatives)\n",
    "    # We find the global minimum for this column across Train and Test\n",
    "    min_val = min(X_train_imputed[:, col_idx].min(), X_test_imputed[:, col_idx].min())\n",
    "\n",
    "    shift = 0\n",
    "    if min_val < 0:\n",
    "        # If negatives exist, calculate a shift to make the minimum 0\n",
    "        shift = abs(min_val)\n",
    "\n",
    "    # Apply transformation in-place: Log(x + shift + 1)\n",
    "    X_train_imputed[:, col_idx] = np.log1p(X_train_imputed[:, col_idx] + shift)\n",
    "    X_test_imputed[:, col_idx] = np.log1p(X_test_imputed[:, col_idx] + shift)"
   ],
   "id": "c5d065ed84b20c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaling",
   "id": "51d3ff7d06fd93cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert back to df with column names\n",
    "x_train = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "x_test = pd.DataFrame(X_test_scaled, columns=feature_cols)"
   ],
   "id": "b86af43c55a5ba1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Sanity checks**",
   "id": "ec382f92da049c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Median X_train\n",
    "print(\"Median of scaled features (should be ~0):\")\n",
    "print(x_train.median().round(3))\n",
    "\n",
    "# IQR X_train\n",
    "print(\"\\nIQR of scaled features (should be ~1):\")\n",
    "print((x_train.quantile(0.75) - x_train.quantile(0.25)).round(3))\n",
    "\n",
    "#Checking skewness of the datasets\n",
    "skewness_train = x_train.skew().sort_values(ascending=False)\n",
    "skewness_test = x_train.skew().sort_values(ascending=False)\n",
    "# Filter for highly skewed columns (absolute skew > 1.0)\n",
    "high_skew_cols_train = skewness_train[abs(skewness_train) > 1.0]\n",
    "high_skew_cols_test = skewness_test[abs(skewness_test) > 1.0]\n",
    "\n",
    "print(len(high_skew_cols_train))\n",
    "print(len(high_skew_cols_test))"
   ],
   "id": "26f29c228cb64bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final ML datasets (X_train, X_test, y_train, y_test",
   "id": "2a36ba7605b95a6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "id": "92cc335720ca9288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    # Feature matrices (winsorised > imputation > scaling)\n",
    "    x_train = X_train_scaled\n",
    "    x_test = X_test_scaled\n",
    "\n",
    "    # Target vectors (created earlier from HRV + labels AF(0/1))\n",
    "    y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "    y_test = hrv_test_clean[\"classification\"].copy()\n",
    "\n",
    "    print(\"Final X_train shape:\", x_train.shape)\n",
    "    print(\"Final X_test shape:\", x_test.shape)\n",
    "    print(\"Final y_train shape:\", y_train.shape)\n",
    "    print(\"Final y_test shape:\", y_test.shape)"
   ],
   "id": "ec10523a1e004990",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training Setup",
   "id": "24142419a84363d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Safety check",
   "id": "12d1464a95c41b61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert len(x_train) == len(y_train), \"Misaligned TRAIN matrix and labels!\"\n",
    "assert len(x_test) == len(y_test), \"Misaligned TEST matrix and labels!\"\n",
    "\n",
    "assert not np.isnan(x_train).any(), \"NaNs detected in X_train!\"\n",
    "assert not np.isnan(x_test).any(), \"NaNs detected in X_test!\""
   ],
   "id": "fd31ca358371c93e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison framework",
   "id": "12e32109e5ee457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resultsTable = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC', 'ROC_AUC', 'cm'])\n",
    "\n",
    "\n",
    "def modelResults(model, accuracy, f1, precision, recall, roc_auc, roc_cur, cm):\n",
    "    print(\n",
    "        f\"Model {model} evaluated. \\nAccuracy: {accuracy} \\nF1 Score: {f1} \\nPrecision: {precision} \\nRecall: {recall} \\nROC AUC: {roc_auc}\")\n",
    "    resultsTable.loc[len(resultsTable)] = [model, accuracy, f1, precision, recall, roc_cur, roc_auc, cm]\n",
    "    resultsTable.to_csv(\"trainingResults.csv\", index=False, mode=\"a\")"
   ],
   "id": "37efe7aec3749c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"X train length: {len(x_train)}\\n X test length: {len(x_test)} \\n Y train length: {len(y_train)}\\n Y test length: {len(y_test)}\")"
   ],
   "id": "4886d8bf749e585f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training",
   "id": "50d9a88ad21d69c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Logistic Regression",
   "id": "baa89d245be08390"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### All features",
   "id": "2aff4bd4260f3a56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_LR = LogisticRegression(multi_class='auto', max_iter=1000, class_weight='balanced')\n",
    "model_LR.fit(x_train, y_train)\n",
    "\n",
    "y_pred_proba = model_LR.predict_proba(x_test)[:, 1]\n",
    "y_pred = model_LR.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "f1_score_baseline = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_LR.classes_)\n",
    "disp.plot();"
   ],
   "id": "d39faacbf3c21d4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "modelResults(model_LR, accuracy, f1_score(y_test, y_pred), precision_score(y_test, y_pred),\n",
    "             recall_score(y_test, y_pred), roc_auc_score(y_test, y_pred_proba), roc_curve(y_test, y_pred_proba), cm)\n"
   ],
   "id": "1daa5d3a8fc2eb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Select features",
   "id": "277ab9db6fe4951a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Check if training set is a numpy array\n",
    "if isinstance(x_train, np.ndarray):\n",
    "    x_train = pd.DataFrame(x_train, columns=feature_cols)\n",
    "    x_test = pd.DataFrame(x_test, columns=feature_cols)\n",
    "\n",
    "coeffs = np.abs(model_LR.coef_[0])\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': x_train.columns,\n",
    "    'Importance': coeffs\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "top_features = feature_importance_df['Feature'].head(10).tolist()\n",
    "print(top_features)"
   ],
   "id": "da98c343d2bb74a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Baseline LR f1-score: {f1_score_baseline:.4f}\")\n",
    "#Kept for reproducibility purposes\n",
    "\n",
    "if False:\n",
    "    for k in range(5, len(top_features) + 1):\n",
    "        top_k_features = top_features[:k]\n",
    "        lr_k = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=3003)\n",
    "        lr_k.fit(x_train[top_k_features], y_train)\n",
    "        y_pred = lr_k.predict(x_test[top_k_features])\n",
    "        y_pred_proba = lr_k.predict_proba(x_test[top_k_features])[:, 1]\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"Top {k} Features -> F1 Score: {f1:.4f}\")\n",
    "\n",
    "top_k_features = top_features[:6]\n",
    "lr_top10 = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=3003)\n",
    "lr_top10.fit(x_train[top_k_features], y_train)\n",
    "y_pred_top10 = lr_top10.predict(x_test[top_k_features])\n",
    "y_pred_top10_proba = lr_top10.predict_proba(x_test[top_k_features])[:, 1]\n",
    "modelResults(\n",
    "    \"LR (Top 10 Features)\",\n",
    "    accuracy_score(y_test, y_pred_top10),\n",
    "    f1_score(y_test, y_pred_top10),\n",
    "    precision_score(y_test, y_pred_top10),\n",
    "    recall_score(y_test, y_pred_top10),\n",
    "    roc_auc_score(y_test, y_pred_top10_proba),\n",
    "    roc_curve(y_test, y_pred_top10_proba),\n",
    "    confusion_matrix(y_test, y_pred_top10, normalize='true')\n",
    ")"
   ],
   "id": "3df3ade2641dcbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove features based on correlation",
   "id": "e612adf4612c76c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corr_matrix = x_train.corr().abs()\n",
    "# Select only the upper triangle of the correlation matrix. k=1 excludes the diagonal (self-correlation=1.0) so we don't accidentally delete every feature and ensuring we check each pair (A vs B) only once and ignore mirror duplicates (B vs A).\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "if False:\n",
    "    for threshold in [0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99]:\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "        features_keep = [f for f in x_train.columns if f not in to_drop]\n",
    "        lr_corr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=3003)\n",
    "        lr_corr.fit(x_train[features_keep], y_train)\n",
    "        y_pred = lr_corr.predict_proba(x_test[features_keep])\n",
    "        y_pred_cm = lr_corr.predict(x_test[features_keep])\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred_cm, average='binary')\n",
    "        rec = recall_score(y_test, y_pred_cm)\n",
    "        acc = accuracy_score(y_test, y_pred_cm)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_cm)\n",
    "        print(f\"Thresh {threshold} -> Dropped {len(to_drop)} features. F1: {f1:.4f} (ROC-AUC: {roc_auc:.4f})\")\n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "features_keep = [f for f in x_train.columns if f not in to_drop]\n",
    "lr_corr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=3003)\n",
    "lr_corr.fit(x_train[features_keep], y_train)\n",
    "y_pred_proba = lr_corr.predict_proba(x_test[features_keep])\n",
    "y_pred_cm = lr_corr.predict(x_test[features_keep])\n",
    "\n",
    "modelResults(\n",
    "    f\"LR_Corr (> {0.8})\",\n",
    "    accuracy_score(y_test, y_pred_cm), f1_score(y_test, y_pred_cm),\n",
    "    precision_score(y_test, y_pred_cm), recall_score(y_test, y_pred_cm),\n",
    "    roc_auc_score(y_test, y_pred_cm), roc_curve(y_test, y_pred_cm),\n",
    "    confusion_matrix(y_test, y_pred_cm, normalize='true')\n",
    ")"
   ],
   "id": "eb4fae99e171fc2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding features",
   "id": "2a3b47e5fbabef49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Normalising and computing new features",
   "id": "fbf5155ebf1112e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Going back to basics, the currently used x_train and x_test gave ValueErrors as negative values for Log\n",
    "\n",
    "raw_cols = [c for c in hrv_train_with_labels.columns if c.startswith(\"HRV_\")]\n",
    "raw_train = hrv_train_with_labels[raw_cols].copy()\n",
    "raw_test = hrv_test_clean[raw_cols].copy()\n",
    "\n",
    "raw_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "raw_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer_eng = SimpleImputer(strategy=\"median\")\n",
    "raw_train_imp = pd.DataFrame(imputer_eng.fit_transform(raw_train), columns=raw_cols)\n",
    "raw_test_imp = pd.DataFrame(imputer_eng.transform(raw_test), columns=raw_cols)\n",
    "\n",
    "skewness = raw_train_imp.skew().sort_values(ascending=False)\n",
    "skewed_cols = skewness[abs(skewness) > 1.0].index.tolist()\n",
    "\n",
    "new_features_train = pd.DataFrame(index=raw_train_imp.index)\n",
    "new_features_test = pd.DataFrame(index=raw_test_imp.index)\n",
    "\n",
    "#1. Log Transforms\n",
    "for col in skewed_cols:\n",
    "    # +1e-6 avoids log(0)\n",
    "    new_features_train[f'Log_{col}'] = np.log(raw_train_imp[col] + 1e-6)\n",
    "    new_features_test[f'Log_{col}'] = np.log(raw_test_imp[col] + 1e-6)\n",
    "\n",
    "#2. 2. Coefficient of Variation (CV) computation:\n",
    "if 'HRV_SDNN' in raw_train_imp.columns and 'HRV_MeanNN' in raw_train_imp.columns:\n",
    "    new_features_train['CV_SDNN'] = raw_train_imp['HRV_SDNN'] / (raw_train_imp['HRV_MeanNN'] + 1e-6)\n",
    "    new_features_test['CV_SDNN'] = raw_test_imp['HRV_SDNN'] / (raw_test_imp['HRV_MeanNN'] + 1e-6)\n",
    "\n",
    "# 3. Chaos Index (Amplifies the \"irregularly irregular\" signal specific to AF.):\n",
    "entropy_col = 'HRV_ApEn' if 'HRV_ApEn' in raw_train_imp.columns else 'HRV_SampEn'\n",
    "if 'HRV_RMSSD' in raw_train_imp.columns and entropy_col in raw_train_imp.columns:\n",
    "    new_features_train['Chaos_Index'] = raw_train_imp['HRV_RMSSD'] * raw_train_imp[entropy_col]\n",
    "    new_features_test['Chaos_Index'] = raw_test_imp['HRV_RMSSD'] * raw_test_imp[entropy_col]\n",
    "\n",
    "new_features_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "new_features_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer_new = SimpleImputer(strategy=\"median\")\n",
    "new_train_clean = pd.DataFrame(imputer_new.fit_transform(new_features_train), columns=new_features_train.columns)\n",
    "new_test_clean = pd.DataFrame(imputer_new.transform(new_features_test), columns=new_features_test.columns)\n",
    "\n",
    "scaler_eng = RobustScaler()\n",
    "new_train_scaled = pd.DataFrame(scaler_eng.fit_transform(new_train_clean), columns=new_features_train.columns)\n",
    "new_test_scaled = pd.DataFrame(scaler_eng.transform(new_test_clean), columns=new_features_test.columns)\n",
    "\n",
    "x_train_added = pd.concat([x_train.reset_index(drop=True), new_train_scaled.reset_index(drop=True)], axis=1)\n",
    "x_test_added = pd.concat([x_test.reset_index(drop=True), new_test_scaled.reset_index(drop=True)], axis=1)"
   ],
   "id": "b1269d6613448eee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### \"Use Everything\" Model",
   "id": "1cd3450bc01617b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr_aug = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=3003)\n",
    "lr_aug.fit(x_train_added, y_train)\n",
    "y_pred_aug_proba = lr_aug.predict_proba(x_test_added)[:, 1]\n",
    "y_pred_aug = lr_aug.predict(x_test_added)\n",
    "\n",
    "f1_aug = f1_score(y_test, y_pred_aug)\n",
    "print(f\"Augmented F1: {f1_aug:.4f}\")\n",
    "\n",
    "modelResults(\n",
    "    \"LR_Augmented (All Features)\",\n",
    "    accuracy_score(y_test, y_pred_aug), f1_aug,\n",
    "    precision_score(y_test, y_pred_aug), recall_score(y_test, y_pred_aug),\n",
    "    roc_auc_score(y_test, y_pred_aug_proba), roc_curve(y_test, y_pred_aug_proba),\n",
    "    confusion_matrix(y_test, y_pred_aug, normalize='true')\n",
    ")"
   ],
   "id": "bf9b1255a496b3c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### \"Drop Parents\" Model",
   "id": "a5d2291942ac5f36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parents_to_drop = set()\n",
    "\n",
    "# Dropping Skewed columns (We have Log_RMSSD, so drop RMSSD)\n",
    "parents_to_drop.update(skewed_cols)\n",
    "\n",
    "# Dropping Ratio Parents (We have CV_SDNN, so drop SDNN and MeanNN)\n",
    "if 'CV_SDNN' in new_features_train.columns:\n",
    "    parents_to_drop.update(['HRV_SDNN', 'HRV_MeanNN'])\n",
    "\n",
    "# Dropping Interaction Parents (We have Chaos, so drop RMSSD and Entropy)\n",
    "if 'Chaos_Index' in new_features_train.columns:\n",
    "    parents_to_drop.update(['HRV_RMSSD', entropy_col])\n",
    "\n",
    "features_to_keep = [f for f in x_train_added.columns if f not in parents_to_drop]\n",
    "\n",
    "lr_rep = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=3003)\n",
    "lr_rep.fit(x_train_added[features_to_keep], y_train)\n",
    "y_pred_rep_proba = lr_rep.predict_proba(x_test_added[features_to_keep])[:, 1]\n",
    "y_pred_rep = lr_rep.predict(x_test_added[features_to_keep])\n",
    "\n",
    "f1_rep = f1_score(y_test, y_pred_rep)\n",
    "print(f\"Replacement F1: {f1_rep:.4f}\")\n",
    "\n",
    "modelResults(\n",
    "    \"LR_Replacement (Drop Parents)\",\n",
    "    accuracy_score(y_test, y_pred_rep), f1_rep,\n",
    "    precision_score(y_test, y_pred_rep), recall_score(y_test, y_pred_rep),\n",
    "    roc_auc_score(y_test, y_pred_rep_proba), roc_curve(y_test, y_pred_rep_proba),\n",
    "    confusion_matrix(y_test, y_pred_rep, normalize='true')\n",
    ")"
   ],
   "id": "9898fcb7af798272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest",
   "id": "18c5d7b4b572913b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_names = hrv_train_with_labels.columns",
   "id": "3fcdecf389d5a251",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for n in range(14):\n",
    "    n = n + 1\n",
    "    model_RF = RandomForestClassifier(max_depth=n, random_state=3003)\n",
    "    model_RF.fit(x_train, y_train)\n",
    "    y_pred_proba = model_RF.predict_proba(x_test)[:, 1]\n",
    "    y_pred = model_RF.predict(x_test)\n",
    "\n",
    "    modelResults(model_RF, accuracy, f1_score(y_test, y_pred), precision_score(y_test, y_pred),\n",
    "                 recall_score(y_test, y_pred), roc_auc_score(y_test, y_pred_proba), roc_curve(y_test, y_pred_proba), cm)"
   ],
   "id": "59a7a1e0965a7f3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_ = tree.plot_tree(model_RF.estimators_[0],\n",
    "                   feature_names=feature_names,\n",
    "                   class_names=['Normal rythm', 'Atrial fibrillation'],\n",
    "                   filled=True)"
   ],
   "id": "37982cbe53478cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neighbours Classifiers\n",
    "### K Neighbours"
   ],
   "id": "b2aa179ebe7bfde0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for n in range(15):\n",
    "    n = n + 1\n",
    "    model_KNN = KNeighborsClassifier(n_neighbors=n)\n",
    "    model_KNN.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_proba = model_KNN.predict_proba(x_test)[:, 1]\n",
    "    y_pred = model_KNN.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_KNN.classes_)\n",
    "    disp.plot()\n",
    "\n",
    "    model_KNN_n = str(model_KNN) + str(f\" n={n}\")\n",
    "    modelResults(model_KNN_n, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "id": "99cb9dd8507bd317",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Radius Neighbours",
   "id": "75a1d7dbea20aaf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for radius in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 5.0, 10.0]:\n",
    "    model_RNC = RadiusNeighborsClassifier(radius=radius, outlier_label='most_frequent')\n",
    "    model_RNC.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_proba = model_RNC.predict_proba(x_test)[:, 1]\n",
    "    y_pred = model_RNC.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_RNC.classes_)\n",
    "    disp.plot()\n",
    "\n",
    "    modelResults(model_RNC, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "id": "2a236643cdf3493a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nearest Centroid Classifier",
   "id": "10e02d638c4def62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "model_NCC = NearestCentroid()\n",
    "model_NCC.fit(x_train, y_train)\n",
    "\n",
    "y_pred_proba = model_NCC.predict_proba(x_test)[:, 1]\n",
    "y_pred = model_NCC.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_NCC.classes_)\n",
    "disp.plot()\n",
    "\n",
    "modelResults(model_NCC, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "id": "710a2fb3142ebc6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multi-layer Perceptron Classifier",
   "id": "c265a72133e2e92f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler_mlp = StandardScaler()\n",
    "x_train_scaled = scaler_mlp.fit_transform(x_train)\n",
    "x_test_scaled = scaler_mlp.transform(x_test)"
   ],
   "id": "9f5f1a45cce41ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "##TODO: other solvers? other hidden layer sizes? other max_iter?",
   "id": "2147a10b3f91c1aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_MLP = MLPClassifier(solver=\"lbfgs\", hidden_layer_sizes=(100, 50), max_iter=1000, random_state=3003)\n",
    "model_MLP.fit(x_train_scaled, y_train)\n",
    "\n",
    "y_pred_proba = model_MLP.predict_proba(x_test_scaled)[:, 1]\n",
    "y_pred = model_MLP.predict(x_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_MLP.classes_)\n",
    "disp.plot()"
   ],
   "id": "f1bb2eaad1e66220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "modelResults(model_MLP, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)",
   "id": "8830baf2e29bb840",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gradient Boosting Classifier",
   "id": "2a98bc47be3aa8ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_GBC = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=3003\n",
    ")\n",
    "\n",
    "model_GBC.fit(x_train, y_train)\n",
    "\n",
    "y_pred_proba = model_GBC.predict_proba(x_test)[:, 1]\n",
    "y_pred = model_GBC.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_GBC.classes_)\n",
    "disp.plot()"
   ],
   "id": "940df4a517d506a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "modelResults(model_GBC, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)",
   "id": "eb6c5505be68a785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AdaBoost Classifier",
   "id": "2b8389ea80d93d01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TODO: implement testing\n",
    "for n in [50, 100, 200, 300, 400, 500, 1000, 2000, 5000, 10000]:\n",
    "    clf = AdaBoostClassifier(n_estimators=n, random_state=3003)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_pred_proba = clf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "    model_name = f\"AdaBoost (n_estimators={n})\"\n",
    "    modelResults(model_name, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)\n"
   ],
   "id": "20f6b5f2346446fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Soft voting classifier",
   "id": "97a1a9262acb8046"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voters = [\n",
    "    (\"lr\", LogisticRegression(multi_class='auto', max_iter=1000, class_weight='balanced', random_state=3003)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=300, max_depth=None, random_state=3003)),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=7)),\n",
    "    (\"ada\", AdaBoostClassifier(n_estimators=1000, random_state=3003))\n",
    "]\n",
    "\n",
    "soft_vote = VotingClassifier(estimators=voters, voting=\"soft\", n_jobs=-1)\n",
    "soft_vote.fit(x_train, y_train)\n",
    "\n",
    "y_pred_proba = soft_vote.predict_proba(x_test)[:, 1]\n",
    "y_pred = soft_vote.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=soft_vote.classes_)\n",
    "disp.plot()\n",
    "\n",
    "modelResults(soft_vote, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)\n"
   ],
   "id": "984ec50d4781dd20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gradient-boosted trees",
   "id": "2ae2c6e56a7bb31d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, \\\n",
    "    confusion_matrix, classification_report\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=3003)\n",
    "gbc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = gbc.predict(x_test)\n",
    "y_pred_proba = gbc.predict_proba(x_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gbc.classes_)\n",
    "disp.plot()\n",
    "\n",
    "modelResults(gbc, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "id": "3ffd1527e8e5ae9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 400],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [2, 3, 4],\n",
    "    \"subsample\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gbc_base = GradientBoostingClassifier(random_state=3003)\n",
    "cv = GridSearchCV(gbc_base, param_grid, scoring=\"f1\", cv=5, n_jobs=-1, refit=True)\n",
    "cv.fit(x_train, y_train)\n",
    "\n",
    "best_gbc = cv.best_estimator_\n",
    "print(\"Best params:\", cv.best_params_)\n",
    "\n",
    "y_pred = best_gbc.predict(x_test)\n",
    "y_pred_proba = best_gbc.predict_proba(x_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_gbc.classes_)\n",
    "disp.plot()\n",
    "\n",
    "modelResults(best_gbc, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "id": "7dd229321a089bc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Just gonna play around a bit with voting classifiers",
   "id": "48a56fa0c0d76acd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "voters = [\n",
    "    (\"lr\", LogisticRegression(multi_class='auto', max_iter=1000, class_weight='balanced', random_state=3003)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=1000, class_weight='balanced', random_state=3003)),\n",
    "    (\"ada\", AdaBoostClassifier(n_estimators=1000, random_state=3003)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "]\n",
    "\n",
    "soft_vote = VotingClassifier(estimators=voters, voting=\"soft\", n_jobs=-1)\n",
    "soft_vote.fit(x_train, y_train)\n",
    "\n",
    "y_pred_proba = soft_vote.predict_proba(x_test)[:, 1]\n",
    "y_pred = soft_vote.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=soft_vote.classes_)\n",
    "disp.plot()\n",
    "\n",
    "modelResults(soft_vote, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)"
   ],
   "id": "69b156724b68cc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best Voting Classifier Search",
   "id": "68136b826462600d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "# Define the pool of base classifiers with various hyperparameters\n",
    "classifiers_pool = [\n",
    "    ('LR_balanced', LogisticRegression(multi_class='auto', max_iter=1000, class_weight='balanced', random_state=3003)),\n",
    "    ('LR_default', LogisticRegression(multi_class='auto', max_iter=1000, random_state=3003)),\n",
    "    ('RF_100', RandomForestClassifier(n_estimators=100, random_state=3003)),\n",
    "    ('RF_200', RandomForestClassifier(n_estimators=200, random_state=3003)),\n",
    "    ('KNN_5', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('KNN_7', KNeighborsClassifier(n_neighbors=7)),\n",
    "    ('GBC_100', GradientBoostingClassifier(n_estimators=100, random_state=3003)),\n",
    "    ('GBC_200', GradientBoostingClassifier(n_estimators=200, random_state=3003)),\n",
    "    ('Ada_50', AdaBoostClassifier(n_estimators=50, random_state=3003)),\n",
    "    ('Ada_100', AdaBoostClassifier(n_estimators=100, random_state=3003))\n",
    "]\n",
    "\n",
    "best_f1_found = -1\n",
    "best_voting_model = None\n",
    "best_ensemble_name = \"\"\n",
    "\n",
    "print(\"Searching for best Voting Classifier configuration (optimizing for F1 Score)...\")\n",
    "\n",
    "# Iterate through all possible combinations of length 2 to 4 (limiting to 4 to avoid too long runtime)\n",
    "for r in range(2, 5):\n",
    "    for ensemble in itertools.combinations(classifiers_pool, r):\n",
    "        # Create a name for this combination\n",
    "        names = [name for name, _ in ensemble]\n",
    "        ensemble_name = f\"BestVote ({'+'.join(names)})\"\n",
    "\n",
    "        # Create the voting classifier\n",
    "        # Using soft voting as these models support probability estimates\n",
    "        voter = VotingClassifier(estimators=list(ensemble), voting='soft', n_jobs=-1)\n",
    "\n",
    "        # Train\n",
    "        voter.fit(x_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = voter.predict(x_test)\n",
    "        y_pred_proba = voter.predict_proba(x_test)[:, 1]\n",
    "\n",
    "        # Evaluate\n",
    "        current_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "        modelResults(ensemble, accuracy, current_f1, precision, recall, roc_auc, roc_cur, cm)\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Tested {ensemble_name}: F1 Score = {current_f1:.4f}\")\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "        modelResults(ensemble_name, accuracy, current_f1, precision, recall, roc_auc, roc_cur, cm)\n",
    "\n",
    "        if current_f1 > best_f1_found:\n",
    "            best_f1_found = current_f1\n",
    "            best_voting_model = voter\n",
    "            best_ensemble_name = ensemble_name\n",
    "\n",
    "print(f\"\\nWinner configuration: {best_ensemble_name} with F1 Score: {best_f1_found:.4f}\")\n",
    "\n",
    "# Log the best result to the global resultsTable\n",
    "if best_voting_model:\n",
    "    y_pred = best_voting_model.predict(x_test)\n",
    "    y_pred_proba = best_voting_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "    # Plot matrix for the winner\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_voting_model.classes_)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Best Found Ensemble: {best_ensemble_name}\")\n",
    "    plt.show()"
   ],
   "id": "6f869647ed8d4fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier, BaggingClassifier,\n",
    "                              ExtraTreesClassifier, HistGradientBoostingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (f1_score, accuracy_score, precision_score,\n",
    "                             recall_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay)\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Filter warnings for cleaner output during the sweep\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define a diverse list of classifiers to test\n",
    "classifiers_to_test = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000, class_weight='balanced', random_state=3003)),\n",
    "    (\"Ridge Classifier\", RidgeClassifier(class_weight='balanced', random_state=3003)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(class_weight='balanced', random_state=3003)),\n",
    "    (\"Random Forest\", RandomForestClassifier(class_weight='balanced', random_state=3003)),\n",
    "    (\"Extra Trees\", ExtraTreesClassifier(class_weight='balanced', random_state=3003)),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=3003)),\n",
    "    (\"AdaBoost\", AdaBoostClassifier(random_state=3003)),\n",
    "    (\"Hist Gradient Boosting\", HistGradientBoostingClassifier(random_state=3003)),\n",
    "    (\"SVC (Prob)\", SVC(probability=True, class_weight='balanced', random_state=3003)),\n",
    "    (\"Gaussian NB\", GaussianNB()),\n",
    "    (\"Bernoulli NB\", BernoulliNB()),\n",
    "    (\"KNN (k=5)\", KNeighborsClassifier(n_neighbors=5)),\n",
    "    (\"LDA\", LinearDiscriminantAnalysis()),\n",
    "    (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "    (\"MLP\", MLPClassifier(max_iter=1000, random_state=3003))\n",
    "]\n",
    "\n",
    "best_score = -1\n",
    "best_model_name = None\n",
    "best_model_obj = None\n",
    "\n",
    "print(f\"{'Model':<25} | {'F1 Score'}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for name, clf in classifiers_to_test:\n",
    "    try:\n",
    "        # Train\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = clf.predict(x_test)\n",
    "\n",
    "        # Evaluate\n",
    "        score = f1_score(y_test, y_pred)\n",
    "        print(f\"{name:<25} | {score:.4f}\")\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "        modelResults(ensemble_name, accuracy, score, precision, recall, roc_auc, roc_cur, cm)\n",
    "\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model_name = name\n",
    "            best_model_obj = clf\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{name:<25} | Failed ({str(e)[:20]}...)\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Winner: {best_model_name} with F1 Score: {best_score:.4f}\")\n",
    "\n",
    "# --- Log the Best Model ---\n",
    "if best_model_obj is not None:\n",
    "    # Generate final predictions\n",
    "    y_pred = best_model_obj.predict(x_test)\n",
    "\n",
    "    # Get probabilities for ROC (handle models that don't have predict_proba)\n",
    "    if hasattr(best_model_obj, \"predict_proba\"):\n",
    "        y_pred_proba = best_model_obj.predict_proba(x_test)[:, 1]\n",
    "    elif hasattr(best_model_obj, \"decision_function\"):\n",
    "        y_pred_proba = best_model_obj.decision_function(x_test)\n",
    "    else:\n",
    "        # Fallback for models like RidgeClassifier without helper methods\n",
    "        y_pred_proba = y_pred\n",
    "\n",
    "        # Calculate all required metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "\n",
    "    try:\n",
    "        rauc = roc_auc_score(y_test, y_pred_proba)\n",
    "        rcur = roc_curve(y_test, y_pred_proba)\n",
    "    except ValueError:\n",
    "        rauc = 0.5\n",
    "        rcur = ([0, 1], [0, 1], [0])\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "    # Log using the provided function\n",
    "    log_name = f\"Sweep Winner ({best_model_name})\"\n",
    "    modelResults(log_name, acc, best_score, prec, rec, rauc, rcur, cm)\n",
    "\n",
    "    # Visualize Confusion Matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model_obj.classes_)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Confusion Matrix: {log_name}\")\n",
    "    plt.show()\n"
   ],
   "id": "b898ce310383f69e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model evaluation\n",
    "## Quick conclusion"
   ],
   "id": "e2078b02730ef972"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "original_size = resultsTable.shape[0]\n",
    "resultsTable = resultsTable.drop_duplicates(subset=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC'])\n",
    "print(f\"Dropped {original_size - resultsTable.shape[0]} duplicate rows\")"
   ],
   "id": "a74e735f580bc275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "resultsTable",
   "id": "22fb0d0456fb3e19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_models = resultsTable.sort_values(by='F1 Score', ascending=False).head(5)\n",
    "\n",
    "print(\"Top 5 models based on F1 score:\")\n",
    "for i, (_, row) in enumerate(top_models.iterrows(), 1):\n",
    "    print(f\"{i}. {row['Model']} with an F1 score of {row['F1 Score']:.4f}\")\n"
   ],
   "id": "c38fea59ebb09553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Graphs of numerical metrics\n",
    "### Logarithmic scale"
   ],
   "id": "e8e285a0ca7eadd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    numeric_metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "\n",
    "    model_labels = [str(model).split('(')[0] for model in resultsTable['Model']]\n",
    "\n",
    "    fig, axes = plt.subplots(len(numeric_metrics), 1, figsize=(10, len(numeric_metrics) * 8))\n",
    "\n",
    "    for i, col in enumerate(numeric_metrics):\n",
    "        bars = axes[i].bar(range(len(resultsTable)), resultsTable[col])\n",
    "        axes[i].set_xticks(range(len(resultsTable)))\n",
    "        axes[i].set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "        axes[i].set_ylabel(col)\n",
    "        axes[i].set_title(f'{col} by Model')\n",
    "        axes[i].set_yscale('log')\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "        for j, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axes[i].text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                         f'{resultsTable[col].iloc[j]:.3f}',\n",
    "                         ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "fb7c2b82a89e3bf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "\n",
    "model_labels = [str(model).split('(')[0] for model in resultsTable['Model']]\n",
    "\n",
    "fig, axes = plt.subplots(len(numeric_metrics), 1, figsize=(10, len(numeric_metrics) * 8))\n",
    "\n",
    "for i, col in enumerate(numeric_metrics):\n",
    "    bars = axes[i].bar(range(len(resultsTable)), resultsTable[col])\n",
    "    axes[i].set_xticks(range(len(resultsTable)))\n",
    "    axes[i].set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "    axes[i].set_ylabel(col)\n",
    "    axes[i].set_title(f'{col} by Model')\n",
    "    axes[i].set_ylim(top=1)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                     f'{resultsTable[col].iloc[j]:.3f}',\n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1738534b97511a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ROC Curves",
   "id": "2f15fb5d2e241296"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for idx, row in resultsTable.iterrows():\n",
    "    model_name = str(row['Model']).split('(')[0]\n",
    "    fpr, tpr, thresholds = row['ROC']\n",
    "    roc_auc = row['ROC_AUC']\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(alpha=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fd4a03846706aed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "resultsTable.to_csv(\"trainingResults.csv\", index=False, mode=\"a\")",
   "id": "85a39e8631d71e62",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "data-witches",
   "language": "python",
   "display_name": "Data-Witches"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
