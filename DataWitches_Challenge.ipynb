{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/MAI3003-Data-Witches/AtrialFibrillation-detection/blob/challenge/DataWitches_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "id": "454dfd4a5cdb37b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data Witches**",
   "id": "8eb2b2d669137b67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| **Name**         | **Student ID** |\n",
    "|------------------|----------------|\n",
    "| Claessen, VVHJAE | i6339543       |\n",
    "| Ovsiannikova, AM | i6365923       |\n",
    "| Pubben, J        | i6276134       |\n",
    "| Roca Cugat, M    | i6351071       |\n",
    "| Záboj, J         | i6337952       |"
   ],
   "id": "1b60a3d40f2c345e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Logbook**",
   "id": "2a153880f0a7b7e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Methods\n",
    "\n",
    "Let's ensure we all use the same names for all components.\n",
    "\n",
    "| **Variable**                 | **Name**                                      |\n",
    "|------------------------------|-----------------------------------------------|\n",
    "| Raw ECG dataframe            | df                                            |\n",
    "| Label dataframe              | df_labels                                     |\n",
    "| HRV features (train)         | hrv_train                                     |\n",
    "| HRV features (test)          | hrv_test                                      |\n",
    "| HRV extraction type          | FULL (nk.hrv — time + freq + nonlinear + RSA) |\n",
    "| Clean HRV dataframe (train)  | hrv_train_clean                               |\n",
    "| Clean HRV dataframe (test)   | hrv_test_clean                                |\n",
    "| HRV + labels (train)         | hrv_train_with_labels                         |\n",
    "| Winsorized HRV column        | HRV_MedianNN_winsor                           |\n",
    "| Model feature matrix (train) | X_train                                       |\n",
    "| Model feature matrix (test)  | X_test                                        |\n",
    "| Model target vector (train)  | y_train                                       |\n",
    "| Model target vector (test)   | y_test                                        |\n",
    "\n",
    "\n",
    "| **Function**              | **Description**                                | **Arguments**                                |\n",
    "|---------------------------|------------------------------------------------|----------------------------------------------|\n",
    "| corr_plot_hrv()           | Correlation plot for HRV features              | df, cols=None                                |\n",
    "| distplots_hrv()           | Distribution plots (hist + KDE)                | df, cols=None                                |\n",
    "| boxplots_hrv()            | Boxplots for selected HRV variables            | df, cols                                     |\n",
    "| check_missing_hrv()       | Missingness summary                            | df                                           |\n",
    "| identify_outliers()       | IQR-based outlier detection                    | df, column_name, threshold=1.5               |\n",
    "| model_evaluation()        | Confusion matrix + classification report       | model                                        |\n",
    "| model_desc()              | Accuracy, CV, ROC-AUC, model performance       | model                                        |\n"
   ],
   "id": "788d06f5981fa822"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preface",
   "id": "de8a99ecaf1dde42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Packages imports",
   "id": "a02035f66576f866"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    print(\"Loading required packages...\")\n",
    "    import sys\n",
    "    import random\n",
    "    import os.path\n",
    "    import warnings\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import neurokit2 as nk\n",
    "    from scipy import stats\n",
    "    import scipy.signal as signal\n",
    "    from scipy.signal import welch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from joblib.testing import xfail\n",
    "    import plotly.graph_objects as go\n",
    "    from colorama import Fore, Back, Style\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import make_column_transformer\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "    from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "        f1_score, precision_score, recall_score, roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve\n",
    "    from sympy import false\n",
    "\n",
    "    print(\"Loading successful!\")\n",
    "except Exception:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install -r https://raw.githubusercontent.com/MAI3003-Data-Witches/AtrialFibrillation-detection/refs/heads/challenge/requirements.txt\n",
    "\n",
    "    print(\"Loading required packages...\")\n",
    "    import sys\n",
    "    import random\n",
    "    import os.path\n",
    "    import warnings\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import neurokit2 as nk\n",
    "    from scipy import stats\n",
    "    import scipy.signal as signal\n",
    "    from scipy.signal import welch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from joblib.testing import xfail\n",
    "    import plotly.graph_objects as go\n",
    "    from colorama import Fore, Back, Style\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import make_column_transformer\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "    from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "        f1_score, precision_score, recall_score, roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve\n",
    "    from sympy import false\n",
    "\n",
    "    print(\"Loading successful!\")"
   ],
   "id": "980ea8b07fe8ad6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Options settings",
   "id": "30431263864f4d72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(3003)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DATA_PRESENT = os.path.isfile(\"data/Physionet2017TrainingData.csv\")\n",
    "LoadPremadeDataset = False"
   ],
   "id": "9606c91eec100dd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset download",
   "id": "9ab59dbb9b8bcb3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset_location = 'data/Physionet2017TrainingData.csv'",
   "id": "5a603d83f07ed817"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not DATA_PRESENT:\n",
    "    !mkdir data\n",
    "    !wget https://github.com/MAI3003-Data-Witches/Data-Witches_Project2/raw/refs/heads/main/data/Physionet2017Training.tar.xz -O data/Physionet2017Training.tar.xz\n",
    "    !tar -xf data/Physionet2017Training.tar.xz -C data\n",
    "else:\n",
    "    print(f\"You already have the dataset downloaded at {dataset_location}, skipping\")"
   ],
   "id": "9e6e3bad153e380d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(dataset_location, header=None, index_col=False) * 1000  # Load the dataset already in mV\n",
    "\n",
    "df.head()"
   ],
   "id": "aa5d0d45fe962cee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing\n",
    "## Extract ECG signals and class labels"
   ],
   "id": "1fd38b5ea6ae3785"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_labels = pd.read_csv('data/Physionet2017TrainingLabels.csv', header=None, names=['label'])\n",
    "df_labels['classification'] = df_labels['label'].replace({\"N\": 0, \"A\": 1})\n",
    "df_labels['label'] = df_labels['label'].replace({\"N\": 'Normal Sinus Rhythm', \"A\": 'Atrial Fibrillation'})"
   ],
   "id": "511d49c4c4d89eab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_labels",
   "id": "3af06a2072417490"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset splitting",
   "id": "39a2649c9e91c8d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_labeled = pd.merge(df_labels.drop(columns='label'), df, left_on='classification', right_index=True)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df.index,\n",
    "    test_size=0.2,\n",
    "    stratify=df_labels[\"label\"],\n",
    "    random_state=3003\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "id": "110e77e8add6597b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "## Dataset characteristics"
   ],
   "id": "f3b3a5a614aefd07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_ecgs = len(df)  # Number of ECGs\n",
    "\n",
    "num_samples = df.shape[1]  # Number of samples per ECG\n",
    "\n",
    "sampling_frequency = 300  #Hz\n",
    "duration = num_samples / sampling_frequency  # Duration of each ECG\n",
    "\n",
    "class_distribution = df_labels['label'].value_counts()  # Distribution over classes\n",
    "\n",
    "print(f\"Number of ECGs: {num_ecgs}\")\n",
    "print(f\"Number of samples per ECG: {num_samples}\")\n",
    "print(f\"Duration of each ECG: {duration} seconds\")\n",
    "print(f\"\\nClass Distribution:\\n{class_distribution}\")"
   ],
   "id": "35431ea3d28b3c83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Indices per class (based on df_labels)\n",
    "sinus_indices = df_labels[df_labels[\"label\"] == \"Normal Sinus Rhythm\"].index.tolist()\n",
    "af_indices = df_labels[df_labels[\"label\"] == \"Atrial Fibrillation\"].index.tolist()\n",
    "\n",
    "example_sinus_idx = random.choice(sinus_indices)\n",
    "example_af_idx = random.choice(af_indices)\n",
    "\n",
    "ecg_sinus_raw = df.iloc[example_sinus_idx].astype(float).values\n",
    "ecg_af_raw = df.iloc[example_af_idx].astype(float).values\n",
    "\n",
    "time = np.arange(0, len(ecg_sinus_raw)) / sampling_frequency"
   ],
   "id": "163e1436361f0fd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Summary statistics for each ECG\n",
    "summary_stats = df.describe().T\n",
    "summary_stats = pd.concat([summary_stats, df_labels], axis=1)"
   ],
   "id": "440c70f3e5d9c028"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature extraction",
   "id": "3e953dfd2245a898"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ECG feature engineering",
   "id": "ce582f7561a7a50c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select an ECG in Normal Sinus Rhythm and one in AF and process them\n",
    "selected_sinus_indices = random.sample(sinus_indices, 1)\n",
    "selected_af_indices = random.sample(af_indices, 1)\n",
    "\n",
    "ecg_NSR = df.iloc[selected_sinus_indices[0]].astype(float)\n",
    "signals_NSR, info_NSR = nk.ecg_process(ecg_NSR, sampling_rate=sampling_frequency)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "nk.ecg_plot(signals_NSR, info_NSR)\n",
    "\n",
    "ecg_AF = df.iloc[selected_af_indices[0]].astype(float)\n",
    "signals_AF, info_AF = nk.ecg_process(ecg_AF, sampling_rate=sampling_frequency)\n",
    "\n",
    "nk.ecg_plot(signals_AF, info_AF)"
   ],
   "id": "62ff7a18d6cc5bed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### R-peaks**",
   "id": "7342bd28a881483e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find R-peaks\n",
    "peaks_NSR, info_NSR = nk.ecg_peaks(ecg_NSR, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)\n",
    "peaks_AF, info_AF = nk.ecg_peaks(ecg_AF, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)"
   ],
   "id": "a998916cc84a7f6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Time-domain features",
   "id": "7dd17838efb634b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Time domain features NSR\n",
    "hrv_time_NSR = nk.hrv_time(peaks_NSR, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_NSR"
   ],
   "id": "aa0723be4a0c6bb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Time domain features AF\n",
    "hrv_time_AF = nk.hrv_time(peaks_AF, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_AF"
   ],
   "id": "2d79728c9d9e5252"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TRAIN)",
   "id": "6b1745d5ded68aa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Getting all the ECG readouts so we can extract P-wave information\n",
    "\n",
    "def get_ECG_readout_train():\n",
    "    test_run = 0\n",
    "    ecg_full = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for i in tqdm(train_idx):\n",
    "\n",
    "        ecg = df.iloc[i].astype(float)\n",
    "        signals, info = nk.ecg_process(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # Assign the current ecg_index to the signals DataFrame before concatenation\n",
    "        signals[\"ecg_index\"] = i\n",
    "\n",
    "        ecg_full = pd.concat([ecg_full, signals], ignore_index=True)\n",
    "\n",
    "        #test_run += 1\n",
    "\n",
    "        if test_run == 10:\n",
    "            break  # Stop after 10 iterations for the example\n",
    "\n",
    "    return ecg_full"
   ],
   "id": "e7a945832485e1c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_full = get_ECG_readout_train()"
   ],
   "id": "98cf6e955ef0ec81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_ECG_metrics(ecg_full):\n",
    "    ecg_metrics_list = []\n",
    "\n",
    "    for i in tqdm(train_idx[:]):\n",
    "        mean_quality = ecg_full.loc[ecg_full.ecg_index == i]['ECG_Quality'].mean()\n",
    "        mean_pwave_amplitude = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)][\n",
    "            'ECG_Clean'].mean()  #You could consider taking sqrt, mean and then **2\n",
    "        #(more robust) to outliers\n",
    "        stdev_pwave = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)]['ECG_Quality'].std()\n",
    "        #Perhaps I could add something about irregularly irregular rhythm, but it's (really) difficult mathematically\n",
    "        ecg_metrics_list.append({\n",
    "            'Mean_Quality': mean_quality,\n",
    "            'Mean_PWave_Amplitude': mean_pwave_amplitude,\n",
    "            'STDEV_Pwave': stdev_pwave,\n",
    "            'ecg_index': i\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(ecg_metrics_list)"
   ],
   "id": "1ae78b22a505ec73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_metrics = get_ECG_metrics(ecg_full)"
   ],
   "id": "7e4782adc009b181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    #FULL HRV feature extraction for all ECGs (TRAIN)\n",
    "\n",
    "    hrv_features_train = []\n",
    "\n",
    "    for i in tqdm(train_idx, desc=\"HRV (ALL FEATURES): TRAIN SET\"):\n",
    "        # Grab raw ECG\n",
    "        ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "        try:\n",
    "            # 1. Clean ECG\n",
    "            ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "            # 2. Detect R-peaks\n",
    "            peaks, _ = nk.ecg_peaks(\n",
    "                ecg_clean,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                correct_artifacts=True\n",
    "            )\n",
    "\n",
    "            # 3. Compute FULL HRV feature set\n",
    "            hrv_full = nk.hrv(\n",
    "                peaks,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                show=False\n",
    "            )\n",
    "\n",
    "            # Ensure row is a proper 1-row DataFrame and add ecg_index\n",
    "            hrv_full = hrv_full.copy()\n",
    "            hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "            hrv_features_train.append(hrv_full)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing TRAIN ECG {i}: {e}\")\n",
    "\n",
    "            if hrv_features_train:\n",
    "                empty = pd.DataFrame(\n",
    "                    [np.nan] * hrv_features_train[0].shape[1],\n",
    "                    index=hrv_features_train[0].columns\n",
    "                ).T\n",
    "                empty[\"ecg_index\"] = i\n",
    "                hrv_features_train.append(empty)\n",
    "\n",
    "    # Combine to single DataFrame\n",
    "    hrv_train = pd.concat(hrv_features_train, ignore_index=True)\n",
    "\n",
    "    print(\"hrv_train shape:\", hrv_train.shape)\n",
    "    hrv_train.head()"
   ],
   "id": "641d646cc05377e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Merge our new dataframe with our extra variables\n",
    "    hrv_train = pd.merge(hrv_train, ecg_metrics, on='ecg_index', how='left')\n",
    "    hrv_train.head()"
   ],
   "id": "eecf057664438b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Remove all columns from the dataframe that contain more than 50% NaN\n",
    "    threshold = 0.5\n",
    "    hrv_train_clean = hrv_train.dropna(thresh=len(hrv_train) * threshold, axis=1)\n",
    "\n",
    "    # Remove all rows that are all NaN\n",
    "    hrv_train_clean = hrv_train_clean.dropna(how='all')\n",
    "\n",
    "    hrv_train_clean.to_csv(\"data/hrv_train.csv\", index=False)\n",
    "    hrv_train_clean.head()"
   ],
   "id": "a26684b5faa77915"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TEST)",
   "id": "f42125251e76303e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Getting all the ECG readouts so we can extract P-wave information\n",
    "\n",
    "def get_ECG_readout_test():\n",
    "    test_run = 0\n",
    "    ecg_full = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for i in tqdm(test_idx):\n",
    "\n",
    "        ecg = df.iloc[i].astype(float)\n",
    "        signals, info = nk.ecg_process(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # Assign the current ecg_index to the signals DataFrame before concatenation\n",
    "        signals[\"ecg_index\"] = i\n",
    "\n",
    "        ecg_full = pd.concat([ecg_full, signals], ignore_index=True)\n",
    "\n",
    "        #test_run += 1\n",
    "\n",
    "        if test_run == 10:\n",
    "            break  # Stop after 10 iterations for the example\n",
    "\n",
    "    return ecg_full"
   ],
   "id": "42f654699f1629d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_full_test = get_ECG_readout_test()"
   ],
   "id": "e20642a571ab773"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_ECG_metrics(ecg_full):\n",
    "    ecg_metrics_list = []\n",
    "\n",
    "    for i in tqdm(test_idx[:]):\n",
    "        #mean_quality = ecg_full.loc[ecg_full.ecg_index == i]['ECG_Quality'].mean()\n",
    "        #Note:ecg quality is only to make the training data less noisy\n",
    "        mean_pwave_amplitude = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)][\n",
    "            'ECG_Clean'].mean()  #You could consider taking sqrt, mean and then **2\n",
    "        #(more robust) to outliers\n",
    "        stdev_pwave = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)]['ECG_Quality'].std()\n",
    "        #Perhaps I could add something about irregularly irregular rhythm, but it's (really) difficult mathematically\n",
    "        ecg_metrics_list.append({\n",
    "            'Mean_PWave_Amplitude': mean_pwave_amplitude,\n",
    "            'STDEV_Pwave': stdev_pwave,\n",
    "            'ecg_index': i\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(ecg_metrics_list)"
   ],
   "id": "b668fb1c47f7ebe4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_metrics_test = get_ECG_metrics(ecg_full_test)"
   ],
   "id": "1930ae53a05b8dcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    hrv_features_test = []\n",
    "\n",
    "    for i in tqdm(test_idx, desc=\"HRV (ALL FEATURES): TEST SET\"):\n",
    "        ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "        try:\n",
    "            # 1. Clean ECG\n",
    "            ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "            # 2. Detect R-peaks\n",
    "            peaks, _ = nk.ecg_peaks(\n",
    "                ecg_clean,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                correct_artifacts=True\n",
    "            )\n",
    "\n",
    "            # 3. Compute FULL HRV feature set\n",
    "            hrv_full = nk.hrv(\n",
    "                peaks,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                show=False\n",
    "            )\n",
    "\n",
    "            # Same as TRAIN: keep as 1-row DataFrame, add index\n",
    "            hrv_full = hrv_full.copy()\n",
    "            hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "            hrv_features_test.append(hrv_full)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing TEST ECG {i}: {e}\")\n",
    "\n",
    "            if hrv_features_test:\n",
    "                empty = pd.DataFrame(\n",
    "                    [np.nan] * hrv_features_test[0].shape[1],\n",
    "                    index=hrv_features_test[0].columns\n",
    "                ).T\n",
    "                empty[\"ecg_index\"] = i\n",
    "                hrv_features_test.append(empty)\n",
    "\n",
    "    hrv_test = pd.concat(hrv_features_test, ignore_index=True)\n",
    "\n",
    "    print(\"hrv_test shape:\", hrv_test.shape)\n",
    "    hrv_test.head()"
   ],
   "id": "8166cc8f9a9ab5e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Merge our new dataframe with our extra variables\n",
    "    hrv_test = pd.merge(hrv_test, ecg_metrics_test, on='ecg_index', how='left')\n",
    "    hrv_test.head()"
   ],
   "id": "ca9614ad2b3cbdf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Remove all columns from the dataframe that contain more than 50% NaN\n",
    "    threshold = 0.5\n",
    "    hrv_test_clean = hrv_test.dropna(thresh=len(hrv_test) * threshold, axis=1)\n",
    "\n",
    "    # Remove all rows that are all NaN\n",
    "    hrv_test_clean = hrv_test_clean.dropna(how='all')\n",
    "\n",
    "    hrv_test_clean.head()\n",
    "\n",
    "    hrv_test.to_csv(\"data/hrv_test.csv\", index=False)"
   ],
   "id": "c12335c2097814e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# If you don't want to wait that long",
   "id": "6216dc76705487cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == True:\n",
    "    hrv_test_clean = pd.read_csv(\"data/hrv_test.csv\")\n",
    "    hrv_train_clean= pd.read_csv(\"data/hrv_train.csv\")"
   ],
   "id": "e119f4eb6ad3c4ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature exploration",
   "id": "c8a4d8c3ea3a1bdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the HRV data with the rhythm labels\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on='ecg_index', right_index=True\n",
    ").reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "selectedMetric = 'HRV_MedianNN'\n",
    "rhythms = hrv_train_with_labels['label'].unique()\n",
    "for rhythm in rhythms:\n",
    "    subset = hrv_train_with_labels[hrv_train_with_labels['label'] == rhythm]\n",
    "    plt.hist(subset[selectedMetric], alpha=0.7, label=rhythm, bins='auto')\n",
    "\n",
    "plt.xlabel(selectedMetric)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution by Rhythm')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "c974193a80dfc115"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "5207b2669182b19f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missingness",
   "id": "cd141f9e420f222d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def check_missing_hrv(df):\n",
    "    \"\"\"\n",
    "    Summarize missingness across HRV features.\n",
    "    \"\"\"\n",
    "    missing = df.isna().sum()\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": df.columns,\n",
    "        \"missing_n\": missing,\n",
    "        \"missing_%\": (missing / len(df)) * 100\n",
    "    })\n",
    "    display(out.sort_values(\"missing_%\", ascending=False))\n",
    "    return out"
   ],
   "id": "758df04698a9345"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "check_missing_hrv(hrv_train_clean)",
   "id": "beb3251180767cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##Removing reads with low quality scores (TEST ONLY)",
   "id": "7e3af4aecd569402"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#ecg_metrics_test.head()",
   "id": "96a0fb1867302c9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hrv_train_clean_og = hrv_train_clean",
   "id": "eeae730d2dfd065f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.hist(hrv_train_clean['Mean_Quality']); #You can base number below on this perhaps",
   "id": "da44bf5542e2a7f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hrv_train_clean = hrv_train_clean.loc[hrv_train_clean['Mean_Quality'] >= 0] #You can adjust this number to be higher or lower",
   "id": "eaeab0e7aeca8ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Outlier Detection",
   "id": "1841fb7bb865518c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to identify outliers in the data\n",
    "def identify_outliers(df, column_name, threshold=1.5):\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    row_indices = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)].index.tolist()\n",
    "    outlier_values = df.loc[row_indices, column_name].tolist()\n",
    "\n",
    "    return row_indices, outlier_values, lower_bound, upper_bound"
   ],
   "id": "7f80407814942d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Outlier detection ONLY ON TRAIN\n",
    "\n",
    "# Merge labels with TRAIN features (cleaned hrv_train)\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on=\"ecg_index\", right_index=True\n",
    ")\n",
    "\n",
    "# Outlier detection ONLY on TRAIN\n",
    "train_outlier_idx, outlier_values, iqr_lower, iqr_upper = identify_outliers(\n",
    "    hrv_train_with_labels,\n",
    "    \"HRV_MedianNN\",\n",
    "    threshold=1.5\n",
    ")\n",
    "\n",
    "# ecg_index as (int)\n",
    "hrv_train_with_labels[\"ecg_index\"] = hrv_train_with_labels[\"ecg_index\"].astype(int)\n",
    "\n",
    "print(\"Train outliers detected:\", len(train_outlier_idx))\n",
    "print(\"Row indices (in hrv_train_with_labels) with outliers:\", train_outlier_idx)\n",
    "print(\"Outlier HRV_MedianNN values:\", outlier_values)"
   ],
   "id": "5fb057a15e0f155b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualise one outlier ECG\n",
    "\n",
    "example_outlier_row = train_outlier_idx[0]\n",
    "\n",
    "# Single row\n",
    "row = hrv_train_with_labels.loc[example_outlier_row]\n",
    "\n",
    "# Extract ECG index value\n",
    "ecg_index_values = row.filter(like=\"ecg_index\").values\n",
    "\n",
    "# Use first value\n",
    "ecg_idx = int(ecg_index_values[0])\n",
    "\n",
    "# Extract raw ECG from df\n",
    "ecg_raw = df.iloc[ecg_idx].astype(float).values\n",
    "\n",
    "# Visualise R-Peaks\n",
    "peaks_outlier, info_outlier = nk.ecg_peaks(\n",
    "    ecg_raw,\n",
    "    sampling_rate=sampling_frequency,\n",
    "    correct_artifacts=True,\n",
    "    show=True\n",
    ")"
   ],
   "id": "fafe213d952d6255"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hrv_train_with_labels.loc[example_outlier_row]",
   "id": "aa8b5efe86e46798"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Outliers TEST set** done the same way as for TRAINING",
   "id": "985e293de4305cea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Align TEST columns to TRAIN columns\n",
    "\n",
    "# Align TEST columns to TRAIN columns (no leakage, same feature space)\n",
    "train_cols = hrv_train_clean.columns  # already cleaned on TRAIN\n",
    "shared_cols = [c for c in train_cols if c in hrv_test_clean.columns]\n",
    "\n",
    "hrv_test_aligned = hrv_test_clean[shared_cols].copy()\n",
    "\n",
    "# Merge TEST HRV with labels\n",
    "hrv_test_with_labels = pd.merge(\n",
    "    hrv_test_aligned,\n",
    "    df_labels[[\"label\", \"classification\"]],\n",
    "    left_on=\"ecg_index\",\n",
    "    right_index=True\n",
    ")"
   ],
   "id": "1775283110c90443"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Same IQR bounds as on hrv_train\n",
    "\n",
    "Q1 = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3 = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_test_clean = hrv_test_with_labels[\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] >= lower_bound) &\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] <= upper_bound)\n",
    "    ].copy()\n",
    "\n",
    "print(\"hrv_test shape:\", hrv_test_clean.shape)\n",
    "print(\"hrv_test_with_labels shape:\", hrv_test_with_labels.shape)\n",
    "print(\"hrv_test_clean shape:\", hrv_test_clean.shape)"
   ],
   "id": "9d2021c7eb77330e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution TRAIN + TEST | Sanity check",
   "id": "a9b5472ff794e0d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(hrv_train_clean.columns[:5])\n",
    "print(hrv_test_clean.columns[:5])\n",
    "print(hrv_test_clean[[\"HRV_MedianNN\", \"classification\"]].head())"
   ],
   "id": "7aa4506894de4c01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for feat in [\"HRV_MedianNN\", \"HRV_SDNN\"]:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.kdeplot(\n",
    "        data=hrv_train_clean, x=feat, label=\"Train\", fill=True, common_norm=False\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=hrv_test_clean, x=feat, label=\"Test\", fill=True, common_norm=False, color=\"orange\"\n",
    "    )\n",
    "    plt.title(f\"{feat}: Train vs Test\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "efc030ae420dc291"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling TRAIN",
   "id": "67ab0b29e2f390db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Winsorising outliers",
   "id": "2df2699e0ee5c58b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Q1 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_clip = Q1 - 1.5 * IQR\n",
    "upper_clip = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_train_winsor = hrv_train_with_labels.copy()\n",
    "hrv_train_winsor[\"HRV_MedianNN_winsor\"] = hrv_train_with_labels[\"HRV_MedianNN\"].clip(\n",
    "    lower=lower_clip, upper=upper_clip\n",
    ")\n",
    "\n",
    "print(\"Shape after winsorizing (same as original):\", hrv_train_winsor.shape)"
   ],
   "id": "73bcc05f5a4525a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling Comparison",
   "id": "68ed2fec854857de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(hrv_train_with_labels[\"HRV_MedianNN\"], kde=True, color=\"red\", label=\"Original\")\n",
    "sns.histplot(hrv_train_winsor[\"HRV_MedianNN_winsor\"], kde=True, color=\"green\", label=\"Winsorized\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Outlier Handling Comparison\")\n",
    "plt.show()"
   ],
   "id": "857280dcdac10736"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final Preprocessing: Building ML Matrices (X_train, X_test)",
   "id": "5d4a29c4f0096eb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select HRV feature columns only\n",
    "feature_cols = [col for col in hrv_train_with_labels.columns if col.startswith(\"HRV_\")]\n",
    "\n",
    "# TRAIN data\n",
    "x_train = hrv_train_with_labels[feature_cols].copy()\n",
    "y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "\n",
    "# TEST data\n",
    "x_test = hrv_test_clean[feature_cols].copy()\n",
    "y_test = hrv_test_clean[\"classification\"].copy()"
   ],
   "id": "514a5b4873e5768"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Replace +/- inf with NaN in both TRAIN and TEST\n",
    "for df_ in (x_train, x_test):\n",
    "    df_.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop columns that are all-NaN (if any)\n",
    "all_nan_cols = x_train.columns[x_train.isna().all()]\n",
    "if len(all_nan_cols) > 0:\n",
    "    print(\"Dropping all-NaN columns before imputation:\", list(all_nan_cols))\n",
    "    x_train.drop(columns=all_nan_cols, inplace=True)\n",
    "    x_test.drop(columns=all_nan_cols, inplace=True)"
   ],
   "id": "f604c4792a0b63ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputation",
   "id": "69fc3f83189fe0eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Median imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_imputed = imputer.fit_transform(x_train)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_imputed = imputer.transform(x_test)"
   ],
   "id": "a1d6c8856af5845a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalisation",
   "id": "3c1ebd30f12fd18c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Temporarily convert to DataFrame to calculate Skewness easily\n",
    "temp_df = pd.DataFrame(X_train_imputed, columns=feature_cols)\n",
    "skewness = temp_df.skew().sort_values(ascending=False)\n",
    "\n",
    "#Identify skewed columns (Threshold > 1.0)\n",
    "skewed_cols = skewness[abs(skewness) > 1.0].index.tolist()\n",
    "\n",
    "#Apply Log Transform directly to the NumPy arrays\n",
    "for col_name in skewed_cols:\n",
    "    # Find the column index (integer position)\n",
    "    col_idx = feature_cols.index(col_name)\n",
    "\n",
    "    # Check for negative values (Log crashes on negatives)\n",
    "    # We find the global minimum for this column across Train and Test\n",
    "    min_val = min(X_train_imputed[:, col_idx].min(), X_test_imputed[:, col_idx].min())\n",
    "\n",
    "    shift = 0\n",
    "    if min_val < 0:\n",
    "        # If negatives exist, calculate a shift to make the minimum 0\n",
    "        shift = abs(min_val)\n",
    "\n",
    "    # Apply transformation in-place: Log(x + shift + 1)\n",
    "    X_train_imputed[:, col_idx] = np.log1p(X_train_imputed[:, col_idx] + shift)\n",
    "    X_test_imputed[:, col_idx] = np.log1p(X_test_imputed[:, col_idx] + shift)"
   ],
   "id": "8976f1a622a04b40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaling",
   "id": "75032dadb32b4982"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert back to df with column names\n",
    "x_train = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "x_test = pd.DataFrame(X_test_scaled, columns=feature_cols)"
   ],
   "id": "6ab2134c2a48bd93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Sanity checks**",
   "id": "cee47ec723a2763e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Median X_train\n",
    "print(\"Median of scaled features (should be ~0):\")\n",
    "print(x_train.median().round(3))\n",
    "\n",
    "# IQR X_train\n",
    "print(\"\\nIQR of scaled features (should be ~1):\")\n",
    "print((x_train.quantile(0.75) - x_train.quantile(0.25)).round(3))\n",
    "\n",
    "#Checking skewness of the datasets\n",
    "skewness_train = x_train.skew().sort_values(ascending=False)\n",
    "skewness_test = x_train.skew().sort_values(ascending=False)\n",
    "# Filter for highly skewed columns (absolute skew > 1.0)\n",
    "high_skew_cols_train = skewness_train[abs(skewness_train) > 1.0]\n",
    "high_skew_cols_test = skewness_test[abs(skewness_test) > 1.0]\n",
    "\n",
    "print(len(high_skew_cols_train))\n",
    "print(len(high_skew_cols_test))"
   ],
   "id": "514cfa4daac35dc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final ML datasets (X_train, X_test, y_train, y_test",
   "id": "228989f6112bd97d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "id": "e9f135e866219398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if True:\n",
    "    # Feature matrices (winsorised > imputation > scaling)\n",
    "    x_train = X_train_scaled\n",
    "    x_test = X_test_scaled\n",
    "\n",
    "    # Target vectors (created earlier from HRV + labels AF(0/1))\n",
    "    y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "    y_test = hrv_test_clean[\"classification\"].copy()\n",
    "\n",
    "    print(\"Final X_train shape:\", x_train.shape)\n",
    "    print(\"Final X_test shape:\", x_test.shape)\n",
    "    print(\"Final y_train shape:\", y_train.shape)\n",
    "    print(\"Final y_test shape:\", y_test.shape)"
   ],
   "id": "f32cb09e1423b592"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training Setup",
   "id": "cedfee9b28848a98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Safety check",
   "id": "5ce5bda1ded97792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "assert len(x_train) == len(y_train), \"Misaligned TRAIN matrix and labels!\"\n",
    "assert len(x_test) == len(y_test), \"Misaligned TEST matrix and labels!\"\n",
    "\n",
    "assert not np.isnan(x_train).any(), \"NaNs detected in X_train!\"\n",
    "assert not np.isnan(x_test).any(), \"NaNs detected in X_test!\""
   ],
   "id": "ba6b2c0cb552efc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison framework",
   "id": "42a1aa0320a37687"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "resultsTable = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC', 'ROC_AUC', 'cm'])\n",
    "\n",
    "def modelResults(model, accuracy, f1, precision, recall, roc_auc, roc_cur, cm):\n",
    "    print(\n",
    "        f\"Model {model} evaluated. \\nAccuracy: {accuracy} \\nF1 Score: {f1} \\nPrecision: {precision} \\nRecall: {recall} \\nROC AUC: {roc_auc}\")\n",
    "    resultsTable.loc[len(resultsTable)] = [model, accuracy, f1, precision, recall, roc_cur, roc_auc, cm]\n",
    "    resultsTable.to_csv(\"data/trainingResults.csv\", index=False, mode=\"a\")"
   ],
   "id": "e9ad28e79c6fec77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\n",
    "    f\"X train length: {len(x_train)}\\n X test length: {len(x_test)} \\n Y train length: {len(y_train)}\\n Y test length: {len(y_test)}\")"
   ],
   "id": "a5a6221aa358355e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Going back to basics, the currently used x_train and x_test gave ValueErrors as negative values for Log\n",
    "\n",
    "raw_cols = [c for c in hrv_train_with_labels.columns if c.startswith(\"HRV_\")]\n",
    "raw_train = hrv_train_with_labels[raw_cols].copy()\n",
    "raw_test = hrv_test_clean[raw_cols].copy()\n",
    "\n",
    "raw_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "raw_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer_eng = SimpleImputer(strategy=\"median\")\n",
    "raw_train_imp = pd.DataFrame(imputer_eng.fit_transform(raw_train), columns=raw_cols)\n",
    "raw_test_imp = pd.DataFrame(imputer_eng.transform(raw_test), columns=raw_cols)\n",
    "\n",
    "skewness = raw_train_imp.skew().sort_values(ascending=False)\n",
    "skewed_cols = skewness[abs(skewness) > 1.0].index.tolist()\n",
    "\n",
    "new_features_train = pd.DataFrame(index=raw_train_imp.index)\n",
    "new_features_test = pd.DataFrame(index=raw_test_imp.index)\n",
    "\n",
    "#1. Log Transforms\n",
    "for col in skewed_cols:\n",
    "    # +1e-6 avoids log(0)\n",
    "    new_features_train[f'Log_{col}'] = np.log(raw_train_imp[col] + 1e-6)\n",
    "    new_features_test[f'Log_{col}'] = np.log(raw_test_imp[col] + 1e-6)\n",
    "\n",
    "#2. 2. Coefficient of Variation (CV) computation:\n",
    "if 'HRV_SDNN' in raw_train_imp.columns and 'HRV_MeanNN' in raw_train_imp.columns:\n",
    "    new_features_train['CV_SDNN'] = raw_train_imp['HRV_SDNN'] / (raw_train_imp['HRV_MeanNN'] + 1e-6)\n",
    "    new_features_test['CV_SDNN'] = raw_test_imp['HRV_SDNN'] / (raw_test_imp['HRV_MeanNN'] + 1e-6)\n",
    "\n",
    "# 3. Chaos Index (Amplifies the \"irregularly irregular\" signal specific to AF.):\n",
    "entropy_col = 'HRV_ApEn' if 'HRV_ApEn' in raw_train_imp.columns else 'HRV_SampEn'\n",
    "if 'HRV_RMSSD' in raw_train_imp.columns and entropy_col in raw_train_imp.columns:\n",
    "    new_features_train['Chaos_Index'] = raw_train_imp['HRV_RMSSD'] * raw_train_imp[entropy_col]\n",
    "    new_features_test['Chaos_Index'] = raw_test_imp['HRV_RMSSD'] * raw_test_imp[entropy_col]\n",
    "\n",
    "new_features_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "new_features_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer_new = SimpleImputer(strategy=\"median\")\n",
    "new_train_clean = pd.DataFrame(imputer_new.fit_transform(new_features_train), columns=new_features_train.columns)\n",
    "new_test_clean = pd.DataFrame(imputer_new.transform(new_features_test), columns=new_features_test.columns)\n",
    "\n",
    "scaler_eng = RobustScaler()\n",
    "new_train_scaled = pd.DataFrame(scaler_eng.fit_transform(new_train_clean), columns=new_features_train.columns)\n",
    "new_test_scaled = pd.DataFrame(scaler_eng.transform(new_test_clean), columns=new_features_test.columns)\n",
    "\n",
    "if not isinstance(x_train, pd.DataFrame):\n",
    "    x_train = pd.DataFrame(x_train, columns=feature_cols)\n",
    "if not isinstance(x_test, pd.DataFrame):\n",
    "    x_test = pd.DataFrame(x_test, columns=feature_cols)\n",
    "\n",
    "x_train_added = pd.concat([x_train, new_train_scaled], axis=1)\n",
    "x_test_added = pd.concat([x_test, new_test_scaled], axis=1)"
   ],
   "id": "19e689e7abe20968"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Selection & Filtering",
   "id": "91e80fabeed2560d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "id": "60837a6e4285e140"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train_filtered = x_train.copy()\n",
    "x_test_filtered = x_test.copy()\n",
    "\n",
    "corr_matrix = pd.DataFrame(x_train_filtered).corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "\n",
    "x_train_filtered = x_train_filtered.drop(columns=high_corr_features)\n",
    "x_test_filtered = x_test_filtered.drop(columns=high_corr_features)\n",
    "\n",
    "print(f\"Removed {len(high_corr_features)} highly correlated features\")\n",
    "print(f\"Remaining features: {x_train_filtered.shape[1]}\")"
   ],
   "id": "d8e51a0577eb5e33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "variance_threshold = 0.01\n",
    "feature_variances = pd.DataFrame(x_train_filtered).var()\n",
    "low_var_features = feature_variances[feature_variances < variance_threshold].index.tolist()\n",
    "\n",
    "x_train_filtered = pd.DataFrame(x_train_filtered).drop(columns=low_var_features)\n",
    "x_test_filtered = pd.DataFrame(x_test_filtered).drop(columns=low_var_features)\n",
    "\n",
    "print(f\"Removed {len(low_var_features)} low variance features\")\n",
    "print(f\"Remaining features: {x_train_filtered.shape[1]}\")"
   ],
   "id": "b87eb7b88a6d7ae8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selector_anova = SelectKBest(score_func=f_classif, k=min(50, x_train_filtered.shape[1]))\n",
    "x_train_anova = selector_anova.fit_transform(x_train_filtered, y_train)\n",
    "x_test_anova = selector_anova.transform(x_test_filtered)\n",
    "\n",
    "selected_features_anova = pd.DataFrame(x_train_filtered).columns[selector_anova.get_support()].tolist()\n",
    "print(f\"ANOVA F-test selected {len(selected_features_anova)} features\")"
   ],
   "id": "4a3b066176454c7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=min(50, x_train_filtered.shape[1]))\n",
    "x_train_mi = selector_mi.fit_transform(x_train_filtered, y_train)\n",
    "x_test_mi = selector_mi.transform(x_test_filtered)\n",
    "\n",
    "selected_features_mi = pd.DataFrame(x_train_filtered).columns[selector_mi.get_support()].tolist()\n",
    "print(f\"Mutual Information selected {len(selected_features_mi)} features\")"
   ],
   "id": "88233e7b1588bfbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=3003, n_jobs=-1)\n",
    "rf_selector.fit(x_train_filtered, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': pd.DataFrame(x_train_filtered).columns,\n",
    "    'importance': rf_selector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_k_features = min(50, len(feature_importance))\n",
    "selected_features_rf = feature_importance.head(top_k_features)['feature'].tolist()\n",
    "\n",
    "x_train_rf = pd.DataFrame(x_train_filtered)[selected_features_rf]\n",
    "x_test_rf = pd.DataFrame(x_test_filtered)[selected_features_rf]\n",
    "\n",
    "print(f\"Random Forest selected top {len(selected_features_rf)} features\")"
   ],
   "id": "3024b03655b3c7c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance.head(20)['feature'], feature_importance.head(20)['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "46711b292388064e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "common_features = list(set(selected_features_anova) & set(selected_features_mi) & set(selected_features_rf))\n",
    "print(f\"Features selected by all methods: {len(common_features)}\")\n",
    "\n",
    "if len(common_features) < 20:\n",
    "    union_features = list(set(selected_features_anova) | set(selected_features_mi) | set(selected_features_rf))\n",
    "    x_train_final = pd.DataFrame(x_train_filtered)[union_features]\n",
    "    x_test_final = pd.DataFrame(x_test_filtered)[union_features]\n",
    "    print(f\"Using union of all methods: {len(union_features)} features\")\n",
    "else:\n",
    "    x_train_final = pd.DataFrame(x_train_filtered)[common_features]\n",
    "    x_test_final = pd.DataFrame(x_test_filtered)[common_features]\n",
    "    print(f\"Using common features: {len(common_features)} features\")"
   ],
   "id": "4a20a00a0f2f56d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training",
   "id": "6c35b29e684619fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Soft voting classifier",
   "id": "56a60e4529186536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "voters = [\n",
    "    (\"lr\", LogisticRegression(multi_class='auto', max_iter=1000, class_weight='balanced', random_state=3003)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=500, random_state=3003, class_weight='balanced', criterion=\"log_loss\")),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=8, weights='distance', algorithm='auto', p=2)),\n",
    "    (\"ada\", AdaBoostClassifier(n_estimators=1500, random_state=3003))\n",
    "]\n",
    "\n",
    "soft_vote = VotingClassifier(estimators=voters, voting=\"soft\", n_jobs=-1)\n",
    "soft_vote.fit(x_train, y_train)\n",
    "\n",
    "y_pred_proba = soft_vote.predict_proba(x_test)[:, 1]\n",
    "y_pred = soft_vote.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "modelResults(soft_vote, accuracy, f1, precision, recall, roc_auc, roc_cur, cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=soft_vote.classes_)\n",
    "disp.plot()"
   ],
   "id": "328e8ea4142a1662"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model evaluation",
   "id": "cd9b514bf9d887d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resultsTable",
   "id": "801161daf65e349d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ROC Curves",
   "id": "65a0dc2b86a19d4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for idx, row in resultsTable.iterrows():\n",
    "    model_name = str(row['Model']).split('(')[0]\n",
    "    fpr, tpr, thresholds = row['ROC']\n",
    "    roc_auc = row['ROC_AUC']\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(alpha=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fb42c5ffd3a3f2ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "data-witches",
   "language": "python",
   "display_name": "Data-Witches"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
