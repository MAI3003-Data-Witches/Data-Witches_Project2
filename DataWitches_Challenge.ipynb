{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/MAI3003-Data-Witches/AtrialFibrillation-detection/blob/challenge/DataWitches_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "id": "c8e4060c4e39c528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data Witches**",
   "id": "5ef94415884de7ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| **Name**         | **Student ID** |\n",
    "|------------------|----------------|\n",
    "| Claessen, VVHJAE | i6339543       |\n",
    "| Ovsiannikova, AM | i6365923       |\n",
    "| Pubben, J        | i6276134       |\n",
    "| Roca Cugat, M    | i6351071       |\n",
    "| Záboj, J         | i6337952       |"
   ],
   "id": "8277ad73eacf46ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Logbook**",
   "id": "316c9d75834fec44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Methods\n",
    "\n",
    "Let's ensure we all use the same names for all components.\n",
    "\n",
    "| **Variable**                 | **Name**                                      |\n",
    "|------------------------------|-----------------------------------------------|\n",
    "| Raw ECG dataframe            | df                                            |\n",
    "| Label dataframe              | df_labels                                     |\n",
    "| HRV features (train)         | hrv_train                                     |\n",
    "| HRV features (test)          | hrv_test                                      |\n",
    "| HRV extraction type          | FULL (nk.hrv — time + freq + nonlinear + RSA) |\n",
    "| Clean HRV dataframe (train)  | hrv_train_clean                               |\n",
    "| Clean HRV dataframe (test)   | hrv_test_clean                                |\n",
    "| HRV + labels (train)         | hrv_train_with_labels                         |\n",
    "| Winsorized HRV column        | HRV_MedianNN_winsor                           |\n",
    "| Model feature matrix (train) | X_train                                       |\n",
    "| Model feature matrix (test)  | X_test                                        |\n",
    "| Model target vector (train)  | y_train                                       |\n",
    "| Model target vector (test)   | y_test                                        |\n",
    "\n",
    "\n",
    "| **Function**              | **Description**                                | **Arguments**                                |\n",
    "|---------------------------|------------------------------------------------|----------------------------------------------|\n",
    "| corr_plot_hrv()           | Correlation plot for HRV features              | df, cols=None                                |\n",
    "| distplots_hrv()           | Distribution plots (hist + KDE)                | df, cols=None                                |\n",
    "| boxplots_hrv()            | Boxplots for selected HRV variables            | df, cols                                     |\n",
    "| check_missing_hrv()       | Missingness summary                            | df                                           |\n",
    "| identify_outliers()       | IQR-based outlier detection                    | df, column_name, threshold=1.5               |\n",
    "| model_evaluation()        | Confusion matrix + classification report       | model                                        |\n",
    "| model_desc()              | Accuracy, CV, ROC-AUC, model performance       | model                                        |\n"
   ],
   "id": "48a4ff717364afa4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preface",
   "id": "50a17c407a5b47e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Packages imports",
   "id": "b74050974a65e510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    print(\"Loading required packages...\")\n",
    "    import sys\n",
    "    import random\n",
    "    import os.path\n",
    "    import warnings\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import neurokit2 as nk\n",
    "    from scipy import stats\n",
    "    import scipy.signal as signal\n",
    "    from scipy.signal import welch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from joblib.testing import xfail\n",
    "    import plotly.graph_objects as go\n",
    "    from colorama import Fore, Back, Style\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import make_column_transformer\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "    from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "        f1_score, precision_score, recall_score, roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve\n",
    "    from sympy import false\n",
    "\n",
    "    print(\"Loading successful!\")\n",
    "except Exception:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install -r https://raw.githubusercontent.com/MAI3003-Data-Witches/AtrialFibrillation-detection/refs/heads/challenge/requirements.txt\n",
    "\n",
    "    print(\"Loading required packages...\")\n",
    "    import sys\n",
    "    import random\n",
    "    import os.path\n",
    "    import warnings\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import neurokit2 as nk\n",
    "    from scipy import stats\n",
    "    import scipy.signal as signal\n",
    "    from scipy.signal import welch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from joblib.testing import xfail\n",
    "    import plotly.graph_objects as go\n",
    "    from colorama import Fore, Back, Style\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import make_column_transformer\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "    from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "        f1_score, precision_score, recall_score, roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve\n",
    "    from sympy import false\n",
    "\n",
    "    print(\"Loading successful!\")"
   ],
   "id": "4c65e16bcfc4a044"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Options settings",
   "id": "3fee32269b64ecf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(3003)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "DATA_PRESENT = os.path.isfile(\"data/Physionet2017Training.tar.xz\")\n",
    "LoadPremadeDataset = True"
   ],
   "id": "e7403a85bcb391b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset download",
   "id": "b73b6b0d9959e4c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset_location = 'data/Physionet2017TrainingData.csv'",
   "id": "d0b4085b431e1a63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not DATA_PRESENT:\n",
    "    !mkdir data\n",
    "    !wget https://github.com/MAI3003-Data-Witches/Data-Witches_Project2/raw/refs/heads/main/data/Physionet2017Training.tar.xz -O data/Physionet2017Training.tar.xz\n",
    "    !tar -xf data/Physionet2017Training.tar.xz -C data\n",
    "else:\n",
    "    print(f\"You already have the dataset downloaded at {dataset_location}, skipping\")"
   ],
   "id": "cef67dffd2f0eee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(dataset_location, header=None, index_col=False) * 1000  # Load the dataset already in mV\n",
    "\n",
    "df.head()"
   ],
   "id": "885d180e13604e71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing\n",
    "## Extract ECG signals and class labels"
   ],
   "id": "9faf076aaf22d268"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_labels = pd.read_csv('data/Physionet2017TrainingLabels.csv', header=None, names=['label'])\n",
    "df_labels['classification'] = df_labels['label'].replace({\"N\": 0, \"A\": 1})\n",
    "df_labels['label'] = df_labels['label'].replace({\"N\": 'Normal Sinus Rhythm', \"A\": 'Atrial Fibrillation'})"
   ],
   "id": "adea384b3857534f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_labels",
   "id": "72bc088c31e1e021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset splitting",
   "id": "bf30a7b8e4bb0a5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_labeled = pd.merge(df_labels.drop(columns='label'), df, left_on='classification', right_index=True)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df.index,\n",
    "    test_size=0.2,\n",
    "    stratify=df_labels[\"label\"],\n",
    "    random_state=3003\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "id": "52b69dd5c3584551"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "## Dataset characteristics"
   ],
   "id": "292ce7fa5b8932a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_ecgs = len(df)  # Number of ECGs\n",
    "\n",
    "num_samples = df.shape[1]  # Number of samples per ECG\n",
    "\n",
    "sampling_frequency = 300  #Hz\n",
    "duration = num_samples / sampling_frequency  # Duration of each ECG\n",
    "\n",
    "class_distribution = df_labels['label'].value_counts()  # Distribution over classes\n",
    "\n",
    "print(f\"Number of ECGs: {num_ecgs}\")\n",
    "print(f\"Number of samples per ECG: {num_samples}\")\n",
    "print(f\"Duration of each ECG: {duration} seconds\")\n",
    "print(f\"\\nClass Distribution:\\n{class_distribution}\")"
   ],
   "id": "cd26fb331b802024"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Indices per class (based on df_labels)\n",
    "sinus_indices = df_labels[df_labels[\"label\"] == \"Normal Sinus Rhythm\"].index.tolist()\n",
    "af_indices = df_labels[df_labels[\"label\"] == \"Atrial Fibrillation\"].index.tolist()\n",
    "\n",
    "example_sinus_idx = random.choice(sinus_indices)\n",
    "example_af_idx = random.choice(af_indices)\n",
    "\n",
    "ecg_sinus_raw = df.iloc[example_sinus_idx].astype(float).values\n",
    "ecg_af_raw = df.iloc[example_af_idx].astype(float).values\n",
    "\n",
    "time = np.arange(0, len(ecg_sinus_raw)) / sampling_frequency"
   ],
   "id": "b593e12489ff4fcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Summary statistics for each ECG\n",
    "summary_stats = df.describe().T\n",
    "summary_stats = pd.concat([summary_stats, df_labels], axis=1)"
   ],
   "id": "9dc5eef155024f47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature extraction",
   "id": "c27cb350e41757bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ECG feature engineering",
   "id": "e850ccd6de7f5740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select an ECG in Normal Sinus Rhythm and one in AF and process them\n",
    "selected_sinus_indices = random.sample(sinus_indices, 1)\n",
    "selected_af_indices = random.sample(af_indices, 1)\n",
    "\n",
    "ecg_NSR = df.iloc[selected_sinus_indices[0]].astype(float)\n",
    "signals_NSR, info_NSR = nk.ecg_process(ecg_NSR, sampling_rate=sampling_frequency)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "nk.ecg_plot(signals_NSR, info_NSR)\n",
    "\n",
    "ecg_AF = df.iloc[selected_af_indices[0]].astype(float)\n",
    "signals_AF, info_AF = nk.ecg_process(ecg_AF, sampling_rate=sampling_frequency)\n",
    "\n",
    "nk.ecg_plot(signals_AF, info_AF)"
   ],
   "id": "c3706b52f1b1ec77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### R-peaks**",
   "id": "56cbc8003b7be0c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find R-peaks\n",
    "peaks_NSR, info_NSR = nk.ecg_peaks(ecg_NSR, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)\n",
    "peaks_AF, info_AF = nk.ecg_peaks(ecg_AF, sampling_rate=sampling_frequency, correct_artifacts=True, show=True)"
   ],
   "id": "c736f25d168e8d86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Time-domain features",
   "id": "a40cf20230632379"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Time domain features NSR\n",
    "hrv_time_NSR = nk.hrv_time(peaks_NSR, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_NSR"
   ],
   "id": "b29da4c874020188"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Time domain features AF\n",
    "hrv_time_AF = nk.hrv_time(peaks_AF, sampling_rate=sampling_frequency, show=True)\n",
    "hrv_time_AF"
   ],
   "id": "676625bd9c85d3e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TRAIN)",
   "id": "61dc9deab8b5b676"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Getting all the ECG readouts so we can extract P-wave information\n",
    "\n",
    "def get_ECG_readout_train():\n",
    "    test_run = 0\n",
    "    ecg_full = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for i in tqdm(train_idx):\n",
    "\n",
    "        ecg = df.iloc[i].astype(float)\n",
    "        signals, info = nk.ecg_process(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # Assign the current ecg_index to the signals DataFrame before concatenation\n",
    "        signals[\"ecg_index\"] = i\n",
    "\n",
    "        ecg_full = pd.concat([ecg_full, signals], ignore_index=True)\n",
    "\n",
    "        #test_run += 1\n",
    "\n",
    "        if test_run == 10:\n",
    "            break  # Stop after 10 iterations for the example\n",
    "\n",
    "    return ecg_full"
   ],
   "id": "917af3113e61e6f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_full = get_ECG_readout_train()"
   ],
   "id": "3dc090b8e45c4df5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_ECG_metrics(ecg_full):\n",
    "    ecg_metrics_list = []\n",
    "\n",
    "    for i in tqdm(train_idx[:]):\n",
    "        mean_quality = ecg_full.loc[ecg_full.ecg_index == i]['ECG_Quality'].mean()\n",
    "        mean_pwave_amplitude = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)][\n",
    "            'ECG_Clean'].mean()  #You could consider taking sqrt, mean and then **2\n",
    "        #(more robust) to outliers\n",
    "        stdev_pwave = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)]['ECG_Quality'].std()\n",
    "        #Perhaps I could add something about irregularly irregular rhythm, but it's (really) difficult mathematically\n",
    "        ecg_metrics_list.append({\n",
    "            'Mean_Quality': mean_quality,\n",
    "            'Mean_PWave_Amplitude': mean_pwave_amplitude,\n",
    "            'STDEV_Pwave': stdev_pwave,\n",
    "            'ecg_index': i\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(ecg_metrics_list)"
   ],
   "id": "14957db2c350bcbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_metrics = get_ECG_metrics(ecg_full)"
   ],
   "id": "824d00b8f2a47b0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    #FULL HRV feature extraction for all ECGs (TRAIN)\n",
    "\n",
    "    hrv_features_train = []\n",
    "\n",
    "    for i in tqdm(train_idx, desc=\"HRV (ALL FEATURES): TRAIN SET\"):\n",
    "        # Grab raw ECG\n",
    "        ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "        try:\n",
    "            # 1. Clean ECG\n",
    "            ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "            # 2. Detect R-peaks\n",
    "            peaks, _ = nk.ecg_peaks(\n",
    "                ecg_clean,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                correct_artifacts=True\n",
    "            )\n",
    "\n",
    "            # 3. Compute FULL HRV feature set\n",
    "            hrv_full = nk.hrv(\n",
    "                peaks,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                show=False\n",
    "            )\n",
    "\n",
    "            # Ensure row is a proper 1-row DataFrame and add ecg_index\n",
    "            hrv_full = hrv_full.copy()\n",
    "            hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "            hrv_features_train.append(hrv_full)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing TRAIN ECG {i}: {e}\")\n",
    "\n",
    "            if hrv_features_train:\n",
    "                empty = pd.DataFrame(\n",
    "                    [np.nan] * hrv_features_train[0].shape[1],\n",
    "                    index=hrv_features_train[0].columns\n",
    "                ).T\n",
    "                empty[\"ecg_index\"] = i\n",
    "                hrv_features_train.append(empty)\n",
    "\n",
    "    # Combine to single DataFrame\n",
    "    hrv_train = pd.concat(hrv_features_train, ignore_index=True)\n",
    "\n",
    "    print(\"hrv_train shape:\", hrv_train.shape)\n",
    "    hrv_train.head()"
   ],
   "id": "9cb34949f9fef848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Merge our new dataframe with our extra variables\n",
    "    hrv_train = pd.merge(hrv_train, ecg_metrics, on='ecg_index', how='left')\n",
    "    hrv_train.head()"
   ],
   "id": "7ad80669c6b45cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Remove all columns from the dataframe that contain more than 50% NaN\n",
    "    threshold = 0.5\n",
    "    hrv_train_clean = hrv_train.dropna(thresh=len(hrv_train) * threshold, axis=1)\n",
    "\n",
    "    # Remove all rows that are all NaN\n",
    "    hrv_train_clean = hrv_train_clean.dropna(how='all')\n",
    "\n",
    "    hrv_train_clean.to_csv(\"data/hrv_train.csv\", index=False)\n",
    "    hrv_train_clean.head()"
   ],
   "id": "5fc5763d2a2890f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FULL HRV feature extraction for all ECGs (TEST)",
   "id": "ce978ab2f4ddeb20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Getting all the ECG readouts so we can extract P-wave information\n",
    "\n",
    "def get_ECG_readout_test():\n",
    "    test_run = 0\n",
    "    ecg_full = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for i in tqdm(test_idx):\n",
    "\n",
    "        ecg = df.iloc[i].astype(float)\n",
    "        signals, info = nk.ecg_process(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "        # Assign the current ecg_index to the signals DataFrame before concatenation\n",
    "        signals[\"ecg_index\"] = i\n",
    "\n",
    "        ecg_full = pd.concat([ecg_full, signals], ignore_index=True)\n",
    "\n",
    "        #test_run += 1\n",
    "\n",
    "        if test_run == 10:\n",
    "            break  # Stop after 10 iterations for the example\n",
    "\n",
    "    return ecg_full"
   ],
   "id": "54aaef1dfb5ae429"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_full_test = get_ECG_readout_test()"
   ],
   "id": "3e8ba3267ce37c8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_ECG_metrics(ecg_full):\n",
    "    ecg_metrics_list = []\n",
    "\n",
    "    for i in tqdm(test_idx[:]):\n",
    "        #mean_quality = ecg_full.loc[ecg_full.ecg_index == i]['ECG_Quality'].mean()\n",
    "        #Note:ecg quality is only to make the training data less noisy\n",
    "        mean_pwave_amplitude = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)][\n",
    "            'ECG_Clean'].mean()  #You could consider taking sqrt, mean and then **2\n",
    "        #(more robust) to outliers\n",
    "        stdev_pwave = ecg_full.loc[(ecg_full.ecg_index == i) & (ecg_full['ECG_P_Peaks'] == 1)]['ECG_Quality'].std()\n",
    "        #Perhaps I could add something about irregularly irregular rhythm, but it's (really) difficult mathematically\n",
    "        ecg_metrics_list.append({\n",
    "            'Mean_PWave_Amplitude': mean_pwave_amplitude,\n",
    "            'STDEV_Pwave': stdev_pwave,\n",
    "            'ecg_index': i\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(ecg_metrics_list)"
   ],
   "id": "f9aa4a665b4640c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "  ecg_metrics_test = get_ECG_metrics(ecg_full_test)"
   ],
   "id": "a7fa511aa5d64a93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    hrv_features_test = []\n",
    "\n",
    "    for i in tqdm(test_idx, desc=\"HRV (ALL FEATURES): TEST SET\"):\n",
    "        ecg = df.iloc[i].astype(float).values\n",
    "\n",
    "        try:\n",
    "            # 1. Clean ECG\n",
    "            ecg_clean = nk.ecg_clean(ecg, sampling_rate=sampling_frequency)\n",
    "\n",
    "            # 2. Detect R-peaks\n",
    "            peaks, _ = nk.ecg_peaks(\n",
    "                ecg_clean,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                correct_artifacts=True\n",
    "            )\n",
    "\n",
    "            # 3. Compute FULL HRV feature set\n",
    "            hrv_full = nk.hrv(\n",
    "                peaks,\n",
    "                sampling_rate=sampling_frequency,\n",
    "                show=False\n",
    "            )\n",
    "\n",
    "            # Same as TRAIN: keep as 1-row DataFrame, add index\n",
    "            hrv_full = hrv_full.copy()\n",
    "            hrv_full[\"ecg_index\"] = i\n",
    "\n",
    "            hrv_features_test.append(hrv_full)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing TEST ECG {i}: {e}\")\n",
    "\n",
    "            if hrv_features_test:\n",
    "                empty = pd.DataFrame(\n",
    "                    [np.nan] * hrv_features_test[0].shape[1],\n",
    "                    index=hrv_features_test[0].columns\n",
    "                ).T\n",
    "                empty[\"ecg_index\"] = i\n",
    "                hrv_features_test.append(empty)\n",
    "\n",
    "    hrv_test = pd.concat(hrv_features_test, ignore_index=True)\n",
    "\n",
    "    print(\"hrv_test shape:\", hrv_test.shape)\n",
    "    hrv_test.head()"
   ],
   "id": "25859927ced3a5af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Merge our new dataframe with our extra variables\n",
    "    hrv_test = pd.merge(hrv_test, ecg_metrics_test, on='ecg_index', how='left')\n",
    "    hrv_test.head()"
   ],
   "id": "169457a66b15f760"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == False:\n",
    "    # Remove all columns from the dataframe that contain more than 50% NaN\n",
    "    threshold = 0.5\n",
    "    hrv_test_clean = hrv_test.dropna(thresh=len(hrv_test) * threshold, axis=1)\n",
    "\n",
    "    # Remove all rows that are all NaN\n",
    "    hrv_test_clean = hrv_test_clean.dropna(how='all')\n",
    "\n",
    "    hrv_test_clean.head()\n",
    "\n",
    "    hrv_test.to_csv(\"data/hrv_test.csv\", index=False)"
   ],
   "id": "bedbdabbcd112dfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# If you don't want to wait that long",
   "id": "653487c1b8cc4134"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if LoadPremadeDataset == True:\n",
    "    hrv_test_clean = pd.read_csv(\"data/hrv_test.csv\")\n",
    "    hrv_train_clean= pd.read_csv(\"data/hrv_train.csv\")"
   ],
   "id": "d536fb6e4c2d1d96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature exploration",
   "id": "3a878524d0bb1027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the HRV data with the rhythm labels\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on='ecg_index', right_index=True\n",
    ").reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "selectedMetric = 'HRV_MedianNN'\n",
    "rhythms = hrv_train_with_labels['label'].unique()\n",
    "for rhythm in rhythms:\n",
    "    subset = hrv_train_with_labels[hrv_train_with_labels['label'] == rhythm]\n",
    "    plt.hist(subset[selectedMetric], alpha=0.7, label=rhythm, bins='auto')\n",
    "\n",
    "plt.xlabel(selectedMetric)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution by Rhythm')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "fa865cadb560df4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "b5a0f09d8205f1b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missingness",
   "id": "532113479bbd45d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def check_missing_hrv(df):\n",
    "    \"\"\"\n",
    "    Summarize missingness across HRV features.\n",
    "    \"\"\"\n",
    "    missing = df.isna().sum()\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": df.columns,\n",
    "        \"missing_n\": missing,\n",
    "        \"missing_%\": (missing / len(df)) * 100\n",
    "    })\n",
    "    display(out.sort_values(\"missing_%\", ascending=False))\n",
    "    return out"
   ],
   "id": "4f1963826b778c05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "check_missing_hrv(hrv_train_clean)",
   "id": "97c12d287534b6ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##Removing reads with low quality scores (TEST ONLY)",
   "id": "357b482b2c6de45e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ecg_metrics.head()",
   "id": "3a597e4fdd85052d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hrv_train_clean_og = hrv_train_clean",
   "id": "e38b3517666dac61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.hist(hrv_train_clean['Mean_Quality']); #You can base number below on this perhaps",
   "id": "63fe0d2f48d58645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hrv_train_clean = hrv_train_clean.loc[hrv_train_clean['Mean_Quality'] >= 0.6] #You can adjust this number to be higher or lower",
   "id": "657cf2c8920fa7bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Outlier Detection",
   "id": "ed6ad46146680351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to identify outliers in the data\n",
    "def identify_outliers(df, column_name, threshold=1.5):\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    row_indices = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)].index.tolist()\n",
    "    outlier_values = df.loc[row_indices, column_name].tolist()\n",
    "\n",
    "    return row_indices, outlier_values, lower_bound, upper_bound"
   ],
   "id": "bcffec3f54009ecf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Outlier detection ONLY ON TRAIN\n",
    "\n",
    "# Merge labels with TRAIN features (cleaned hrv_train)\n",
    "hrv_train_with_labels = pd.merge(\n",
    "    hrv_train_clean, df_labels, left_on=\"ecg_index\", right_index=True\n",
    ")\n",
    "\n",
    "# Outlier detection ONLY on TRAIN\n",
    "train_outlier_idx, outlier_values, iqr_lower, iqr_upper = identify_outliers(\n",
    "    hrv_train_with_labels,\n",
    "    \"HRV_MedianNN\",\n",
    "    threshold=1.5\n",
    ")\n",
    "\n",
    "# ecg_index as (int)\n",
    "hrv_train_with_labels[\"ecg_index\"] = hrv_train_with_labels[\"ecg_index\"].astype(int)\n",
    "\n",
    "print(\"Train outliers detected:\", len(train_outlier_idx))\n",
    "print(\"Row indices (in hrv_train_with_labels) with outliers:\", train_outlier_idx)\n",
    "print(\"Outlier HRV_MedianNN values:\", outlier_values)"
   ],
   "id": "c7824dc2aaec28df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualise one outlier ECG\n",
    "\n",
    "example_outlier_row = train_outlier_idx[0]\n",
    "\n",
    "# Single row\n",
    "row = hrv_train_with_labels.loc[example_outlier_row]\n",
    "\n",
    "# Extract ECG index value\n",
    "ecg_index_values = row.filter(like=\"ecg_index\").values\n",
    "\n",
    "# Use first value\n",
    "ecg_idx = int(ecg_index_values[0])\n",
    "\n",
    "# Extract raw ECG from df\n",
    "ecg_raw = df.iloc[ecg_idx].astype(float).values\n",
    "\n",
    "# Visualise R-Peaks\n",
    "peaks_outlier, info_outlier = nk.ecg_peaks(\n",
    "    ecg_raw,\n",
    "    sampling_rate=sampling_frequency,\n",
    "    correct_artifacts=True,\n",
    "    show=True\n",
    ")"
   ],
   "id": "9806fb964f3bb84d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hrv_train_with_labels.loc[example_outlier_row]",
   "id": "4dd388011ed37a4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Outliers TEST set** done the same way as for TRAINING",
   "id": "775d6e5c6bd4a742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Align TEST columns to TRAIN columns\n",
    "\n",
    "# Align TEST columns to TRAIN columns (no leakage, same feature space)\n",
    "train_cols = hrv_train_clean.columns  # already cleaned on TRAIN\n",
    "shared_cols = [c for c in train_cols if c in hrv_test_clean.columns]\n",
    "\n",
    "hrv_test_aligned = hrv_test_clean[shared_cols].copy()\n",
    "\n",
    "# Merge TEST HRV with labels\n",
    "hrv_test_with_labels = pd.merge(\n",
    "    hrv_test_aligned,\n",
    "    df_labels[[\"label\", \"classification\"]],\n",
    "    left_on=\"ecg_index\",\n",
    "    right_index=True\n",
    ")"
   ],
   "id": "ccdb1323f8e7fe4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Same IQR bounds as on hrv_train\n",
    "\n",
    "Q1 = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3 = hrv_train_clean[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_test_clean = hrv_test_with_labels[\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] >= lower_bound) &\n",
    "    (hrv_test_with_labels[\"HRV_MedianNN\"] <= upper_bound)\n",
    "    ].copy()\n",
    "\n",
    "print(\"hrv_test shape:\", hrv_test_clean.shape)\n",
    "print(\"hrv_test_with_labels shape:\", hrv_test_with_labels.shape)\n",
    "print(\"hrv_test_clean shape:\", hrv_test_clean.shape)"
   ],
   "id": "f4fc39977e32af4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Distribution TRAIN + TEST | Sanity check",
   "id": "3a12a28c5d92d6f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(hrv_train_clean.columns[:5])\n",
    "print(hrv_test_clean.columns[:5])\n",
    "print(hrv_test_clean[[\"HRV_MedianNN\", \"classification\"]].head())"
   ],
   "id": "336a2ba29f0d5cc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for feat in [\"HRV_MedianNN\", \"HRV_SDNN\"]:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.kdeplot(\n",
    "        data=hrv_train_clean, x=feat, label=\"Train\", fill=True, common_norm=False\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=hrv_test_clean, x=feat, label=\"Test\", fill=True, common_norm=False, color=\"orange\"\n",
    "    )\n",
    "    plt.title(f\"{feat}: Train vs Test\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "302a8e460e6ca363"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling TRAIN",
   "id": "6f62d16bec9d03d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Winsorising outliers",
   "id": "adbc4f011b987db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Q1 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.25)\n",
    "Q3 = hrv_train_with_labels[\"HRV_MedianNN\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_clip = Q1 - 1.5 * IQR\n",
    "upper_clip = Q3 + 1.5 * IQR\n",
    "\n",
    "hrv_train_winsor = hrv_train_with_labels.copy()\n",
    "hrv_train_winsor[\"HRV_MedianNN_winsor\"] = hrv_train_with_labels[\"HRV_MedianNN\"].clip(\n",
    "    lower=lower_clip, upper=upper_clip\n",
    ")\n",
    "\n",
    "print(\"Shape after winsorizing (same as original):\", hrv_train_winsor.shape)"
   ],
   "id": "e5ced3445e088fac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Handling Comparison",
   "id": "bbfa90ccf00e166a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(hrv_train_with_labels[\"HRV_MedianNN\"], kde=True, color=\"red\", label=\"Original\")\n",
    "sns.histplot(hrv_train_winsor[\"HRV_MedianNN_winsor\"], kde=True, color=\"green\", label=\"Winsorized\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Outlier Handling Comparison\")\n",
    "plt.show()"
   ],
   "id": "9f9ea2cc7bcd9edf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final Preprocessing: Building ML Matrices (X_train, X_test)",
   "id": "6f042d66779f3b82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select HRV feature columns only\n",
    "feature_cols = [col for col in hrv_train_with_labels.columns if col.startswith(\"HRV_\")]\n",
    "\n",
    "# TRAIN data\n",
    "x_train = hrv_train_with_labels[feature_cols].copy()\n",
    "y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "\n",
    "# TEST data\n",
    "x_test = hrv_test_clean[feature_cols].copy()\n",
    "y_test = hrv_test_clean[\"classification\"].copy()"
   ],
   "id": "536bbae273e3427e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Replace +/- inf with NaN in both TRAIN and TEST\n",
    "for df_ in (x_train, x_test):\n",
    "    df_.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop columns that are all-NaN (if any)\n",
    "all_nan_cols = x_train.columns[x_train.isna().all()]\n",
    "if len(all_nan_cols) > 0:\n",
    "    print(\"Dropping all-NaN columns before imputation:\", list(all_nan_cols))\n",
    "    x_train.drop(columns=all_nan_cols, inplace=True)\n",
    "    x_test.drop(columns=all_nan_cols, inplace=True)"
   ],
   "id": "afad121871f20927"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputation",
   "id": "7b31aaa32b631363"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Median imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_imputed = imputer.fit_transform(x_train)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_imputed = imputer.transform(x_test)"
   ],
   "id": "25205cbd6ee8d5b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalisation",
   "id": "4ed6250d798f72f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Temporarily convert to DataFrame to calculate Skewness easily\n",
    "temp_df = pd.DataFrame(X_train_imputed, columns=feature_cols)\n",
    "skewness = temp_df.skew().sort_values(ascending=False)\n",
    "\n",
    "#Identify skewed columns (Threshold > 1.0)\n",
    "skewed_cols = skewness[abs(skewness) > 1.0].index.tolist()\n",
    "\n",
    "#Apply Log Transform directly to the NumPy arrays\n",
    "for col_name in skewed_cols:\n",
    "    # Find the column index (integer position)\n",
    "    col_idx = feature_cols.index(col_name)\n",
    "\n",
    "    # Check for negative values (Log crashes on negatives)\n",
    "    # We find the global minimum for this column across Train and Test\n",
    "    min_val = min(X_train_imputed[:, col_idx].min(), X_test_imputed[:, col_idx].min())\n",
    "\n",
    "    shift = 0\n",
    "    if min_val < 0:\n",
    "        # If negatives exist, calculate a shift to make the minimum 0\n",
    "        shift = abs(min_val)\n",
    "\n",
    "    # Apply transformation in-place: Log(x + shift + 1)\n",
    "    X_train_imputed[:, col_idx] = np.log1p(X_train_imputed[:, col_idx] + shift)\n",
    "    X_test_imputed[:, col_idx] = np.log1p(X_test_imputed[:, col_idx] + shift)"
   ],
   "id": "91695eee64dcca86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaling",
   "id": "87dd99e33eae0d21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "# X_train: fit_transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# X_test: only transform (so test set remains untouched)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert back to df with column names\n",
    "x_train = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "x_test = pd.DataFrame(X_test_scaled, columns=feature_cols)"
   ],
   "id": "2fc6aa1f82a682c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Sanity checks**",
   "id": "fec5be905ccfd1a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Median X_train\n",
    "print(\"Median of scaled features (should be ~0):\")\n",
    "print(x_train.median().round(3))\n",
    "\n",
    "# IQR X_train\n",
    "print(\"\\nIQR of scaled features (should be ~1):\")\n",
    "print((x_train.quantile(0.75) - x_train.quantile(0.25)).round(3))\n",
    "\n",
    "#Checking skewness of the datasets\n",
    "skewness_train = x_train.skew().sort_values(ascending=False)\n",
    "skewness_test = x_train.skew().sort_values(ascending=False)\n",
    "# Filter for highly skewed columns (absolute skew > 1.0)\n",
    "high_skew_cols_train = skewness_train[abs(skewness_train) > 1.0]\n",
    "high_skew_cols_test = skewness_test[abs(skewness_test) > 1.0]\n",
    "\n",
    "print(len(high_skew_cols_train))\n",
    "print(len(high_skew_cols_test))"
   ],
   "id": "414d9865f60f62a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final ML datasets (X_train, X_test, y_train, y_test",
   "id": "f9956636b842d317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Test size:\", len(test_idx))"
   ],
   "id": "1a83a29699085317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if True:\n",
    "    # Feature matrices (winsorised > imputation > scaling)\n",
    "    x_train = X_train_scaled\n",
    "    x_test = X_test_scaled\n",
    "\n",
    "    # Target vectors (created earlier from HRV + labels AF(0/1))\n",
    "    y_train = hrv_train_with_labels[\"classification\"].copy()\n",
    "    y_test = hrv_test_clean[\"classification\"].copy()\n",
    "\n",
    "    print(\"Final X_train shape:\", x_train.shape)\n",
    "    print(\"Final X_test shape:\", x_test.shape)\n",
    "    print(\"Final y_train shape:\", y_train.shape)\n",
    "    print(\"Final y_test shape:\", y_test.shape)"
   ],
   "id": "54bf326eb616ded2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training Setup",
   "id": "97585b27f9b8a3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Safety check",
   "id": "9b82785ab557f420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "assert len(x_train) == len(y_train), \"Misaligned TRAIN matrix and labels!\"\n",
    "assert len(x_test) == len(y_test), \"Misaligned TEST matrix and labels!\"\n",
    "\n",
    "assert not np.isnan(x_train).any(), \"NaNs detected in X_train!\"\n",
    "assert not np.isnan(x_test).any(), \"NaNs detected in X_test!\""
   ],
   "id": "bf01daed78197daa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison framework",
   "id": "5dc2fe184b40ace9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "resultsTable = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC', 'ROC_AUC', 'cm'])\n",
    "\n",
    "def modelResults(model, accuracy, f1, precision, recall, roc_auc, roc_cur, cm):\n",
    "    print(\n",
    "        f\"Model {model} evaluated. \\nAccuracy: {accuracy} \\nF1 Score: {f1} \\nPrecision: {precision} \\nRecall: {recall} \\nROC AUC: {roc_auc}\")\n",
    "    resultsTable.loc[len(resultsTable)] = [model, accuracy, f1, precision, recall, roc_cur, roc_auc, cm]\n",
    "    resultsTable.to_csv(\"data/trainingResults.csv\", index=False, mode=\"a\")"
   ],
   "id": "8703906109be3f14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hyperParameterResultsTable = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC', 'ROC_AUC', 'cm'])\n",
    "\n",
    "def hyperParameterResults(model, accuracy, f1, precision, recall, roc_auc, roc_cur, cm):\n",
    "    print(\n",
    "        f\"Model {model} evaluated. \\nAccuracy: {accuracy} \\nF1 Score: {f1} \\nPrecision: {precision} \\nRecall: {recall} \\nROC AUC: {roc_auc}\")\n",
    "    hyperParameterResultsTable.loc[len(resultsTable)] = [model, accuracy, f1, precision, recall, roc_cur, roc_auc, cm]\n",
    "    hyperParameterResultsTable.to_csv(\"data/hyperParameterResults.csv\", index=False, mode=\"a\")\n"
   ],
   "id": "5091287242f4149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\n",
    "    f\"X train length: {len(x_train)}\\n X test length: {len(x_test)} \\n Y train length: {len(y_train)}\\n Y test length: {len(y_test)}\")"
   ],
   "id": "7254f592606db866"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Training",
   "id": "3ad0cb83208aa973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Going back to basics, the currently used x_train and x_test gave ValueErrors as negative values for Log\n",
    "\n",
    "raw_cols = [c for c in hrv_train_with_labels.columns if c.startswith(\"HRV_\")]\n",
    "raw_train = hrv_train_with_labels[raw_cols].copy()\n",
    "raw_test = hrv_test_clean[raw_cols].copy()\n",
    "\n",
    "raw_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "raw_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer_eng = SimpleImputer(strategy=\"median\")\n",
    "raw_train_imp = pd.DataFrame(imputer_eng.fit_transform(raw_train), columns=raw_cols)\n",
    "raw_test_imp = pd.DataFrame(imputer_eng.transform(raw_test), columns=raw_cols)\n",
    "\n",
    "skewness = raw_train_imp.skew().sort_values(ascending=False)\n",
    "skewed_cols = skewness[abs(skewness) > 1.0].index.tolist()\n",
    "\n",
    "new_features_train = pd.DataFrame(index=raw_train_imp.index)\n",
    "new_features_test = pd.DataFrame(index=raw_test_imp.index)\n",
    "\n",
    "#1. Log Transforms\n",
    "for col in skewed_cols:\n",
    "    # +1e-6 avoids log(0)\n",
    "    new_features_train[f'Log_{col}'] = np.log(raw_train_imp[col] + 1e-6)\n",
    "    new_features_test[f'Log_{col}'] = np.log(raw_test_imp[col] + 1e-6)\n",
    "\n",
    "#2. 2. Coefficient of Variation (CV) computation:\n",
    "if 'HRV_SDNN' in raw_train_imp.columns and 'HRV_MeanNN' in raw_train_imp.columns:\n",
    "    new_features_train['CV_SDNN'] = raw_train_imp['HRV_SDNN'] / (raw_train_imp['HRV_MeanNN'] + 1e-6)\n",
    "    new_features_test['CV_SDNN'] = raw_test_imp['HRV_SDNN'] / (raw_test_imp['HRV_MeanNN'] + 1e-6)\n",
    "\n",
    "# 3. Chaos Index (Amplifies the \"irregularly irregular\" signal specific to AF.):\n",
    "entropy_col = 'HRV_ApEn' if 'HRV_ApEn' in raw_train_imp.columns else 'HRV_SampEn'\n",
    "if 'HRV_RMSSD' in raw_train_imp.columns and entropy_col in raw_train_imp.columns:\n",
    "    new_features_train['Chaos_Index'] = raw_train_imp['HRV_RMSSD'] * raw_train_imp[entropy_col]\n",
    "    new_features_test['Chaos_Index'] = raw_test_imp['HRV_RMSSD'] * raw_test_imp[entropy_col]\n",
    "\n",
    "new_features_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "new_features_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer_new = SimpleImputer(strategy=\"median\")\n",
    "new_train_clean = pd.DataFrame(imputer_new.fit_transform(new_features_train), columns=new_features_train.columns)\n",
    "new_test_clean = pd.DataFrame(imputer_new.transform(new_features_test), columns=new_features_test.columns)\n",
    "\n",
    "scaler_eng = RobustScaler()\n",
    "new_train_scaled = pd.DataFrame(scaler_eng.fit_transform(new_train_clean), columns=new_features_train.columns)\n",
    "new_test_scaled = pd.DataFrame(scaler_eng.transform(new_test_clean), columns=new_features_test.columns)\n",
    "\n",
    "if not isinstance(x_train, pd.DataFrame):\n",
    "    x_train = pd.DataFrame(x_train, columns=feature_cols)\n",
    "if not isinstance(x_test, pd.DataFrame):\n",
    "    x_test = pd.DataFrame(x_test, columns=feature_cols)\n",
    "\n",
    "x_train_added = pd.concat([x_train, new_train_scaled], axis=1)\n",
    "x_test_added = pd.concat([x_test, new_test_scaled], axis=1)"
   ],
   "id": "8b4ebf9e4038b30f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Soft voting classifier",
   "id": "eec8099119f0833b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter sweep",
   "id": "d166fb12c5a2b9e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, \\\n",
    "    confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_tr = x_train_added if 'x_train_added' in globals() else x_train\n",
    "X_te = x_test_added if 'x_test_added' in globals() else x_test\n",
    "\n",
    "if isinstance(X_tr, np.ndarray):\n",
    "    X_tr_use = X_tr\n",
    "    X_te_use = X_te\n",
    "else:\n",
    "    X_tr_use = X_tr.values\n",
    "    X_te_use = X_te.values\n",
    "\n",
    "y_tr_use = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_te_use = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "solvers_grid = ['lbfgs', 'liblinear', 'saga']\n",
    "penalties_by_solver = {\n",
    "    'lbfgs': ['l2', None],\n",
    "    'liblinear': ['l1', 'l2'],\n",
    "    'saga': ['l1', 'l2', 'elasticnet']\n",
    "}\n",
    "Cs = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "class_weights = [None, 'balanced']\n",
    "max_iter = 2000\n",
    "l1_ratio_values = [0.0, 0.25, 0.5, 0.75]\n",
    "\n",
    "for solver in solvers_grid:\n",
    "    for penalty in penalties_by_solver[solver]:\n",
    "        for C in Cs:\n",
    "            for cw in class_weights:\n",
    "                if penalty == 'elasticnet' and solver == 'saga':\n",
    "                    for l1r in l1_ratio_values:\n",
    "                        try:\n",
    "                            model = LogisticRegression(\n",
    "                                solver=solver,\n",
    "                                penalty=penalty,\n",
    "                                C=C,\n",
    "                                l1_ratio=l1r,\n",
    "                                class_weight=cw,\n",
    "                                max_iter=max_iter,\n",
    "                                n_jobs=-1 if solver in ['lbfgs', 'saga'] else None,\n",
    "                                random_state=3003\n",
    "                            )\n",
    "                            model.fit(X_tr_use, y_tr_use)\n",
    "                            y_pred = model.predict(X_te_use)\n",
    "                            y_proba = model.predict_proba(X_te_use)[:, 1]\n",
    "                            acc = accuracy_score(y_te_use, y_pred)\n",
    "                            f1 = f1_score(y_te_use, y_pred)\n",
    "                            prec = precision_score(y_te_use, y_pred)\n",
    "                            rec = recall_score(y_te_use, y_pred)\n",
    "                            auc = roc_auc_score(y_te_use, y_proba)\n",
    "                            roc_cur = roc_curve(y_te_use, y_proba)\n",
    "                            cm = confusion_matrix(y_true=y_te_use, y_pred=y_pred, normalize='true')\n",
    "                            name = f\"LR(solver={solver},penalty={penalty},C={C},class_weight={cw},l1_ratio={l1r})\"\n",
    "                            hyperParameterResults(name, acc, f1, prec, rec, auc, roc_cur, cm)\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "                else:\n",
    "                    try:\n",
    "                        kwargs = dict(\n",
    "                            solver=solver,\n",
    "                            penalty=penalty if penalty is not None else 'l2',\n",
    "                            C=C,\n",
    "                            class_weight=cw,\n",
    "                            max_iter=max_iter,\n",
    "                            n_jobs=-1 if solver in ['lbfgs', 'saga'] else None,\n",
    "                            random_state=3003\n",
    "                        )\n",
    "                        if penalty != 'elasticnet' and 'l1_ratio' in kwargs:\n",
    "                            kwargs.pop('l1_ratio', None)\n",
    "                        if solver == 'liblinear' and kwargs.get('n_jobs') is not None:\n",
    "                            kwargs.pop('n_jobs', None)\n",
    "\n",
    "                        model = LogisticRegression(**kwargs)\n",
    "                        model.fit(X_tr_use, y_tr_use)\n",
    "                        y_pred = model.predict(X_te_use)\n",
    "                        y_proba = model.predict_proba(X_te_use)[:, 1]\n",
    "                        acc = accuracy_score(y_te_use, y_pred)\n",
    "                        f1 = f1_score(y_te_use, y_pred)\n",
    "                        prec = precision_score(y_te_use, y_pred)\n",
    "                        rec = recall_score(y_te_use, y_pred)\n",
    "                        auc = roc_auc_score(y_te_use, y_proba)\n",
    "                        roc_cur = roc_curve(y_te_use, y_proba)\n",
    "                        cm = confusion_matrix(y_true=y_te_use, y_pred=y_pred, normalize='true')\n",
    "                        name = f\"LR(solver={solver},penalty={penalty},C={C},class_weight={cw})\"\n",
    "                        hyperParameterResults(name, acc, f1, prec, rec, auc, roc_cur, cm)\n",
    "                    except Exception as e:\n",
    "                        continue\n"
   ],
   "id": "38a0cd9252874c5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, \\\n",
    "    confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_tr = x_train_added if 'x_train_added' in globals() else x_train\n",
    "X_te = x_test_added if 'x_test_added' in globals() else x_test\n",
    "\n",
    "if isinstance(X_tr, np.ndarray):\n",
    "    X_tr_use = X_tr\n",
    "    X_te_use = X_te\n",
    "else:\n",
    "    X_tr_use = X_tr.values\n",
    "    X_te_use = X_te.values\n",
    "\n",
    "y_tr_use = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_te_use = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "n_estimators_grid = [50, 100, 200, 300, 500]\n",
    "max_depth_grid = [None, 5, 10, 20, 30]\n",
    "min_samples_split_grid = [2, 5, 10]\n",
    "min_samples_leaf_grid = [1, 2, 4]\n",
    "max_features_grid = ['sqrt', 'log2', None]\n",
    "bootstrap_grid = [True, False]\n",
    "class_weight_grid = [None, 'balanced', 'balanced_subsample']\n",
    "criterion_grid = ['gini', 'entropy', 'log_loss']\n",
    "oob_score_grid = [False, True]\n",
    "\n",
    "for n_est in n_estimators_grid:\n",
    "    for crit in criterion_grid:\n",
    "        for md in max_depth_grid:\n",
    "            for mss in min_samples_split_grid:\n",
    "                for msl in min_samples_leaf_grid:\n",
    "                    for mf in max_features_grid:\n",
    "                        for bs in bootstrap_grid:\n",
    "                            if (not bs):\n",
    "                                oob = False\n",
    "                                oob_list = [False]\n",
    "                            else:\n",
    "                                oob_list = oob_score_grid\n",
    "                            for oob in oob_list:\n",
    "                                for cw in class_weight_grid:\n",
    "                                    try:\n",
    "                                        model = RandomForestClassifier(\n",
    "                                            n_estimators=n_est,\n",
    "                                            criterion=crit,\n",
    "                                            max_depth=md,\n",
    "                                            min_samples_split=mss,\n",
    "                                            min_samples_leaf=msl,\n",
    "                                            max_features=mf,\n",
    "                                            bootstrap=bs,\n",
    "                                            oob_score=oob,\n",
    "                                            class_weight=cw,\n",
    "                                            n_jobs=-1,\n",
    "                                            random_state=3003\n",
    "                                        )\n",
    "                                        model.fit(X_tr_use, y_tr_use)\n",
    "                                        y_pred = model.predict(X_te_use)\n",
    "                                        y_proba = model.predict_proba(X_te_use)[:, 1]\n",
    "                                        acc = accuracy_score(y_te_use, y_pred)\n",
    "                                        f1 = f1_score(y_te_use, y_pred)\n",
    "                                        prec = precision_score(y_te_use, y_pred)\n",
    "                                        rec = recall_score(y_te_use, y_pred)\n",
    "                                        auc = roc_auc_score(y_te_use, y_proba)\n",
    "                                        roc_cur = roc_curve(y_te_use, y_proba)\n",
    "                                        cm = confusion_matrix(y_true=y_te_use, y_pred=y_pred, normalize='true')\n",
    "                                        name = f\"RF(n_estimators={n_est},criterion={crit},max_depth={md},min_samples_split={mss},min_samples_leaf={msl},max_features={mf},bootstrap={bs},oob_score={oob},class_weight={cw})\"\n",
    "                                        hyperParameterResults(name, acc, f1, prec, rec, auc, roc_cur, cm)\n",
    "                                    except Exception:\n",
    "                                        continue\n"
   ],
   "id": "d18d7e6ecaab8d1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, \\\n",
    "    confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_tr = x_train_added if 'x_train_added' in globals() else x_train\n",
    "X_te = x_test_added if 'x_test_added' in globals() else x_test\n",
    "\n",
    "if isinstance(X_tr, np.ndarray):\n",
    "    X_tr_use = X_tr\n",
    "    X_te_use = X_te\n",
    "else:\n",
    "    X_tr_use = X_tr.values\n",
    "    X_te_use = X_te.values\n",
    "\n",
    "y_tr_use = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_te_use = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "n_neighbors_grid = list(range(1, 51))\n",
    "weights_grid = ['uniform', 'distance']\n",
    "metric_grid = ['minkowski', 'euclidean', 'manhattan', 'chebyshev']\n",
    "p_by_metric = {'minkowski': [1, 2, 3], 'euclidean': [2], 'manhattan': [1], 'chebyshev': [None]}\n",
    "leaf_size_grid = [15, 30, 45, 60]\n",
    "algorithm_grid = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "\n",
    "for n in n_neighbors_grid:\n",
    "    for w in weights_grid:\n",
    "        for alg in algorithm_grid:\n",
    "            for leaf in leaf_size_grid:\n",
    "                for m in metric_grid:\n",
    "                    for p in p_by_metric[m]:\n",
    "                        try:\n",
    "                            kwargs = dict(\n",
    "                                n_neighbors=n,\n",
    "                                weights=w,\n",
    "                                algorithm=alg,\n",
    "                                leaf_size=leaf,\n",
    "                                metric=m,\n",
    "                                n_jobs=-1\n",
    "                            )\n",
    "                            if p is not None and m == 'minkowski':\n",
    "                                kwargs['p'] = p\n",
    "                            elif m != 'minkowski' and 'p' in kwargs:\n",
    "                                kwargs.pop('p', None)\n",
    "                            elif m == 'minkowski' and p is None:\n",
    "                                kwargs['p'] = 2\n",
    "\n",
    "                            model = KNeighborsClassifier(**kwargs)\n",
    "                            model.fit(X_tr_use, y_tr_use)\n",
    "                            y_pred = model.predict(X_te_use)\n",
    "                            if hasattr(model, \"predict_proba\"):\n",
    "                                y_proba = model.predict_proba(X_te_use)[:, 1]\n",
    "                            else:\n",
    "                                # Fallback using distance to neighbors if proba not available\n",
    "                                # For KNN classifiers with weights this should exist; safeguard:\n",
    "                                y_proba = y_pred.astype(float)\n",
    "\n",
    "                            acc = accuracy_score(y_te_use, y_pred)\n",
    "                            f1 = f1_score(y_te_use, y_pred)\n",
    "                            prec = precision_score(y_te_use, y_pred)\n",
    "                            rec = recall_score(y_te_use, y_pred)\n",
    "\n",
    "                            try:\n",
    "                                auc = roc_auc_score(y_te_use, y_proba)\n",
    "                                roc_cur = roc_curve(y_te_use, y_proba)\n",
    "                            except Exception:\n",
    "                                # In rare cases when y_proba is degenerate\n",
    "                                auc = float('nan')\n",
    "                                roc_cur = (np.array([0.0, 1.0]), np.array([0.0, 1.0]), np.array([0.5]))\n",
    "\n",
    "                            cm = confusion_matrix(y_true=y_te_use, y_pred=y_pred, normalize='true')\n",
    "                            name = f\"KNN(n_neighbors={n},weights={w},algorithm={alg},leaf_size={leaf},metric={m}\" + (\n",
    "                                f\",p={p}\" if m == 'minkowski' else \"\") + \")\"\n",
    "                            hyperParameterResults(name, acc, f1, prec, rec, auc, roc_cur, cm)\n",
    "                        except Exception:\n",
    "                            continue\n"
   ],
   "id": "b5e0d14faf112106"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, \\\n",
    "    confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_tr = x_train_added if 'x_train_added' in globals() else x_train\n",
    "X_te = x_test_added if 'x_test_added' in globals() else x_test\n",
    "\n",
    "if isinstance(X_tr, np.ndarray):\n",
    "    X_tr_use = X_tr\n",
    "    X_te_use = X_te\n",
    "else:\n",
    "    X_tr_use = X_tr.values\n",
    "    X_te_use = X_te.values\n",
    "\n",
    "y_tr_use = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_te_use = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "n_estimators_grid = [50, 100, 200, 300, 500, 800, 1000, 2000, 4000, 5000]\n",
    "learning_rate_grid = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "algorithm_grid = ['SAMME', 'SAMME.R']\n",
    "\n",
    "dt_max_depth_grid = [1, 2, 3, 4, 5]\n",
    "dt_min_samples_split_grid = [2, 5, 10]\n",
    "dt_min_samples_leaf_grid = [1, 2, 4]\n",
    "\n",
    "for n_est in n_estimators_grid:\n",
    "    for lr in learning_rate_grid:\n",
    "        for algo in algorithm_grid:\n",
    "            for md in dt_max_depth_grid:\n",
    "                for mss in dt_min_samples_split_grid:\n",
    "                    for msl in dt_min_samples_leaf_grid:\n",
    "                        try:\n",
    "                            base = DecisionTreeClassifier(\n",
    "                                max_depth=md,\n",
    "                                min_samples_split=mss,\n",
    "                                min_samples_leaf=msl,\n",
    "                                random_state=3003\n",
    "                            )\n",
    "                            model = AdaBoostClassifier(\n",
    "                                estimator=base,\n",
    "                                n_estimators=n_est,\n",
    "                                learning_rate=lr,\n",
    "                                algorithm=algo,\n",
    "                                random_state=3003\n",
    "                            )\n",
    "                            model.fit(X_tr_use, y_tr_use)\n",
    "                            y_pred = model.predict(X_te_use)\n",
    "                            if hasattr(model, \"predict_proba\"):\n",
    "                                y_proba = model.predict_proba(X_te_use)[:, 1]\n",
    "                            else:\n",
    "                                if hasattr(model, \"decision_function\"):\n",
    "                                    scores = model.decision_function(X_te_use)\n",
    "                                    y_proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "                                else:\n",
    "                                    y_proba = y_pred.astype(float)\n",
    "\n",
    "                            acc = accuracy_score(y_te_use, y_pred)\n",
    "                            f1 = f1_score(y_te_use, y_pred)\n",
    "                            prec = precision_score(y_te_use, y_pred)\n",
    "                            rec = recall_score(y_te_use, y_pred)\n",
    "\n",
    "                            try:\n",
    "                                auc = roc_auc_score(y_te_use, y_proba)\n",
    "                                roc_cur = roc_curve(y_te_use, y_proba)\n",
    "                            except Exception:\n",
    "                                auc = float('nan')\n",
    "                                roc_cur = (np.array([0.0, 1.0]), np.array([0.0, 1.0]), np.array([0.5]))\n",
    "\n",
    "                            cm = confusion_matrix(y_true=y_te_use, y_pred=y_pred, normalize='true')\n",
    "                            name = f\"AdaBoost(n_estimators={n_est},lr={lr},alg={algo},DT(max_depth={md},min_split={mss},min_leaf={msl}))\"\n",
    "                            hyperParameterResults(name, acc, f1, prec, rec, auc, roc_cur, cm)\n",
    "                        except Exception:\n",
    "                            continue\n"
   ],
   "id": "43c494b0e59c980b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Best Voting Classifier Search",
   "id": "d072c9748ae9f584"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import itertools\n",
    "\n",
    "# Define the pool of base classifiers with various hyperparameters\n",
    "classifiers_pool = [\n",
    "    ('LR_balanced', LogisticRegression(multi_class='auto', max_iter=1000, class_weight='balanced', random_state=3003)),\n",
    "    ('LR_default', LogisticRegression(multi_class='auto', max_iter=1000, random_state=3003)),\n",
    "    ('RF_100', RandomForestClassifier(n_estimators=100, random_state=3003)),\n",
    "    ('RF_200', RandomForestClassifier(n_estimators=200, random_state=3003)),\n",
    "    ('RF_300', RandomForestClassifier(n_estimators=200, random_state=3003)),\n",
    "    ('KNN_5', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('KNN_7', KNeighborsClassifier(n_neighbors=7)),\n",
    "    ('GBC_100', GradientBoostingClassifier(n_estimators=100, random_state=3003)),\n",
    "    ('GBC_200', GradientBoostingClassifier(n_estimators=200, random_state=3003)),\n",
    "    ('Ada_50', AdaBoostClassifier(n_estimators=50, random_state=3003)),\n",
    "    ('Ada_100', AdaBoostClassifier(n_estimators=100, random_state=3003))\n",
    "]\n",
    "\n",
    "best_f1_found = -1\n",
    "best_voting_model = None\n",
    "best_ensemble_name = \"\"\n",
    "\n",
    "print(\"Searching for best Voting Classifier configuration (optimizing for F1 Score)...\")\n",
    "\n",
    "# Iterate through all possible combinations of length 2 to 4 (limiting to 4 to avoid too long runtime)\n",
    "for r in range(2, 5):\n",
    "    for ensemble in itertools.combinations(classifiers_pool, r):\n",
    "        # Create a name for this combination\n",
    "        names = [name for name, _ in ensemble]\n",
    "        ensemble_name = f\"BestVote ({'+'.join(names)})\"\n",
    "\n",
    "        # Create the voting classifier\n",
    "        # Using soft voting as these models support probability estimates\n",
    "        voter = VotingClassifier(estimators=list(ensemble), voting='soft', n_jobs=-1)\n",
    "\n",
    "        # Train\n",
    "        voter.fit(x_train_added, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = voter.predict(x_test_added)\n",
    "        y_pred_proba = voter.predict_proba(x_test_added)[:, 1]\n",
    "\n",
    "        # Evaluate\n",
    "        current_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "        modelResults(ensemble, accuracy, current_f1, precision, recall, roc_auc, roc_cur, cm)\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Tested {ensemble_name}: F1 Score = {current_f1:.4f}\")\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "        modelResults(ensemble_name, accuracy, current_f1, precision, recall, roc_auc, roc_cur, cm)\n",
    "\n",
    "        if current_f1 > best_f1_found:\n",
    "            best_f1_found = current_f1\n",
    "            best_voting_model = voter\n",
    "            best_ensemble_name = ensemble_name\n",
    "\n",
    "print(f\"\\nWinner configuration: {best_ensemble_name} with F1 Score: {best_f1_found:.4f}\")\n",
    "\n",
    "# Log the best result to the global resultsTable\n",
    "if best_voting_model:\n",
    "    y_pred = best_voting_model.predict(x_test)\n",
    "    y_pred_proba = best_voting_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    roc_cur = roc_curve(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "    # Plot matrix for the winner\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_voting_model.classes_)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Best Found Ensemble: {best_ensemble_name}\")\n",
    "    plt.show()"
   ],
   "id": "9c6dda7208d1989e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hyperParameterResultsTable.to_csv(\"data/hyperParameterResults.csv\", index=False, mode=\"a\")",
   "id": "7e4a7fbda16fa496"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model evaluation\n",
    "## Quick conclusion"
   ],
   "id": "f92ff0d2bd31847d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "original_size = resultsTable.shape[0]\n",
    "resultsTable = resultsTable.drop_duplicates(subset=['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC'])\n",
    "print(f\"Dropped {original_size - resultsTable.shape[0]} duplicate rows\")"
   ],
   "id": "4562c23e2874816c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resultsTable",
   "id": "d04d993a7ddf447e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_models = hyperParameterResultsTable.sort_values(by='F1 Score', ascending=False).head(5)\n",
    "\n",
    "print(\"Top 5 models based on F1 score:\")\n",
    "for i, (_, row) in enumerate(top_models.iterrows(), 1):\n",
    "    print(f\"{i}. {row['Model']} with an F1 score of {row['F1 Score']:.4f}\")\n"
   ],
   "id": "d3b9ead45abcbb08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Graphs of numerical metrics\n",
    "### Logarithmic scale"
   ],
   "id": "78c3e5776584c37e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if True:\n",
    "    numeric_metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "\n",
    "    model_labels = [str(model).split('(')[0] for model in resultsTable['Model']]\n",
    "\n",
    "    fig, axes = plt.subplots(len(numeric_metrics), 1, figsize=(10, len(numeric_metrics) * 8))\n",
    "\n",
    "    for i, col in enumerate(numeric_metrics):\n",
    "        bars = axes[i].bar(range(len(resultsTable)), resultsTable[col])\n",
    "        axes[i].set_xticks(range(len(resultsTable)))\n",
    "        axes[i].set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "        axes[i].set_ylabel(col)\n",
    "        axes[i].set_title(f'{col} by Model')\n",
    "        axes[i].set_yscale('log')\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "        for j, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axes[i].text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                         f'{resultsTable[col].iloc[j]:.3f}',\n",
    "                         ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d15c9a478d366e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numeric_metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC_AUC']\n",
    "\n",
    "model_labels = [str(model).split('(')[0] for model in resultsTable['Model']]\n",
    "\n",
    "fig, axes = plt.subplots(len(numeric_metrics), 1, figsize=(10, len(numeric_metrics) * 8))\n",
    "\n",
    "for i, col in enumerate(numeric_metrics):\n",
    "    bars = axes[i].bar(range(len(resultsTable)), resultsTable[col])\n",
    "    axes[i].set_xticks(range(len(resultsTable)))\n",
    "    axes[i].set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "    axes[i].set_ylabel(col)\n",
    "    axes[i].set_title(f'{col} by Model')\n",
    "    axes[i].set_ylim(top=1)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                     f'{resultsTable[col].iloc[j]:.3f}',\n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5a3bc2498ddff9d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ROC Curves",
   "id": "df603893fe8e9f67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for idx, row in resultsTable.iterrows():\n",
    "    model_name = str(row['Model']).split('(')[0]\n",
    "    fpr, tpr, thresholds = row['ROC']\n",
    "    roc_auc = row['ROC_AUC']\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(alpha=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "54bc34670d7e1aec"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
